# RAG ì‹œìŠ¤í…œ í•µì‹¬ ëª¨ë“ˆ
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import re
from dataclasses import dataclass
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# AI ëª¨ë¸ ê´€ë ¨ import
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - LLM ê¸°ëŠ¥ ë¹„í™œì„±í™”")

# LangChain + OpenAI ê´€ë ¨ import
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    LANGCHAIN_AVAILABLE = True
    print("âœ… LangChain + OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    print(f"âš ï¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")

# Semantic Search ê´€ë ¨ import
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    import numpy as np
    SEMANTIC_SEARCH_AVAILABLE = True
except ImportError:
    SEMANTIC_SEARCH_AVAILABLE = False
    print("âš ï¸ semantic search ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (sentence-transformers, faiss-cpu)")

# ì„ë² ë”© ìºì‹œìš©
import pickle

@dataclass
class Document:
    """ë¬¸ì„œ ê°ì²´"""
    content: str
    metadata: Dict[str, Any]
    filename: str = ""
    
class SimpleRAG:
    """Semantic RAG ì‹œìŠ¤í…œ - HuggingFace + FAISS ê¸°ë°˜"""
    
    def __init__(self, knowledge_base_path: str):
        # MPS ì™„ì „ ë¹„í™œì„±í™” (MacOS M1/M2/M3)
        import os
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        
        self.knowledge_base_path = Path(knowledge_base_path)
        self.documents: List[Document] = []
        self.tokenizer = None
        self.model = None
        
        # Semantic Search ì»´í¬ë„ŒíŠ¸
        self.embedding_model = None
        self.faiss_index = None
        self.document_embeddings = None
        self.embedding_cache_path = self.knowledge_base_path.parent / "embeddings_cache.pkl"
        
        # OpenAI LLM
        self.openai_llm = None
        self.openai_chain = None
        
        self.load_knowledge_base()
        self.load_embedding_model()
        self.build_semantic_index()
        self.setup_openai_llm()
    
    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if not SEMANTIC_SEARCH_AVAILABLE:
            print("âš ï¸ semantic search ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            return
            
        try:
            print("ğŸ¤— Sentence Transformer ëª¨ë¸ ë¡œë”©...")
            # í•œêµ­ì–´ + ì˜ì–´ ì§€ì›í•˜ëŠ” multilingual ëª¨ë¸ ì‚¬ìš©
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embedding_model = None
    
    def build_semantic_index(self):
        """ë¬¸ì„œ ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        if not self.embedding_model or not self.documents:
            print("âš ï¸ ì„ë² ë”© ëª¨ë¸ì´ ì—†ê±°ë‚˜ ë¬¸ì„œê°€ ì—†ì–´ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
            
        print("ğŸ” ë¬¸ì„œ ì„ë² ë”© ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # ìºì‹œëœ ì„ë² ë”©ì´ ìˆëŠ”ì§€ í™•ì¸
        if self._load_cached_embeddings():
            print("âœ… ìºì‹œëœ ì„ë² ë”© ì‚¬ìš©")
            return
            
        try:
            # ê° ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
            document_chunks = []
            chunk_to_doc_map = []
            
            for doc_idx, doc in enumerate(self.documents):
                # ë¬¸ì„œë¥¼ 512ì ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í•  (ì˜¤ë²„ë© 100ì)
                chunks = self._split_document_into_chunks(doc.content, chunk_size=512, overlap=100)
                for chunk in chunks:
                    if len(chunk.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                        document_chunks.append(chunk)
                        chunk_to_doc_map.append(doc_idx)
            
            if not document_chunks:
                print("âš ï¸ ìœ íš¨í•œ ë¬¸ì„œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
                
            print(f"ğŸ“„ ì´ {len(document_chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            # ì„ë² ë”© ìƒì„±
            print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.embedding_model.encode(
                document_chunks,
                show_progress_bar=True,
                batch_size=16
            )
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            dimension = embeddings.shape[1]
            self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            
            # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
            faiss.normalize_L2(embeddings)
            self.faiss_index.add(embeddings.astype(np.float32))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self.document_embeddings = {
                'chunks': document_chunks,
                'chunk_to_doc_map': chunk_to_doc_map,
                'embeddings': embeddings
            }
            
            # ìºì‹œ ì €ì¥
            self._save_embeddings_cache()
            
            print(f"âœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ ({len(document_chunks)}ê°œ ì²­í¬, {dimension}ì°¨ì›)")
            
        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.faiss_index = None
            self.document_embeddings = None
    
    def _split_document_into_chunks(self, text: str, chunk_size: int = 512, overlap: int = 100) -> List[str]:
        """ë¬¸ì„œë¥¼ ì˜¤ë²„ë©ì´ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• """
        if len(text) <= chunk_size:
            return [text]
            
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            chunk = text[start:end]
            last_space = chunk.rfind(' ')
            if last_space != -1 and last_space > chunk_size * 0.7:
                chunk = chunk[:last_space]
                end = start + last_space
            
            chunks.append(chunk)
            start = end - overlap
            
        return chunks
    
    def _load_cached_embeddings(self) -> bool:
        """ìºì‹œëœ ì„ë² ë”© ë¡œë“œ"""
        try:
            if self.embedding_cache_path.exists():
                with open(self.embedding_cache_path, 'rb') as f:
                    cache_data = pickle.load(f)
                
                # ë¬¸ì„œ ê°œìˆ˜ê°€ ê°™ì€ì§€ í™•ì¸
                if cache_data.get('doc_count') == len(self.documents):
                    self.document_embeddings = cache_data['embeddings']
                    
                    # FAISS ì¸ë±ìŠ¤ ë³µì›
                    embeddings = cache_data['embeddings']['embeddings']
                    dimension = embeddings.shape[1]
                    self.faiss_index = faiss.IndexFlatIP(dimension)
                    self.faiss_index.add(embeddings.astype(np.float32))
                    
                    return True
            return False
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _save_embeddings_cache(self):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        try:
            cache_data = {
                'doc_count': len(self.documents),
                'embeddings': self.document_embeddings
            }
            with open(self.embedding_cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            print(f"ğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥: {self.embedding_cache_path}")
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        print("ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ ë¡œë”© ì‹œì‘...")
        
        self.documents = []
        if not self.knowledge_base_path.exists():
            print("âš ï¸ ì§€ì‹ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ë“¤
        supported_extensions = ["*.md", "*.txt", "*.json", "*.csv"]
        
        # PDF ì§€ì› (PyPDF2ê°€ ì„¤ì¹˜ëœ ê²½ìš°)
        try:
            import PyPDF2
            supported_extensions.append("*.pdf")
        except ImportError:
            print("ğŸ’¡ PyPDF2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ PDF íŒŒì¼ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   PDF ì§€ì›ì„ ì›í•œë‹¤ë©´: poetry add PyPDF2")
        
        # ì›Œë“œ ë¬¸ì„œ ì§€ì› (python-docxê°€ ì„¤ì¹˜ëœ ê²½ìš°)  
        try:
            import docx
            supported_extensions.append("*.docx")
        except ImportError:
            print("ğŸ’¡ python-docxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ DOCX íŒŒì¼ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   DOCX ì§€ì›ì„ ì›í•œë‹¤ë©´: poetry add python-docx")
        
        for pattern in supported_extensions:
            for file_path in self.knowledge_base_path.rglob(pattern):
                try:
                    content = self._load_file_content(file_path)
                    if content:
                        doc = Document(
                            content=content,
                            metadata={
                                'source_file': file_path.name,
                                'category': file_path.parent.name,
                                'file_path': str(file_path),
                                'file_type': file_path.suffix[1:]  # í™•ì¥ì ì €ì¥
                            },
                            filename=file_path.name
                        )
                        self.documents.append(doc)
                        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {file_path.name} ({file_path.suffix})")
                        
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path.name} - {e}")
        
        print(f"ğŸ“– ì´ {len(self.documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    
    def _load_file_content(self, file_path: Path) -> str:
        """íŒŒì¼ í˜•ì‹ë³„ ë‚´ìš© ë¡œë“œ"""
        try:
            if file_path.suffix.lower() == '.md':
                # ë§ˆí¬ë‹¤ìš´ íŒŒì¼
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            
            elif file_path.suffix.lower() == '.txt':
                # í…ìŠ¤íŠ¸ íŒŒì¼
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            
            elif file_path.suffix.lower() == '.json':
                # JSON íŒŒì¼ - êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return self._json_to_text(data, file_path.name)
            
            elif file_path.suffix.lower() == '.csv':
                # CSV íŒŒì¼ - í‘œ í˜•íƒœ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                try:
                    import pandas as pd
                    df = pd.read_csv(file_path)
                    return f"# {file_path.name}\n\n{df.to_string()}"
                except ImportError:
                    # pandas ì—†ìœ¼ë©´ ê¸°ë³¸ CSV ì½ê¸°
                    import csv
                    content = [f"# {file_path.name}\n"]
                    with open(file_path, 'r', encoding='utf-8') as f:
                        reader = csv.reader(f)
                        for row in reader:
                            content.append(" | ".join(row))
                    return "\n".join(content)
            
            elif file_path.suffix.lower() == '.pdf':
                # PDF íŒŒì¼
                try:
                    import PyPDF2
                    content = [f"# {file_path.name}\n"]
                    with open(file_path, 'rb') as f:
                        pdf_reader = PyPDF2.PdfReader(f)
                        for page in pdf_reader.pages:
                            content.append(page.extract_text())
                    return "\n".join(content)
                except ImportError:
                    print(f"âš ï¸ PyPDF2ê°€ ì—†ì–´ PDF íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path.name}")
                    return ""
            
            elif file_path.suffix.lower() == '.docx':
                # Word ë¬¸ì„œ
                try:
                    import docx
                    doc = docx.Document(file_path)
                    content = [f"# {file_path.name}\n"]
                    for paragraph in doc.paragraphs:
                        if paragraph.text.strip():
                            content.append(paragraph.text)
                    return "\n".join(content)
                except ImportError:
                    print(f"âš ï¸ python-docxê°€ ì—†ì–´ DOCX íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path.name}")
                    return ""
            
            else:
                print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}")
                return ""
                
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file_path.name}): {e}")
            return ""
    
    def _json_to_text(self, data: dict, filename: str) -> str:
        """JSON ë°ì´í„°ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = [f"# {filename}\n"]
        
        def extract_text(obj, prefix=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    text_parts.append(f"{prefix}{key}: {value}")
                    if isinstance(value, (dict, list)):
                        extract_text(value, f"{prefix}  ")
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    text_parts.append(f"{prefix}[{i}]: {item}")
                    if isinstance(item, (dict, list)):
                        extract_text(item, f"{prefix}  ")
        
        extract_text(data)
        return "\n".join(text_parts)
    
    def load_model(self):
        """í•œêµ­ì–´ íŠ¹í™” LLM ë¡œë“œ - í•˜ë“œì½”ë”© ì—†ì´ ììœ ë¡­ê²Œ ëŒ€í™”"""
        if not TRANSFORMERS_AVAILABLE:
            print("âš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            return
            
        # í•œêµ­ì–´ ìƒì„± ê°€ëŠ¥ ëª¨ë¸ë“¤
        model_candidates = [
            # í•œêµ­ì–´ íŠ¹í™”
            ("skt/kogpt2-base-v2", "SKT KoGPT-2 í•œêµ­ì–´ ëª¨ë¸"),
            ("skt/ko-gpt-trinity-1.2B-v0.5", "KoGPT Trinity"),
            # ë‹¤êµ­ì–´ ì§€ì›
            ("facebook/opt-125m", "OPT 125M"),
            ("EleutherAI/gpt-neo-125M", "GPT-Neo 125M"),
            # Fallback
            ("gpt2", "GPT-2"),
        ]
        
        for model_name, description in model_candidates:
            try:
                print(f"ğŸ§  {description} ë¡œë”© ì¤‘... ({model_name})")
                
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                # MacOS MPS ë¬¸ì œ íšŒí”¼ - CPUë§Œ ì‚¬ìš©
                import os
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                
                # CPU ê°•ì œ
                self.model = self.model.to('cpu')
                self.model.eval()
                print("ğŸ“ ëª¨ë¸ì„ CPUì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤")
                
                # íŒ¨ë”© í† í° ì„¤ì •
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                    if self.model.config.pad_token_id is None:
                        self.model.config.pad_token_id = self.tokenizer.eos_token_id
                    
                print(f"âœ… {description} ë¡œë“œ ì„±ê³µ! ììœ ë¡œìš´ í•œêµ­ì–´ ëŒ€í™” ê°€ëŠ¥")
                return
                
            except Exception as e:
                print(f"âš ï¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
        
        print("âŒ ëª¨ë“  LLM ë¡œë“œ ì‹¤íŒ¨. ì§€ì‹ë² ì´ìŠ¤ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        self.tokenizer = None
        self.model = None
    
    def setup_openai_llm(self):
        """OpenAI GPT-3.5-turbo ì„¤ì •"""
        if not LANGCHAIN_AVAILABLE:
            print("âš ï¸ LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ OpenAIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or api_key == "your_openai_api_key_here":
            print("âš ï¸ OPENAI_API_KEYê°€ .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            print("ğŸ’¡ .env íŒŒì¼ì„ ìƒì„±í•˜ê³  OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
            return
        
        try:
            model_name = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
            
            # OpenAI LLM ì´ˆê¸°í™”
            self.openai_llm = ChatOpenAI(
                model=model_name,
                temperature=0.7,  # ì°½ì˜ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€
                api_key=api_key
            )
            
            # RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ íƒì§€ ì‹œìŠ¤í…œ ë° AI/ë¨¸ì‹ ëŸ¬ë‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì •ë³´ì™€ ì „ë¬¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì›ì¹™:
1. Contextì— ì§ì ‘ì ì¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ìš°ì„  í™œìš©
2. Contextì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì „ë¬¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì—¬ ë‹µë³€
3. ì™„ì „íˆ ëª¨ë¥´ëŠ” ë‚´ìš©ë§Œ "ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"
4. ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…
5. ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ 2-4ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€
6. mAP, ì‹ ë¢°ë„, íƒì§€ ì„±ëŠ¥ ë“± ê¸°ìˆ ì  ì§ˆë¬¸ì—ëŠ” ì¼ë°˜ì ì¸ ì‚°ì—… ê¸°ì¤€ ì œì‹œ"""),
                ("user", """ì°¸ê³  ìë£Œ: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:""")
            ])
            
            # Chain êµ¬ì„±
            self.openai_chain = prompt | self.openai_llm | StrOutputParser()
            
            print(f"âœ… OpenAI {model_name} ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ OpenAI ì„¤ì • ì‹¤íŒ¨: {e}")
            self.openai_llm = None
            self.openai_chain = None
    
    def simple_search(self, query: str, top_k: int = 3) -> List[Document]:
        """Semantic ê²€ìƒ‰ (FAISS) ë˜ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        if not self.documents:
            print("ğŸ“š ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # Semantic Search ìš°ì„  ì‹œë„
        if self.faiss_index is not None and self.embedding_model is not None:
            return self._semantic_search(query, top_k)
        else:
            print("ğŸ”„ Semantic search ë¶ˆê°€, í‚¤ì›Œë“œ ê²€ìƒ‰ ì‚¬ìš©")
            return self._keyword_search(query, top_k)
    
    def _semantic_search(self, query: str, top_k: int = 3) -> List[Document]:
        """FAISS ê¸°ë°˜ Semantic ê²€ìƒ‰"""
        try:
            print(f"ğŸ¤— Semantic ê²€ìƒ‰: '{query}' (ì´ {len(self.documents)}ê°œ ë¬¸ì„œ)")
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            # FAISS ê²€ìƒ‰
            scores, indices = self.faiss_index.search(
                query_embedding.astype(np.float32), 
                k=min(top_k * 3, len(self.document_embeddings['chunks']))  # ë” ë§ì´ ì°¾ì•„ì„œ ë‹¤ì–‘ì„± í™•ë³´
            )
            
            # ì²­í¬ë¥¼ ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í•‘
            doc_scores = {}
            for score, idx in zip(scores[0], indices[0]):
                if idx == -1:  # ê²€ìƒ‰ ì‹¤íŒ¨
                    continue
                    
                doc_idx = self.document_embeddings['chunk_to_doc_map'][idx]
                chunk_text = self.document_embeddings['chunks'][idx]
                
                if doc_idx not in doc_scores:
                    doc_scores[doc_idx] = {
                        'max_score': score,
                        'avg_score': score,
                        'count': 1,
                        'best_chunk': chunk_text[:200] + '...'
                    }
                else:
                    doc_scores[doc_idx]['max_score'] = max(doc_scores[doc_idx]['max_score'], score)
                    doc_scores[doc_idx]['avg_score'] = (doc_scores[doc_idx]['avg_score'] * doc_scores[doc_idx]['count'] + score) / (doc_scores[doc_idx]['count'] + 1)
                    doc_scores[doc_idx]['count'] += 1
                    
                    # ë” ë†’ì€ ì ìˆ˜ ì²­í¬ë¡œ ëŒ€í‘œ ì²­í¬ ì—…ë°ì´íŠ¸
                    if score > doc_scores[doc_idx]['max_score'] * 0.95:
                        doc_scores[doc_idx]['best_chunk'] = chunk_text[:200] + '...'
            
            # ë¬¸ì„œë³„ ì ìˆ˜ë¡œ ì •ë ¬ - ê´€ë ¨ì„±ì„ ë” ì¤‘ìš”í•˜ê²Œ ê³ ë ¤
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ì¶”ê°€
            query_lower = query.lower()
            key_terms = ['ì¬ì„ ì¶©', 'ì†Œë‚˜ë¬´ì¬ì„ ì¶©', 'pine wilt', 'íƒì§€', 'detection', 'í™•ì¸', 'ë°©ë²•', 'ì¦ìƒ']
            
            for doc_idx, score_info in doc_scores.items():
                doc = self.documents[doc_idx]
                content_lower = doc.content.lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                keyword_bonus = 0
                for term in key_terms:
                    if term in query_lower and term in content_lower:
                        keyword_bonus += 0.1
                
                # íŒŒì¼ëª… ê´€ë ¨ì„± ë³´ë„ˆìŠ¤
                filename = doc.metadata.get('source_file', '').lower()
                if any(term in filename for term in ['ì¬ì„ ì¶©', 'ì†Œë‚˜ë¬´', 'pine', 'íƒì§€', 'detection']):
                    keyword_bonus += 0.15
                
                # ì„¤ì • íŒŒì¼ í˜ë„í‹°
                if 'setting' in filename or 'ì„¤ì •' in filename or 'config' in filename:
                    keyword_bonus -= 0.3
                
                score_info['final_score'] = score_info['max_score'] + score_info['avg_score'] + keyword_bonus
            
            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬ (final_score ìš°ì„ )
            sorted_docs = sorted(doc_scores.items(), 
                               key=lambda x: x[1]['final_score'], 
                               reverse=True)
            
            # ìƒìœ„ ë¬¸ì„œ ë°˜í™˜
            result_docs = []
            for doc_idx, score_info in sorted_docs[:top_k]:
                doc = self.documents[doc_idx]
                print(f"ğŸ“„ {doc.filename}: ì ìˆ˜ {score_info['max_score']:.3f} (ì²­í¬ {score_info['count']}ê°œ)")
                print(f"   ëŒ€í‘œ ì²­í¬: {score_info['best_chunk']}")
                result_docs.append(doc)
            
            print(f"âœ… Semantic ê²€ìƒ‰ ì™„ë£Œ: {len(result_docs)}ê°œ ë¬¸ì„œ")
            return result_docs
            
        except Exception as e:
            print(f"âš ï¸ Semantic ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self._keyword_search(query, top_k)
    
    def _keyword_search(self, query: str, top_k: int = 3) -> List[Document]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ë°±ì—…)"""
        print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: '{query}' (ì´ {len(self.documents)}ê°œ ë¬¸ì„œ)")
        
        # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_lower = query.lower()
        query_keywords = self._extract_keywords(query_lower)
        print(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {query_keywords}")
        
        # ê° ë¬¸ì„œì— ëŒ€í•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        scored_docs = []
        for doc in self.documents:
            score = self._calculate_similarity(query_keywords, doc.content.lower())
            print(f"ğŸ“„ {doc.filename}: ì ìˆ˜ {score:.3f}")
            scored_docs.append((doc, score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        result = [doc for doc, score in scored_docs[:top_k]]
        print(f"âœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(result)}ê°œ ë¬¸ì„œ")
        return result
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-z0-9]+', text)
        # 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ì‚¬ìš©
        keywords = [word for word in words if len(word) >= 2]
        return keywords
    
    def _calculate_similarity(self, query_keywords: List[str], doc_content: str) -> float:
        """ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ìœ ì‚¬ë„"""
        doc_keywords = self._extract_keywords(doc_content)
        
        if not query_keywords:
            return 0.1  # ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬
        
        if not doc_keywords:
            return 0.0
        
        # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜ ê¸°ë°˜ ì ìˆ˜
        matches = 0
        partial_matches = 0
        
        for query_keyword in query_keywords:
            # ì™„ì „ ë§¤ì¹­
            if query_keyword in doc_keywords:
                matches += 1
            # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ ë¬¸ì„œ ë‚´ìš©ì— í¬í•¨)
            elif query_keyword in doc_content:
                partial_matches += 1
        
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        if matches > 0:
            similarity = matches / len(query_keywords)
        elif partial_matches > 0:
            similarity = (partial_matches * 0.5) / len(query_keywords)
        else:
            # ì¼ë°˜ì ì¸ ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ê´€ë ¨ ë¬¸ì„œë©´ ê¸°ë³¸ ì ìˆ˜
            if any(word in doc_content for word in ['ì†Œë‚˜ë¬´', 'ì¬ì„ ì¶©', 'íƒì§€', 'ë³‘í•´', 'ì‚°ë¦¼']):
                similarity = 0.3
            else:
                similarity = 0.1
        
        # íŠ¹ì • í‚¤ì›Œë“œì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        high_value_keywords = ['ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘', 'ì¦ìƒ', 'ì´ˆê¸°', 'íƒì§€', 'ì‹ ë¢°ë„', 'gps', 'ì¢Œí‘œ', 'ë°©ì œ', 'í”¼í•´ëª©', 'í™•ì‚°']
        for keyword in query_keywords:
            if keyword in high_value_keywords:
                if keyword in doc_content:
                    similarity += 0.4
        
        return min(similarity, 1.0)
    
    def _classify_question_type(self, keywords: List[str], question_text: str) -> str:
        """ì§ˆë¬¸ ìœ í˜•ì„ ë¶„ë¥˜"""
        
        # YOLO/ëª¨ë¸ ê´€ë ¨ ì§ˆë¬¸
        yolo_patterns = ['yolo', 'ìšœë¡œ', 'ëª¨ë¸', 'íƒì§€', 'ë”¥ëŸ¬ë‹', 'í•™ìŠµ', 'í›ˆë ¨', 'ì•Œê³ ë¦¬ì¦˜', 'ai']
        if any(pattern in question_text for pattern in yolo_patterns):
            if any(kw in keywords for kw in ['ì–´ë–»ê²Œ', 'ë°©ë²•', 'í•˜ë©´', 'ì¢‹ì„ê¹Œ', 'ê°œì„ ']):
                return "yolo_detection"
        
        # ìš”ì•½/ì •ë¦¬ ê´€ë ¨ ì§ˆë¬¸  
        summary_patterns = ['ì •ë¦¬', 'ìš”ì•½', 'ê°„ëµ', 'ë“±ë¡', 'ì°¸ê³ ', 'ë¬¸ì„œ', 'md', 'pdf']
        if any(pattern in question_text for pattern in summary_patterns):
            return "summary"
            
        # ì¦ìƒ ê´€ë ¨ ì§ˆë¬¸
        symptom_patterns = ['ì¦ìƒ', 'ì´ˆê¸°', 'íŠ¹ì§•', 'ë³€í™”', 'ì–´ë–¤']
        if any(pattern in question_text for pattern in symptom_patterns):
            return "symptoms"
            
        # ë°©ì œ ê´€ë ¨ ì§ˆë¬¸
        control_patterns = ['ë°©ì œ', 'ì²˜ë¦¬', 'ëŒ€ì‘', 'ì¡°ì¹˜', 'ê´€ë¦¬']
        if any(pattern in question_text for pattern in control_patterns):
            return "control"
            
        # ê¸°ìˆ ì  ì§ˆë¬¸
        technical_patterns = ['ì •í™•ë„', 'ì„±ëŠ¥', 'ê°œì„ ', 'ìµœì í™”', 'í–¥ìƒ']
        if any(pattern in question_text for pattern in technical_patterns):
            return "technical"
            
        # ê¸°ë³¸ê°’
        return "general"
    
    def get_context_for_question(self, question: str) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        relevant_docs = self.simple_search(question, top_k=3)
        
        if not relevant_docs:
            return "ê´€ë ¨ ì „ë¬¸ ì§€ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            # ë¬¸ì„œ ë‚´ìš© ìš”ì•½ (ì²˜ìŒ 500ì)
            content_preview = doc.content[:500].strip()
            if len(doc.content) > 500:
                content_preview += "..."
            
            context_part = f"""
ğŸ“– ì°¸ê³ ìë£Œ {i} ({doc.metadata['source_file']}):
{content_preview}
"""
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def generate_korean_response(self, question: str, context: str = "", data_analysis: str = "") -> str:
        """OpenAI GPTë¥¼ í™œìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±"""
        
        print(f"ğŸ’¬ ì§ˆë¬¸: {question}")
        
        # 1. ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
        if not context:
            context = self.get_context_for_question(question)
        
        # 2. OpenAIë¡œ ë‹µë³€ ìƒì„±
        if self.openai_chain:
            try:
                print("ğŸ¤– OpenAIë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
                print(f"ğŸ“ Context ê¸¸ì´: {len(context)} ê¸€ì")
                print(f"ğŸ“ Context ë¯¸ë¦¬ë³´ê¸°: {context[:200]}...")
                
                response = self.openai_chain.invoke({
                    "context": context,
                    "question": question
                })
                
                print(f"ğŸ“¤ OpenAI ì›ë³¸ ì‘ë‹µ: '{response}'")
                print(f"ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response)} ê¸€ì")
                
                if response and len(response.strip()) > 10:
                    print(f"âœ… OpenAI ë‹µë³€ ìƒì„± ì™„ë£Œ ({len(response)} ê¸€ì)")
                    return response.strip()
                else:
                    print(f"âš ï¸ OpenAI ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: '{response}'")
                    
            except Exception as e:
                print(f"âš ï¸ OpenAI ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # 3. Fallback: ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì§ì ‘ ì¶”ì¶œ
        print("ğŸ”„ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì§ì ‘ ë‹µë³€ ì¶”ì¶œ")
        return self._generate_dynamic_answer(question, context)
    
    def _generate_simple_gpt_response(self, question: str, context: str, data_analysis: str = "") -> str:
        """ê°„ë‹¨í•˜ê³  í™•ì‹¤í•œ GPT ì‘ë‹µ ìƒì„±"""
        
        # ë§¤ìš° ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if context and len(context.strip()) > 10:
            prompt = f"""ì§ˆë¬¸: {question}

ê´€ë ¨ ì •ë³´:
{context[:500]}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
        else:
            prompt = f"""ì§ˆë¬¸: {question}

ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ì „ë¬¸ê°€ë¡œì„œ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""

        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ê¸€ì")
        
        # ì•ˆì „í•œ í† í°í™”
        try:
            inputs = self.tokenizer.encode(
                prompt, 
                return_tensors='pt', 
                max_length=400,  # ë” ì§§ê²Œ
                truncation=True
            )
            
            print(f"ğŸ¤– GPT ìƒì„± ì¤‘... (ì…ë ¥ í† í°: {inputs.shape[1]})")
            
            # ë§¤ìš° ë³´ìˆ˜ì ì¸ ìƒì„± ì„¤ì •
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=100,  # ì§§ê²Œ
                    min_new_tokens=20,   # ìµœì†Œ ë³´ì¥
                    temperature=0.8,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # ì‘ë‹µ ì¶”ì¶œ
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ì‘ë‹µë§Œ ì¶”ì¶œ
            if len(full_response) > len(prompt):
                response = full_response[len(prompt):].strip()
            else:
                response = ""
            
            print(f"ğŸ“¤ ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response)}")
            
            # ì‘ë‹µ ì •ì œ
            if response:
                # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±°
                sentences = response.split('.')
                if len(sentences) > 1:
                    complete_sentences = [s.strip() for s in sentences[:-1] if len(s.strip()) > 10]
                    if complete_sentences:
                        response = '. '.join(complete_sentences) + '.'
                
                # ìµœì†Œ ê¸¸ì´ í™•ì¸
                if len(response) >= 20:
                    print(f"âœ… ì •ì œëœ ì‘ë‹µ: {len(response)} ê¸€ì")
                    return response
            
            print("âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ")
            return ""
            
        except Exception as e:
            print(f"âš ï¸ GPT ìƒì„± ì˜¤ë¥˜: {e}")
            return ""

    def _generate_natural_gpt_response(self, question: str, context: str, data_analysis: str = "") -> str:
        """ìì—°ìŠ¤ëŸ¬ìš´ GPT ì‘ë‹µ ìƒì„± - LangChain ìŠ¤íƒ€ì¼"""
        
        # ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_message = """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í•˜ë“¯ì´ ë‹µë³€í•´ì£¼ì„¸ìš”.

ëŒ€í™” ìŠ¤íƒ€ì¼:
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€
- ì´ëª¨ì§€ ì ì ˆíˆ ì‚¬ìš© (ğŸŒ²ğŸ¤–ğŸ“Š ë“±)
- ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì œê³µ
- ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ë§ˆë¬´ë¦¬"""

        # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í¬í•¨
        knowledge_section = ""
        if context and "ì°¸ê³ ìë£Œ" in context:
            knowledge_section = f"\n\n### ì°¸ê³  ì§€ì‹:\n{context}"

        # ë°ì´í„° ë¶„ì„ì´ ìˆìœ¼ë©´ í¬í•¨  
        data_section = ""
        if data_analysis:
            data_section = f"\n\n### í˜„ì¬ ë°ì´í„°:\n{data_analysis}"

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"""{system_message}

{knowledge_section}
{data_section}

### ì‚¬ìš©ì ì§ˆë¬¸:
{question}

### ì „ë¬¸ê°€ ë‹µë³€:
"""

        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(full_prompt)} ê¸€ì")
        
        # í† í°í™” (attention_mask í¬í•¨)
        tokenized = self.tokenizer(
            full_prompt,
            return_tensors='pt',
            max_length=800,
            truncation=True,
            padding=True
        )

        input_ids = tokenized['input_ids']
        attention_mask = tokenized.get('attention_mask', None)

        # pad_token ì„¤ì •(ì—†ëŠ” ê²½ìš° eosë¡œ ëŒ€ì²´)
        if getattr(self.tokenizer, 'pad_token', None) is None:
            try:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            except Exception:
                pass

        # ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=200,
                min_new_tokens=50,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                top_p=0.92,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # ì‘ë‹µ ì¶”ì¶œ ë° ì •ì œ
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°í•˜ê³  ì‘ë‹µë§Œ ì¶”ì¶œ
        if "### ì „ë¬¸ê°€ ë‹µë³€:" in full_response:
            response = full_response.split("### ì „ë¬¸ê°€ ë‹µë³€:")[-1].strip()
        else:
            # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë§Œí¼ ì œê±°
            response = full_response[len(full_prompt):].strip()
        
        # ì‘ë‹µ ì •ì œ
        response = self._clean_gpt_response(response)
        
        print(f"ğŸ” GPT ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response)} ê¸€ì")
        print(f"ğŸ” GPT ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response[:100]}...")
        
        # ìµœì†Œ ê¸¸ì´ í™•ì¸ (ê¸°ì¤€ì„ ë‚®ì¶¤)
        if len(response) < 15:
            print("âš ï¸ GPT ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ, ë°±ì—… ì‚¬ìš©")
            return self._generate_simple_fallback(question, context)
        
        # ì˜ë¯¸ìˆëŠ” ì‘ë‹µì¸ì§€ í™•ì¸
        if response.count(' ') < 5:  # ë‹¨ì–´ê°€ ë„ˆë¬´ ì ìŒ
            print("âš ï¸ GPT ì‘ë‹µì´ ì˜ë¯¸ì—†ìŒ, ë°±ì—… ì‚¬ìš©")
            return self._generate_simple_fallback(question, context)
        
        return response
    
    def _clean_gpt_response(self, response: str) -> str:
        """GPT ì‘ë‹µ ì •ì œ"""
        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±°
        lines = response.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            if len(line) > 5:  # ë„ˆë¬´ ì§§ì€ ì¤„ ì œê±°
                # íŠ¹ìˆ˜ë¬¸ìë¡œë§Œ ì´ë£¨ì–´ì§„ ì¤„ ì œê±°
                if not line.replace('-', '').replace('*', '').replace('=', '').strip():
                    continue
                clean_lines.append(line)
        
        result = '\n'.join(clean_lines)
        
        # ë§ˆì§€ë§‰ì— ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±°
        if result and not result[-1] in '.!?ë‹¤ìš”ë‹ˆê¹Œ':
            sentences = result.split('.')
            if len(sentences) > 1:
                result = '.'.join(sentences[:-1]) + '.'
        
        return result.strip()
    
    def _generate_simple_fallback(self, question: str, context: str) -> str:
        """GPT ì‹¤íŒ¨ ì‹œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜• ì‘ë‹µ"""
        
        if context and len(context.strip()) > 30:
            # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
            clean_context = context.replace('\n', ' ').strip()
            
            # ë„ˆë¬´ ê¸´ ê²½ìš° ìš”ì•½
            if len(clean_context) > 200:
                clean_context = clean_context[:200] + "..."
            
            # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ
            if "í™•ì¸" in question or "ë°©ë²•" in question:
                return f"ì¬ì„ ì¶© í™•ì¸ë°©ë²•ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ë©´, {clean_context} ì´ëŸ° ì •ë³´ê°€ ìˆì–´ìš”. ë” êµ¬ì²´ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”!"
            elif "íƒì§€" in question:
                return f"íƒì§€ ê´€ë ¨í•´ì„œëŠ” {clean_context} ì´ëŸ° ë‚´ìš©ì´ ìˆë„¤ìš”. ì–´ë–¤ ë¶€ë¶„ì„ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
            else:
                return f"{question}ì— ëŒ€í•´ì„œëŠ” {clean_context} ì´ëŸ° ìë£Œê°€ ìˆì–´ìš”. ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!"
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•Œ ì§ˆë¬¸ ìœ í˜•ë³„ ì‘ë‹µ
        if "ì¬ì„ ì¶©" in question:
            return "ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ì€ ë§¤ê°œì¶©ì„ í†µí•´ ì „íŒŒë˜ëŠ” ì§ˆë³‘ì´ì—ìš”. ë“œë¡ ê³¼ AIë¡œ ì¡°ê¸° íƒì§€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ì£ . êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
        elif "íƒì§€" in question or "YOLO" in question:
            return "YOLO ëª¨ë¸ë¡œ ë“œë¡  ì˜ìƒì—ì„œ ê°ì—¼ëª©ì„ ì°¾ì•„ë‚´ê³  ìˆì–´ìš”. ì‹ ë¢°ë„ì— ë”°ë¼ ì¦‰ì‹œ ë°©ì œí•˜ê±°ë‚˜ ì¬ì¡°ì‚¬ë¥¼ í•˜ì£ . ë” ìì„¸íˆ ì•Œê³  ì‹¶ì€ ë¶€ë¶„ì´ ìˆë‚˜ìš”?"
        else:
            return f"'{question}' ê´€ë ¨í•´ì„œ ë„ì›€ë“œë¦´ ìˆ˜ ìˆì–´ìš”! ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ì´ë‚˜ AI íƒì§€ì— ëŒ€í•´ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."

    def _generate_gpt_response(self, question: str, context: str, data_analysis: str = "") -> str:
        """ê°„ë‹¨í•˜ê³  í™•ì‹¤í•œ GPT ì‘ë‹µ ìƒì„±"""
        
        try:
            # ì•„ì£¼ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            if context and len(context.strip()) > 20:
                clean_context = context[:300].replace('\n', ' ')
                prompt = f"ì§ˆë¬¸: {question}\nì°¸ê³ ì •ë³´: {clean_context}\në‹µë³€:"
            else:
                prompt = f"ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\nì§ˆë¬¸: {question}\në‹µë³€:"
            
            print(f"ğŸ¯ ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ (ê¸¸ì´: {len(prompt)})")
            
            # í† í°í™”
            inputs = self.tokenizer(
                prompt,
                return_tensors='pt',
                max_length=600,
                truncation=True
            )
            
            print(f"ğŸ¤– GPT ìƒì„± ì¤‘... (ì…ë ¥í† í°: {inputs['input_ids'].shape[1]})")
            
            # ë‹¨ìˆœí•œ ìƒì„± íŒŒë¼ë¯¸í„°
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs['input_ids'],
                    max_new_tokens=150,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ì‘ë‹µ ì¶”ì¶œ
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if "ë‹µë³€:" in generated_text:
                response = generated_text.split("ë‹µë³€:", 1)[-1].strip()
            else:
                response = generated_text[len(prompt):].strip()
            
            # ê¸°ë³¸ ì •ì œ
            response = response.split('\n')[0]  # ì²« ë²ˆì§¸ ì¤„ë§Œ
            response = response.strip()
            
            print(f"âœ… GPT ì‘ë‹µ: {response[:50]}... (ê¸¸ì´: {len(response)})")
            
            if len(response) < 15:
                print("âš ï¸ ì‘ë‹µ ë„ˆë¬´ ì§§ìŒ, ë°±ì—… ì‚¬ìš©")
                return self._generate_simple_fallback(question, context)
                
            return response
            
        except Exception as e:
            print(f"âŒ GPT ì˜¤ë¥˜: {e}")
            return self._generate_simple_fallback(question, context)

    def _generate_smart_gpt_response(self, question: str, context: str, data_analysis: str = "") -> str:
        """GPTê°€ ì§€ì‹ë² ì´ìŠ¤ë¥¼ í™œìš©í•´ì„œ ì•Œì•„ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€"""
        
        # ì™„ì „ ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸ - ëŒ€í™”í•˜ë“¯ì´
        if context and len(context.strip()) > 20:
            prompt = f"""ë‹¹ì‹ ì€ ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì°¸ê³ ìë£Œ:
{context[:800]}

ì§ˆë¬¸: {question}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
        else:
            prompt = f"""ë‹¹ì‹ ì€ ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ê³¼ ì‚°ë¦¼ë³‘í•´ì¶© ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {question}

ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""

        try:
            inputs = self.tokenizer.encode_plus(
                prompt,
                return_tensors='pt',
                max_length=900,
                truncation=True,
                padding=True
            )
            
            print("ğŸ¤– ìì—°ìŠ¤ëŸ¬ìš´ GPT ì‘ë‹µ ìƒì„± ì¤‘...")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs.get('attention_mask'),
                    max_new_tokens=200,
                    min_new_tokens=60,
                    temperature=0.85,
                    do_sample=True,
                    top_p=0.92,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=3,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            response = full_text.replace(prompt, "").strip()
            
            # ì‘ë‹µ ì •ì œ
            if response.startswith(":"):
                response = response[1:].strip()
            
            # ë„ˆë¬´ ì§§ìœ¼ë©´ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ì•ˆ
            if len(response) < 40:
                return self._generate_natural_fallback(question, context)
                
            return response
            
        except Exception as e:
            print(f"âŒ GPT ì˜¤ë¥˜: {e}")
            return self._generate_natural_fallback(question, context)

    def _generate_natural_fallback(self, question: str, context: str) -> str:
        """GPT ì‹¤íŒ¨ì‹œì—ë„ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€"""
        
        if context and len(context.strip()) > 20:
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€
            context_summary = context[:200]
            return f"{question}ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ë©´,\n\n{context_summary}...\n\nì´ëŸ° ì •ë³´ê°€ ìˆë„¤ìš”! ë” ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš” ğŸ˜Š"
        else:
            # ì¼ë°˜ì ì¸ ë„ì›€ ì œê³µ
            return f"{question}ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œëŠ”êµ°ìš”! ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ì´ë‚˜ AI íƒì§€ ê´€ë ¨í•´ì„œ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ì •í™•í•œ ì •ë³´ë¥¼ ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ì–´ë–¤ ë¶€ë¶„ì´ ê°€ì¥ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
    
    def _build_natural_korean_prompt(self, question: str, context: str, data_analysis: str) -> str:
        """ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ êµ¬ì„± - GPT ëª¨ë¸ì— ìµœì í™”"""
        
        prompt_parts = []
        
        # ê°„ê²°í•˜ê³  ëª…í™•í•œ ì§€ì‹œë¬¸
        prompt_parts.append("ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ì „ë¬¸ê°€ë¡œì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.")
        
        # ì°¸ê³  ìë£Œê°€ ìˆìœ¼ë©´ ìš”ì•½í•´ì„œ í¬í•¨
        if context and len(context.strip()) > 20:
            # ë„ˆë¬´ ê¸´ ì»¨í…ìŠ¤íŠ¸ëŠ” ìš”ì•½
            clean_context = context[:400].replace('\n', ' ').strip()
            prompt_parts.append(f"ì°¸ê³ ìë£Œ: {clean_context}")
        
        # ëª…í™•í•œ ì§ˆë¬¸ê³¼ ì‘ë‹µ í˜•ì‹
        prompt_parts.append(f"ì§ˆë¬¸: {question}")
        prompt_parts.append("ë‹µë³€:")
        
        return "\n".join(prompt_parts)
    
    def _clean_and_validate_response(self, response: str, question: str) -> str:
        """GPT ì‘ë‹µ ì •ì œ ë° ê²€ì¦"""
        
        # ê¸°ë³¸ ì •ì œ
        response = response.strip()
        
        # ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°
        prefixes_to_remove = [
            "ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ì „ë¬¸ê°€ë¡œì„œ",
            "ì°¸ê³ ìë£Œ:",
            "ì§ˆë¬¸:",
            "ë‹µë³€:",
            "ì‚¬ìš©ì ì§ˆë¬¸:",
            "ì „ë¬¸ê°€ ë‹µë³€:",
        ]
        
        for prefix in prefixes_to_remove:
            if response.startswith(prefix):
                response = response[len(prefix):].strip()
        
        # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ ì°¾ê¸°
        lines = response.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            if len(line) > 10 and not line.startswith(('ì°¸ê³ ìë£Œ', 'ì§ˆë¬¸:', 'ë‹µë³€:')):
                clean_lines.append(line)
        
        if clean_lines:
            response = '\n'.join(clean_lines[:3])  # ìµœëŒ€ 3ì¤„
        
        # ì¤‘ë³µ ì œê±°
        if question in response:
            response = response.replace(question, "").strip()
        
        return response
    
    def _generate_template_response(self, question: str, context: str, data_analysis: str) -> str:
        """ì§€ëŠ¥í˜• í…œí”Œë¦¿ ê¸°ë°˜ í•œêµ­ì–´ ì‘ë‹µ ìƒì„±"""
        
        # í‚¤ì›Œë“œ ë¶„ì„
        question_keywords = self._extract_keywords(question.lower())
        context_keywords = self._extract_keywords(context.lower()) if context else []
        
        response_parts = []
        
        # GPT ì—†ì´ í…œí”Œë¦¿ ì‚¬ìš©í•  ë•ŒëŠ” ê°„ë‹¨í•˜ê²Œ
        print("ğŸ“ ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...")
        
        # ì»¨í…ìŠ¤íŠ¸ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µ
        if context and len(context.strip()) > 10:
            response_parts.append("ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤:")
            response_parts.append("")
            
            # ì»¨í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì„œ ì œì‹œ
            context_summary = context[:400].replace("ğŸ“– ì°¸ê³ ìë£Œ", "â€¢").strip()
            response_parts.append(context_summary)
            response_parts.append("")
            
            # ì§ˆë¬¸ì— ë”°ë¥¸ ì¶”ê°€ ì„¤ëª…
            if any(kw in question_keywords for kw in ['yolo', 'ìšœë¡œ', 'ëª¨ë¸', 'íƒì§€']):
                response_parts.append("YOLO ëª¨ë¸ ê¸°ë°˜ íƒì§€ì˜ í•µì‹¬ì€ ì •í™•í•œ ë°ì´í„°ì…‹ êµ¬ì¶•ê³¼ ì ì ˆí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì…ë‹ˆë‹¤.")
            elif any(kw in question_keywords for kw in ['ì¦ìƒ', 'íŠ¹ì§•']):
                response_parts.append("ì´ˆê¸° ì¦ìƒì„ ì •í™•íˆ íŒŒì•…í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì¸ ë°©ì œì˜ ì²«ê±¸ìŒì…ë‹ˆë‹¤.")
            elif any(kw in question_keywords for kw in ['ë°©ì œ', 'ì²˜ë¦¬']):
                response_parts.append("ì‹ ì†í•œ ì´ˆê¸° ëŒ€ì‘ì´ í™•ì‚° ë°©ì§€ì˜ í•µì‹¬ì…ë‹ˆë‹¤.")
            
        else:
            response_parts.append("ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            response_parts.append("ì˜ˆ: 'YOLO ëª¨ë¸ í•™ìŠµ ë°©ë²•', 'ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ì¦ìƒ', 'ë°©ì œ ì²˜ë¦¬ ì ˆì°¨' ë“±")
        
        return "\n".join(response_parts) if response_parts else "ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        
        # ë°ì´í„° ìš”ì•½
        if data_analysis:
            response_parts.append(f"ğŸ“Š **í˜„ì¬ ìƒí™©:** {data_analysis}")
            response_parts.append("")
        
        # ë§¥ë½ ê¸°ë°˜ ìƒì„¸ ë¶„ì„
        if context:
            if "ì‹ ë¢°ë„" in context and any(char.isdigit() for char in context):
                # ìˆ«ì ì¶”ì¶œ
                numbers = [int(s) for s in context.split() if s.isdigit()]
                if numbers:
                    confidence = max(numbers)
                    if confidence >= 80:
                        response_parts.append("âœ… **í‰ê°€:** ë§¤ìš° ìš°ìˆ˜í•œ íƒì§€ ê²°ê³¼ì…ë‹ˆë‹¤.")
                        response_parts.append("â€¢ ë†’ì€ ì‹ ë¢°ë„ë¡œ ì¦‰ì‹œ ë°©ì œ ì¡°ì¹˜ ê¶Œì¥")
                    elif confidence >= 60:
                        response_parts.append("âš ï¸ **í‰ê°€:** ì–‘í˜¸í•œ íƒì§€ ê²°ê³¼ì…ë‹ˆë‹¤.")
                        response_parts.append("â€¢ ì¶”ê°€ í™•ì¸ í›„ ë°©ì œ ì¡°ì¹˜ ì‹¤ì‹œ")
                    else:
                        response_parts.append("ğŸ” **í‰ê°€:** ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                        response_parts.append("â€¢ ì¬íƒì§€ ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²• í™œìš© ê¶Œì¥")
                    response_parts.append("")
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ë‹µë³€
        if any(kw in question_keywords for kw in ['ê°œì„ ', 'í–¥ìƒ', 'ì¢‹ê²Œ', 'ë†’ì´']):
            response_parts.append("ğŸ¯ **ì •í™•ë„ ê°œì„  ë°©ì•ˆ:**")
            response_parts.append("1. **ë°ì´í„° í’ˆì§ˆ í–¥ìƒ**")
            response_parts.append("   â€¢ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì‚¬ìš©")
            response_parts.append("   â€¢ ë‹¤ì–‘í•œ ì´¬ì˜ ê°ë„ í™•ë³´")
            response_parts.append("2. **ëª¨ë¸ ì¬í•™ìŠµ**")
            response_parts.append("   â€¢ ì¶”ê°€ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘")
            response_parts.append("   â€¢ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
            response_parts.append("3. **ì•™ìƒë¸” ê¸°ë²• í™œìš©**")
            response_parts.append("   â€¢ ì—¬ëŸ¬ ëª¨ë¸ ê²°ê³¼ ì¡°í•©")
            response_parts.append("")
            response_parts.append("â€¢ ì‹ ë¢°ë„ 40-70%: ì¶”ê°€ í˜„ì¥ í™•ì¸ í›„ ë°©ì œ")
            response_parts.append("â€¢ ì‹ ë¢°ë„ 40% ë¯¸ë§Œ: ì¬íƒì§€ ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²• í™œìš©")
            response_parts.append("")
        
        if any(kw in question_keywords for kw in ['ë°©ì œ', 'ì²˜ë¦¬', 'ëŒ€ì‘']):
            response_parts.append("ğŸš¨ **ë°©ì œ ì²˜ë¦¬ ë°©ì•ˆ:**")
            response_parts.append("â€¢ ê°ì—¼ëª© ì¦‰ì‹œ ë²Œì±„ ë° ë°˜ì¶œ")
            response_parts.append("â€¢ ì£¼ë³€ 500m ë°˜ê²½ ì˜ˆë°© ì‚´í¬")
            response_parts.append("â€¢ GPS ì¢Œí‘œ ê¸°ë°˜ ì²´ê³„ì  ê´€ë¦¬")
            response_parts.append("")
        
        if any(kw in question_keywords for kw in ['í™•ì‚°', 'ì „íŒŒ', 'ì˜ˆë°©']):
            response_parts.append("ğŸ›¡ï¸ **í™•ì‚° ë°©ì§€ ëŒ€ì±…:**")
            response_parts.append("â€¢ ë§¤ê°œì¶© í™œë™ ì‹œê¸° ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§")
            response_parts.append("â€¢ ê°ì—¼ëª© ìš´ë°˜ ê²½ë¡œ ì°¨ë‹¨")
            response_parts.append("â€¢ ì£¼ë³€ ì§€ì—­ ì˜ˆë°© ì¡°ì¹˜ ê°•í™”")
            response_parts.append("")
        
        if context and "ì°¸ê³ ìë£Œ" in context:
            response_parts.append("ğŸ“š **ì „ë¬¸ ì§€ì‹ ì°¸ê³ :**")
            response_parts.append("ìœ„ ë¶„ì„ì€ ì‚°ë¦¼ì²­ ë°©ì œ ê°€ì´ë“œë¼ì¸ê³¼ ìƒíƒœí•™ì  íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(response_parts) if response_parts else "ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
    
    def _postprocess_korean_response(self, response: str) -> str:
        """í•œêµ­ì–´ ì‘ë‹µ í›„ì²˜ë¦¬"""
        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±°
        sentences = response.split('.')
        complete_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and not sentence.endswith(('í•˜ê³ ', 'í•˜ì—¬', 'í•˜ë©´', 'í•˜ëŠ”')):
                complete_sentences.append(sentence)
        
        if complete_sentences:
            result = '. '.join(complete_sentences)
            if not result.endswith('.'):
                result += '.'
            return result
        
        return response[:200] + "..." if len(response) > 200 else response

    def _generate_smart_gpt_response(self, question: str, context: str, data_analysis: str = "") -> str:
        """GPTê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±"""
        
        try:
            # GPTê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸
            if context and len(context.strip()) > 30:
                # ì‹¤ì œ ì§€ì‹ ë‚´ìš©ë§Œ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ì œê±°)
                clean_context = context.replace('ğŸ“– ì°¸ê³ ìë£Œ', '').replace('#', '').replace('*', '')
                clean_context = ' '.join([line.strip() for line in clean_context.split('\n') if len(line.strip()) > 10])[:500]
                
                prompt = f"""ë‹¤ìŒì€ ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ì— ëŒ€í•œ ì „ë¬¸ ìë£Œì…ë‹ˆë‹¤:
{clean_context}

ì§ˆë¬¸: {question}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ê°€ë‹µê²Œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
            else:
                prompt = f"""ë‹¹ì‹ ì€ ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì§ˆë¬¸: {question}
ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
            
            print(f"ğŸ¯ GPT í”„ë¡¬í”„íŠ¸ ì¤€ë¹„: {len(prompt)} ê¸€ì")
            
            # ê°„ë‹¨í•œ í† í°í™”
            inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=900, truncation=True)
            
            print(f"ğŸ”¥ GPT ìƒì„± ì‹œì‘...")
            
            # ë§¤ìš° ì•ˆì „í•œ ìƒì„± íŒŒë¼ë¯¸í„° (NaN/inf ë°©ì§€)
            with torch.no_grad():
                try:
                    # ì²« ë²ˆì§¸ ì‹œë„: ê·¸ë¦¬ë”” ë””ì½”ë”© (ê°€ì¥ ì•ˆì „)
                    outputs = self.model.generate(
                        inputs,
                        max_length=inputs.shape[1] + 100,
                        do_sample=False,  # ê·¸ë¦¬ë”” ë°©ì‹
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        num_return_sequences=1
                    )
                except:
                    # ë‘ ë²ˆì§¸ ì‹œë„: ë§¤ìš° ë³´ìˆ˜ì ì¸ ìƒ˜í”Œë§
                    outputs = self.model.generate(
                        inputs,
                        max_length=inputs.shape[1] + 80,
                        temperature=1.0,  # ê¸°ë³¸ ì˜¨ë„
                        do_sample=True,
                        top_k=50,  # top_p ëŒ€ì‹  top_k ì‚¬ìš©
                        pad_token_id=self.tokenizer.eos_token_id,
                        num_return_sequences=1
                    )
            
            # ì „ì²´ ìƒì„± í…ìŠ¤íŠ¸
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ì›ë˜ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if len(generated_text) > len(prompt):
                answer = generated_text[len(prompt):].strip()
            else:
                answer = generated_text.strip()
            
            print(f"âœ¨ GPT ìƒì„± ì™„ë£Œ: '{answer[:50]}...'")
            
            # ì˜ë¯¸ìˆëŠ” ë‹µë³€ì¸ì§€ í™•ì¸
            if len(answer) > 20 and answer.count(' ') > 3:
                return answer[:300]  # ìµœëŒ€ 300ê¸€ìë¡œ ì œí•œ
            
        except Exception as e:
            print(f"ğŸ’¥ GPT ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        return None
    
    def _clean_smart_response(self, response: str) -> str:
        """ë˜‘ë˜‘í•œ ì‘ë‹µ ì •ë¦¬"""
        # ì›ë³¸ ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        response = response.strip()
        
        # í”„ë¡¬í”„íŠ¸ ì”ì—¬ë¬¼ ì œê±°
        unwanted_patterns = [
            'ğŸ“– ì°¸ê³ ìë£Œ',
            'ê´€ë ¨ ì „ë¬¸ ìë£Œ:',
            'ì§ˆë¬¸:',
            'ë‹µë³€:',
            'ìœ„ ìë£Œë¥¼ ì°¸ê³ í•´ì„œ',
            'ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ',
            '#',
            '##'
        ]
        
        lines = response.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            # ë¶ˆí•„ìš”í•œ íŒ¨í„´ì´ í¬í•¨ëœ ì¤„ ì œê±°
            skip_line = False
            for pattern in unwanted_patterns:
                if pattern in line:
                    skip_line = True
                    break
            
            if not skip_line and len(line) > 10:
                clean_lines.append(line)
                if len(clean_lines) >= 3:  # ìµœëŒ€ 3ì¤„ë¡œ ì œí•œ
                    break
        
        if clean_lines:
            result = ' '.join(clean_lines)
            return result[:300] if len(result) > 300 else result
        else:
            # ëª¨ë“  ì¤„ì´ í•„í„°ë§ë˜ë©´ ì›ë³¸ì˜ ì˜ë¯¸ìˆëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œ
            words = response.split()
            meaningful_words = [w for w in words if len(w) > 2 and not w.startswith(('#', 'ğŸ“–'))]
            return ' '.join(meaningful_words[:30]) if meaningful_words else response[:100]
    
    def _generate_intelligent_answer(self, question: str, context: str) -> str:
        """ì§€ì‹ë² ì´ìŠ¤ë¥¼ ì´ìš©í•´ GPTì²˜ëŸ¼ ë˜‘ë˜‘í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±"""
        
        if not context or len(context.strip()) < 20:
            return f"{question}ì— ëŒ€í•œ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘, YOLO íƒì§€, ë“œë¡  ì´¬ì˜ ë“±ì— ëŒ€í•´ì„œ ì§ˆë¬¸í•´ë³´ì‹œë©´ ë„ì›€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
        useful_content = self._extract_smart_content(context, question)
        
        if not useful_content:
            return f"{question}ì— ëŒ€í•œ ê´€ë ¨ ìë£ŒëŠ” ìˆì§€ë§Œ, ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”!"
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ êµ¬ì„±
        return self._compose_natural_answer(question, useful_content)
    
    def _extract_smart_content(self, context: str, question: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ìœ ìš©í•œ ë‚´ìš©ë§Œ ê°„ë‹¨í•˜ê²Œ ì¶”ì¶œ"""
        
        # ë©”íƒ€ë°ì´í„° ì œê±° íŒ¨í„´
        clean_text = context
        for pattern in ['ğŸ“–', 'ì°¸ê³ ìë£Œ', 'ëŒ€í‘œ ì²­í¬:', 'ì ìˆ˜', 'filepath:', '#', '*']:
            clean_text = clean_text.replace(pattern, '')
        
        # ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë“¤ë§Œ ì¶”ì¶œ
        lines = [line.strip() for line in clean_text.split('\n') if line.strip()]
        useful_lines = []
        
        for line in lines:
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ë©”íƒ€ë°ì´í„°ì¸ ì¤„ ì œì™¸
            if len(line) < 15 or line.startswith('http') or '.pdf' in line or '.jpg' in line:
                continue
            
            # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œê°€ ìˆëŠ” ì¤„ ìš°ì„ 
            if question and any(keyword in line.lower() for keyword in question.lower().split()):
                useful_lines.insert(0, line)
            else:
                useful_lines.append(line)
            
            # ìµœëŒ€ 3ê°œ ë¬¸ì¥ë§Œ
            if len(useful_lines) >= 3:
                break
        
        if useful_lines:
            result = '. '.join(useful_lines[:2])  # ìµœëŒ€ 2ê°œ ë¬¸ì¥
            return result[:300] + ('...' if len(result) > 300 else '')
        
        return context[:200] + '...' if len(context) > 200 else context
    
    def _compose_natural_answer(self, question: str, content: str) -> str:
        """ì§ˆë¬¸ê³¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ êµ¬ì„±"""
        
        # [í•˜ë“œì½”ë”© ì™„ì „ ì œê±° - LLMì´ ììœ ë¡­ê²Œ ë‹µë³€]
        # ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ - LangChainì´ ìë™ ì²˜ë¦¬
        return content[:200] if content else "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def _extract_real_content(self, context: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ìœ ìš©í•œ ë‚´ìš©ë§Œ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ì œê±°)"""
        
        # ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° íŒ¨í„´ë“¤
        skip_patterns = [
            'ğŸ“– ì°¸ê³ ìë£Œ', '# ë¬´ì¸í•­ê³µê¸°ë¥¼', '* í•œêµ­êµí†µì—°êµ¬ì›', 
            '** ì¢…ì‹ íšŒì›', '.pdf', '.jpg', '.png', 'Fig.', 'Table.'
        ]
        
        # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì„œ ì‹¤ì œ ë‚´ìš©ë§Œ ì¶”ì¶œ
        lines = context.split('\n')
        content_lines = []
        
        for line in lines:
            line = line.strip()
            
            # ë©”íƒ€ë°ì´í„° ì¤„ ê±´ë„ˆë›°ê¸°
            if any(skip in line for skip in skip_patterns):
                continue
                
            # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ” ì¤„ë§Œ ì„ íƒ
            if len(line) > 15:
                content_lines.append(line)
                if len(content_lines) >= 4:  # ìµœëŒ€ 4ì¤„
                    break
        
        if content_lines:
            return '\n'.join(content_lines)
        else:
            # ì¤„ ê¸°ì¤€ìœ¼ë¡œ ì•ˆ ë˜ë©´ ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ ì‹œë„
            sentences = context.replace('\n', ' ').split('.')
            for sentence in sentences[:3]:  # ì²˜ìŒ 3ê°œ ë¬¸ì¥ë§Œ
                sentence = sentence.strip()
                if len(sentence) > 20 and not any(skip in sentence for skip in skip_patterns):
                    return sentence + '.'
            
            return "ê´€ë ¨ ì „ë¬¸ ìë£Œì˜ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤."

    def _generate_gpt_knowledge_response(self, question: str, context: str) -> str:
        """GPT ëª¨ë¸ ìì²´ì˜ ì§€ì‹ì„ í™œìš©í•œ ë‹µë³€ ìƒì„±"""
        
        try:
            # GPTê°€ ìì²´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í”„ë¡¬í”„íŠ¸
            if context and len(context.strip()) > 50:
                # ì§€ì‹ë² ì´ìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ë³´ì™„í•´ì„œ ë‹µë³€
                prompt = f"""ì§ˆë¬¸: {question}

ì°¸ê³  ì •ë³´ê°€ ìˆì§€ë§Œ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì§€ì‹ì„ í™œìš©í•´ì„œ ì´ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
            else:
                # ì§€ì‹ë² ì´ìŠ¤ì— ì—†ìœ¼ë©´ GPT ìì²´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€
                prompt = f"""ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
            
            print(f"ğŸ¯ GPT ìì²´ ì§€ì‹ í™œìš© ì¤‘...")
            
            # ê°„ë‹¨í•œ í† í°í™”
            inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=700, truncation=True)
            
            # ì•ˆì „í•œ ìƒì„±
            with torch.no_grad():
                try:
                    outputs = self.model.generate(
                        inputs,
                        max_length=inputs.shape[1] + 100,
                        temperature=0.7,
                        do_sample=True,
                        top_k=50,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                except:
                    # ë” ì•ˆì „í•œ ê·¸ë¦¬ë”” ë°©ì‹
                    outputs = self.model.generate(
                        inputs,
                        max_length=inputs.shape[1] + 80,
                        do_sample=False,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
            
            # ì‘ë‹µ ì¶”ì¶œ
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            if len(generated_text) > len(prompt):
                answer = generated_text[len(prompt):].strip()
            else:
                return None
            
            # ì˜ë¯¸ìˆëŠ” ë‹µë³€ì¸ì§€ í™•ì¸ (ë°˜ë³µë¬¸ì ì œê±°)
            if len(answer) > 30 and not ('!!!' in answer or answer.count(answer[0]) > 10):
                # ë‹µë³€ ì •ë¦¬ (ìµœëŒ€ 300ê¸€ì)
                if len(answer) > 300:
                    answer = answer[:300] + "..."
                return answer
                
        except Exception as e:
            print(f"ğŸ’¥ GPT ì§€ì‹ í™œìš© ì˜¤ë¥˜: {e}")
        
        return None

    def _generate_pure_llm_response(self, question: str, context: str) -> str:
        """ìˆœìˆ˜í•˜ê²Œ LLM ìì²´ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€ ìƒì„± (í•˜ë“œì½”ë”© ì—†ìŒ)"""
        
        if not self.model or not self.tokenizer:
            return f"{question}ì— ëŒ€í•œ ë‹µë³€ì„ ìœ„í•´ ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        try:
            # ìˆœìˆ˜ LLM ë‹µë³€ì„ ìœ„í•œ ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸
            if context and len(context.strip()) > 50:
                prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {question}

ê´€ë ¨ ì°¸ê³  ì •ë³´:
{context[:400]}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
            else:
                prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
            
            print(f"ğŸ¯ ìˆœìˆ˜ LLM í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
            
            # í† í°í™”
            inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=800, truncation=True)
            
            # ì•ˆì „í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 200,
                    min_length=inputs.shape[1] + 50,
                    temperature=0.8,
                    do_sample=True,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=3,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ì‘ë‹µ ì¶”ì¶œ
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ìˆœìˆ˜ ë‹µë³€ë§Œ ì¶”ì¶œ
            if len(generated_text) > len(prompt):
                answer = generated_text[len(prompt):].strip()
            else:
                return self._generate_fallback_response(question, context)
            
            # ë‹µë³€ í’ˆì§ˆ ê²€ì¦
            if len(answer) > 30 and self._is_valid_response(answer):
                return answer[:500]  # ìµœëŒ€ 500ê¸€ì
            else:
                return self._generate_fallback_response(question, context)
                
        except Exception as e:
            print(f"ğŸ’¥ LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return self._generate_fallback_response(question, context)
    
    def _is_valid_response(self, response: str) -> bool:
        """ìƒì„±ëœ ì‘ë‹µì´ ìœ íš¨í•œì§€ ê²€ì¦"""
        # ë°˜ë³µ ë¬¸ìë‚˜ ë¬´ì˜ë¯¸í•œ í† í° ì²´í¬
        if response.count('!') > 5 or response.count('?') > 3:
            return False
        if any(char * 10 in response for char in 'abcdefghijklmnopqrstuvwxyz'):
            return False
        return True
    
    def _generate_fallback_response(self, question: str, context: str) -> str:
        """LLM ì‹¤íŒ¨ì‹œ ìµœì†Œí•œì˜ ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µ"""
        if context and len(context.strip()) > 30:
            clean_context = self._extract_smart_content(context, question)
            if clean_context:
                return f"{question}ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n{clean_context}\n\në” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!"
        
        return f"{question}ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê±°ë‚˜, ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"

    def _generate_pure_gpt_response(self, question: str, context: str) -> str:
        """ìˆœìˆ˜ GPT ëª¨ë¸ ì§€ì‹ë§Œ í™œìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„± (í•˜ë“œì½”ë”© ì—†ìŒ)"""
        
        try:
            # ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ëŒ€í™” í”„ë¡¬í”„íŠ¸ (ì»¨í…ìŠ¤íŠ¸ í™œìš©)
            if context and len(context.strip()) > 30:
                # ì§€ì‹ë² ì´ìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ì°¸ê³ í•´ì„œ ë‹µë³€
                clean_context = context.replace('ğŸ“–', '').replace('#', '').strip()[:300]
                prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {question}
                
ì°¸ê³  ìë£Œ: {clean_context}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì°¸ê³  ìë£Œì™€ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ì¢…í•©í•´ì„œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

ë‹µë³€:"""
            else:
                # ì§€ì‹ë² ì´ìŠ¤ ì •ë³´ê°€ ì—†ìœ¼ë©´ GPT ìì²´ ì§€ì‹ìœ¼ë¡œë§Œ ë‹µë³€
                prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”:

ë‹µë³€:"""
            
            print(f"ğŸ¯ ìˆœìˆ˜ GPT ë‹µë³€ ìƒì„± ì‹œì‘...")
            
            # í† í°í™”
            inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=900, truncation=True)
            
            # ë” ë‚˜ì€ ìƒì„± íŒŒë¼ë¯¸í„° (í•œêµ­ì–´ ì¹œí™”ì )
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 200,  # ë” ê¸´ ë‹µë³€ í—ˆìš©
                    temperature=0.8,  # ì°½ì˜ì  ë‹µë³€
                    do_sample=True,
                    top_p=0.9,
                    top_k=40,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ì „ì²´ ìƒì„± í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if len(full_response) > len(prompt):
                answer = full_response[len(prompt):].strip()
            else:
                return None
            
            # ì‘ë‹µ í’ˆì§ˆ í™•ì¸
            if len(answer) > 30 and self._is_meaningful_response(answer):
                return answer[:500]  # ìµœëŒ€ 500ìë¡œ ì œí•œ
                
        except Exception as e:
            print(f"ğŸ’¥ ìˆœìˆ˜ GPT ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        
        return None
    
    def _generate_langchain_response(self, question: str, context: str) -> str:
        """LangChainì„ í™œìš©í•œ í•œêµ­ì–´ ìì—° ë‹µë³€ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (í•œêµ­ì–´ ëŒ€í™”í˜•)
        if context and len(context.strip()) > 30:
            # ì§€ì‹ë² ì´ìŠ¤ ì •ë³´ í™œìš©
            prompt_template = PromptTemplate(
                input_variables=["question", "context"],
                template="""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì°¸ê³  ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì°¸ê³  ì •ë³´ì™€ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”. ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì–´ì¡°ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

ë‹µë³€:"""
            )
        else:
            # GPT ìì²´ ì§€ì‹ë§Œ í™œìš©
            prompt_template = PromptTemplate(
                input_variables=["question"],
                template="""ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”. ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì–´ì¡°ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

ë‹µë³€:"""
            )
        
        try:
            # LangChain ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ ì²´ì¸ ìƒì„±
            chain = prompt_template | self.langchain_llm
            
            # ë‹µë³€ ìƒì„±
            if context and len(context.strip()) > 30:
                clean_context = context.replace('ğŸ“–', '').replace('#', '').strip()[:400]
                response = chain.invoke({"question": question, "context": clean_context})
            else:
                response = chain.invoke({"question": question})
            
            # ì‘ë‹µ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°)
            if response:
                response = str(response).strip()
                # í”„ë¡¬í”„íŠ¸ ë°˜ë³µ ì œê±°
                if "ë‹µë³€:" in response:
                    response = response.split("ë‹µë³€:")[-1].strip()
                
                # ì˜ë¯¸ìˆëŠ” ì‘ë‹µì¸ì§€ í™•ì¸
                if len(response) > 30 and self._is_meaningful_response(response):
                    return response[:600]  # ìµœëŒ€ 600ìë¡œ ì œí•œ
                    
        except Exception as e:
            print(f"ğŸ’¥ LangChain ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        
        return None
    
    def _create_natural_gpt_response(self, question: str, context: str) -> str:
        """GPTì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ëŠ¥ë ¥ê³¼ ì „ë¬¸ì§€ì‹ì„ í™œìš©í•œ ì‘ë‹µ ìƒì„±"""
        
        # ì§ˆë¬¸ì— ë”°ë¼ GPTê°€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•  ìˆ˜ ìˆë„ë¡ ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if context and len(context.strip()) > 30:
            conversation_prompt = f"""ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AIì™€ ì‚°ë¦¼ë³‘í•´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

"{question}"

ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì œê°€ ì•Œê³  ìˆëŠ” ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…ë“œë¦´ê²Œìš”. 
ì°¸ê³ ë¡œ ê´€ë ¨ ìë£Œì—ì„œëŠ” ì´ëŸ° ë‚´ìš©ì´ ìˆë„¤ìš”: {context[:200]}...

ìì„¸íˆ ì„¤ëª…ë“œë¦¬ë©´:"""
        else:
            conversation_prompt = f"""ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AIì™€ ì‚°ë¦¼ë³‘í•´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

"{question}"

ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì œê°€ ì•Œê³  ìˆëŠ” ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê²Œ ì„¤ëª…ë“œë¦´ê²Œìš”:"""
        
        # ë” ììœ ë¡œìš´ ìƒì„±ì„ ìœ„í•œ ì‹œë„
        try:
            inputs = self.tokenizer.encode(conversation_prompt, return_tensors='pt', max_length=400, truncation=True)
            attention_mask = (inputs != self.tokenizer.pad_token_id).long()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_length=inputs.shape[1] + 300,
                    temperature=1.0,  # ë” ì°½ì˜ì 
                    do_sample=True,
                    top_p=0.95,
                    top_k=50,
                    repetition_penalty=1.3,
                    pad_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3
                )
            
            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  GPT ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "ì„¤ëª…ë“œë¦¬ë©´:" in full_text:
                answer = full_text.split("ì„¤ëª…ë“œë¦¬ë©´:")[-1].strip()
            elif "ì„¤ëª…ë“œë¦´ê²Œìš”:" in full_text:
                answer = full_text.split("ì„¤ëª…ë“œë¦´ê²Œìš”:")[-1].strip()
            else:
                answer = full_text[len(conversation_prompt):].strip()
            
            # ë‹µë³€ í’ˆì§ˆ í™•ì¸
            if len(answer) > 50 and any(ending in answer for ending in ['ë‹ˆë‹¤', 'ì–´ìš”', 'ìŠµë‹ˆë‹¤', 'ì—ìš”']):
                return f"ğŸ¤– **ì „ë¬¸ê°€ ë‹µë³€:**\n\n{answer}"
            
        except Exception as e:
            logger.warning(f"ìì—°ìŠ¤ëŸ¬ìš´ GPT ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return None
    
    def _langchain_free_response(self, question: str, context: str = "") -> str:
        """ì§€ì‹ë² ì´ìŠ¤ ê¸°ë°˜ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±"""
        # API í‚¤ ì—†ì´ ì§€ì‹ë² ì´ìŠ¤ë§Œìœ¼ë¡œ ë‹µë³€
        return self._extract_natural_answer(question, context)
    
    def _generate_free_llm_response(self, question: str, context: str = "") -> str:
        """LLMì´ ì™„ì „íˆ ììœ ë¡­ê²Œ ë‹µë³€ ìƒì„± - í•˜ë“œì½”ë”© ì ˆëŒ€ ê¸ˆì§€"""
        
        # ë§¤ìš° ì‹¬í”Œí•œ ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ - LLMì´ ìê¸° ì§€ì‹ìœ¼ë¡œ ììœ ë¡­ê²Œ ë‹µë³€
        if context and len(context.strip()) > 20:
            # ì§€ì‹ë² ì´ìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ì°¸ê³ ë§Œ
            prompt = f"""### ì‚¬ìš©ì ì§ˆë¬¸:
{question}

### ì°¸ê³  ìë£Œ (ì„ íƒì ):
{context[:400]}

### AI ë‹µë³€:
"""
        else:
            # ì§€ì‹ë² ì´ìŠ¤ ì—†ìœ¼ë©´ LLM ìì²´ ì§€ì‹ë§Œ
            prompt = f"""### ì‚¬ìš©ì ì§ˆë¬¸:
{question}

### AI ë‹µë³€:
"""
        
        try:
            print("ğŸ¤– LLM ììœ  ë‹µë³€ ìƒì„± ì¤‘...")
            
            # í† í°í™” (CPUì—ì„œ ì‹¤í–‰)
            inputs = self.tokenizer(
                prompt,
                return_tensors='pt',
                max_length=600,
                truncation=True,
                padding=True
            ).to('cpu')  # CPU ëª…ì‹œ
            
            # ììœ ë¡œìš´ ìƒì„± ì„¤ì •
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs['input_ids'].to('cpu'),
                    attention_mask=inputs.get('attention_mask').to('cpu') if inputs.get('attention_mask') is not None else None,
                    max_new_tokens=250,
                    min_new_tokens=30,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.92,
                    top_k=50,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3,
                    pad_token_id=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ë‹µë³€ ì¶”ì¶œ
            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            if "### AI ë‹µë³€:" in generated:
                answer = generated.split("### AI ë‹µë³€:")[-1].strip()
            elif len(generated) > len(prompt):
                answer = generated[len(prompt):].strip()
            else:
                answer = generated.strip()
            
            # ê¸°ë³¸ ì •ì œ
            if answer and len(answer) > 20:
                # ë„ˆë¬´ ê¸´ ë‹µë³€ì€ ì ì ˆíˆ ìë¥´ê¸°
                if len(answer) > 500:
                    sentences = answer.split('.')
                    answer = '. '.join(sentences[:5]) + '.'
                
                print(f"âœ… LLM ë‹µë³€ ì„±ê³µ ({len(answer)}ì)")
                return answer
            
        except Exception as e:
            print(f"âŒ LLM ìƒì„± ì˜¤ë¥˜: {e}")
        
        # ì‹¤íŒ¨ì‹œ ì§€ì‹ë² ì´ìŠ¤ë§Œ ë°˜í™˜
        return context[:300] if context else "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_smart_answer_with_gpt_knowledge(self, question: str, context: str) -> str:
        """[DEPRECATED] GPTì˜ ìì²´ ì§€ì‹ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì „ë¬¸ì ì¸ ë‹µë³€ ìƒì„± - í•˜ë“œì½”ë”© ì—†ì´"""
        
        # GPTê°€ ê°€ì§„ ì „ë¬¸ ì§€ì‹ì„ ììœ ë¡­ê²Œ í™œìš©í•˜ë„ë¡ í•˜ëŠ” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ AI, ì»´í“¨í„° ë¹„ì „, ì‚°ë¦¼ë³‘í•´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìì„¸í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”. 
êµ¬ì²´ì ì¸ ë°©ë²•, ë‹¨ê³„, ì›ë¦¬ ë“±ì„ í¬í•¨í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
        
        try:
            # attention maskì™€ í•¨ê»˜ í† í°í™”
            encoding = self.tokenizer(
                prompt, 
                return_tensors='pt', 
                max_length=700, 
                truncation=True,
                padding=True
            )
            
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=encoding['input_ids'],
                    attention_mask=encoding['attention_mask'],
                    max_length=encoding['input_ids'].shape[1] + 200,
                    temperature=0.8,
                    do_sample=True,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ì „ì²´ ìƒì„± í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
            if "ë‹µë³€:" in full_response:
                answer = full_response.split("ë‹µë³€:")[-1].strip()
            elif len(full_response) > len(prompt):
                answer = full_response[len(prompt):].strip()
            else:
                return None
            
            # ì‘ë‹µ í’ˆì§ˆ í™•ì¸ ë° ì •ë¦¬
            if len(answer) > 50 and self._is_meaningful_response(answer):
                # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì œê±°
                sentences = answer.split('.')
                complete_sentences = []
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 20 and not sentence.endswith('...'):
                        complete_sentences.append(sentence)
                    if len(complete_sentences) >= 4:  # ìµœëŒ€ 4ë¬¸ì¥
                        break
                
                if complete_sentences:
                    return '. '.join(complete_sentences) + '.'
                else:
                    return answer[:400]  # ìµœëŒ€ 400ì
                    
        except Exception as e:
            print(f"ğŸ’¥ GPT ì „ë¬¸ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        
        return None
    
    def _is_meaningful_response(self, response: str) -> bool:
        """ì˜ë¯¸ìˆëŠ” ì‘ë‹µì¸ì§€ í™•ì¸ (ë°˜ë³µ ë¬¸ì, ë¬´ì˜ë¯¸í•œ í† í° ì œê±°)"""
        # ê°™ì€ ë¬¸ìë‚˜ í† í°ì˜ ê³¼ë„í•œ ë°˜ë³µ í™•ì¸
        if len(set(response.replace(' ', ''))) < 5:  # ë„ˆë¬´ ì ì€ ì¢…ë¥˜ì˜ ë¬¸ì
            return False
        
        # ë¬´ì˜ë¯¸í•œ ë°˜ë³µ íŒ¨í„´ í™•ì¸
        if '!!!' in response or '???' in response:
            return False
            
        # íŠ¹ì • ë¬¸ìê°€ ê³¼ë„í•˜ê²Œ ë°˜ë³µë˜ëŠ”ì§€ í™•ì¸
        for char in response:
            if response.count(char) > len(response) * 0.3:  # 30% ì´ìƒ ê°™ì€ ë¬¸ì
                return False
        
        return True
    
    def _generate_natural_korean_response(self, question: str, context: str) -> str:
        """í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ì„ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„± (í•˜ë“œì½”ë”© ì—†ìŒ)"""
        
        # í•œêµ­ì–´ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ëª¨ë¸ì´ ììœ ë¡­ê²Œ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡
        if context and len(context.strip()) > 30:
            prompt = f"""ì‚¬ìš©ì: {question}

ì°¸ê³  ì •ë³´: {context[:300]}

AI: ë„¤, ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. """
        else:
            prompt = f"""ì‚¬ìš©ì: {question}

AI: ë„¤, ê·¸ ì§ˆë¬¸ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. """
        
        # ì—¬ëŸ¬ í•œêµ­ì–´ ìƒì„± ë°©ë²• ì‹œë„
        methods = [
            self._try_korean_specialized_model,
            self._try_langchain_korean_generation,
            self._try_improved_korean_generation
        ]
        
        for method in methods:
            try:
                result = method(prompt, question, context)
                if result and len(result.strip()) > 30 and self._is_valid_korean_response(result):
                    return result
            except Exception as e:
                print(f"âš ï¸ í•œêµ­ì–´ ìƒì„± ë°©ë²• ì‹¤íŒ¨: {e}")
                continue
        
        return None
    
    def _try_korean_specialized_model(self, prompt: str, question: str, context: str) -> str:
        """í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±"""
        if not self.model or not self.tokenizer:
            return None
            
        try:
            # í•œêµ­ì–´ ëª¨ë¸ì— ìµœì í™”ëœ ìƒì„± ì„¤ì •
            encoding = self.tokenizer(
                prompt,
                return_tensors='pt',
                max_length=800,
                truncation=True,
                padding=True
            )
            
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=encoding['input_ids'],
                    attention_mask=encoding['attention_mask'],
                    max_length=encoding['input_ids'].shape[1] + 300,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # AI ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "AI: " in generated_text:
                answer = generated_text.split("AI: ")[-1].strip()
                if answer.startswith("ë„¤, "):
                    answer = answer[3:]  # "ë„¤, " ì œê±°
                return answer
                
        except Exception as e:
            print(f"í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return None
    
    def _try_langchain_korean_generation(self, prompt: str, question: str, context: str) -> str:
        """LangChainì„ í†µí•œ í•œêµ­ì–´ ë‹µë³€ ìƒì„±"""
        if not self.langchain_llm:
            return None
            
        try:
            # ê°„ë‹¨í•œ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸
            simple_prompt = f"{question}ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
            
            response = self.langchain_llm.invoke(simple_prompt)
            if response and len(response.strip()) > 20:
                return response.strip()
                
        except Exception as e:
            print(f"LangChain í•œêµ­ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return None
    
    def _try_improved_korean_generation(self, prompt: str, question: str, context: str) -> str:
        """ê°œì„ ëœ í•œêµ­ì–´ ë‹µë³€ ìƒì„±"""
        if not self.model or not self.tokenizer:
            return None
            
        try:
            # ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸
            conversation_prompt = f"ì „ë¬¸ê°€ì™€ì˜ ëŒ€í™”:\n\nì§ˆë¬¸: {question}\nì „ë¬¸ê°€: "
            
            inputs = self.tokenizer.encode(conversation_prompt, return_tensors='pt', max_length=600, truncation=True)
            attention_mask = (inputs != self.tokenizer.pad_token_id).long()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_length=inputs.shape[1] + 250,
                    temperature=0.8,
                    do_sample=True,
                    top_p=0.95,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ì „ë¬¸ê°€ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "ì „ë¬¸ê°€: " in full_text:
                answer = full_text.split("ì „ë¬¸ê°€: ")[-1].strip()
                return answer
                
        except Exception as e:
            print(f"ê°œì„ ëœ í•œêµ­ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return None
    
    def _is_valid_korean_response(self, response: str) -> bool:
        """í•œêµ­ì–´ ë‹µë³€ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if not response or len(response.strip()) < 10:
            return False
            
        # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²°ì–´ë¯¸ í™•ì¸
        korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ì—ìš”', 'ì˜ˆìš”']
        has_korean_ending = any(response.strip().endswith(ending) for ending in korean_endings)
        
        # í•œê¸€ ë¹„ìœ¨ í™•ì¸
        korean_chars = sum(1 for char in response if '\uAC00' <= char <= '\uD7A3')
        korean_ratio = korean_chars / len(response) if len(response) > 0 else 0
        
        return has_korean_ending and korean_ratio > 0.3
    
    def _generate_dynamic_answer(self, question: str, context: str) -> str:
        """ì§€ì‹ë² ì´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë™ì ìœ¼ë¡œ ë‹µë³€ ìƒì„± (í…œí”Œë¦¿ ì—†ìŒ)"""
        
        if not context or len(context.strip()) < 20:
            return f"{question}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        # ê°•ë ¥í•œ ë©”íƒ€ë°ì´í„° í•„í„°
        skip_words = [
            'Received', 'revised', 'accepted', 'Copyright', 'Creative Commons', 
            'Attribution', 'Open Access', 'filepath:', 'koti.re.kr', 'ì—°êµ¬ì›', 
            'êµìˆ˜', 'ê³µí•™ì„ì‚¬', 'ê³µí•™ë°•ì‚¬', 'ğŸ“–', 'ì°¸ê³ ìë£Œ', 'ì ìˆ˜', 'ëŒ€í‘œ ì²­í¬',
            'Korea Transport', 'Fig.', 'Table', 'Graduate School', '.pdf', '.md'
        ]
        
        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
        clean_lines = []
        for line in context.split('\n'):
            line = line.strip()
            
            # ë¹ˆ ì¤„
            if not line:
                continue
            
            # ë©”íƒ€ë°ì´í„° í¬í•¨ ì¤„ ìŠ¤í‚µ
            if any(skip in line for skip in skip_words):
                continue
            
            # ì œëª© í˜•ì‹ ì¤„ ìŠ¤í‚µ  
            if line.startswith('#') or line.startswith('*') or line.startswith('('):
                continue
            
            # ë„ˆë¬´ ì§§ì€ ì¤„ë§Œ ìŠ¤í‚µ
            if len(line) < 20:
                continue
            
            # ì˜ë¯¸ìˆëŠ” ë‚´ìš©
            clean_lines.append(line)
            if len(clean_lines) >= 4:
                break
        
        if clean_lines:
            return '\n\n'.join(clean_lines)
        
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìœ¼ë‚˜ ì ì ˆí•œ ë‹µë³€ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    def _pure_llm_knowledge_response(self, question: str) -> str:
        """[DEPRECATED] LLM ìì²´ ì§€ì‹ìœ¼ë¡œë§Œ ë‹µë³€ - ì§€ì‹ë² ì´ìŠ¤/ì™¸ë¶€ìë£Œ ì ˆëŒ€ ì‚¬ìš© ì•ˆí•¨"""
        
        # ì‹¬í”Œí•œ í”„ë¡¬í”„íŠ¸
        prompt = f"""ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""
        
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors='pt',
                max_length=150,
                truncation=True
            ).to('cpu')
            
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs.get('attention_mask'),
                    max_new_tokens=250,
                    min_new_tokens=40,
                    temperature=0.8,
                    do_sample=True,
                    top_p=0.95,
                    repetition_penalty=1.3,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            if len(response) > len(prompt):
                answer = response[len(prompt):].strip()
            else:
                answer = response
            
            if answer and len(answer) > 20:
                return answer
                
        except Exception as e:
            print(f"âŒ LLM ìƒì„± ì˜¤ë¥˜: {e}")
        
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _extract_natural_answer(self, question: str, context: str) -> str:
        """[DEPRECATED] ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ì¶”ì¶œ (í•˜ë“œì½”ë”© ì—†ì´ ë™ì  ìƒì„±)"""
        
        if not context or len(context.strip()) < 20:
            return f"{question}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
        
        # ê°•ë ¥í•œ ë©”íƒ€ë°ì´í„° ì œê±°
        skip_patterns = [
            'Received', 'revised', 'accepted', 'Creative Commons', 'Attribution',
            'Copyright', 'í•œêµ­êµí†µì—°êµ¬ì›', 'ì¢…ì‹ íšŒì›', 'êµì‹ ì €ì', 'ê³µí•™ì„ì‚¬', 'ê³µí•™ë°•ì‚¬',
            'filepath:', '.pdf', '.md', '.txt', '.jpg', '.png',
            'ğŸ“–', 'ì°¸ê³ ìë£Œ', 'ì ìˆ˜', 'This is an Open Access',
            'The Korea Transport', 'koti.re.kr', 'Fig.', 'Table.',
            'ë¬´ì¸í•­ê³µê¸°ë¥¼ì´ìš©í•œë”¥ëŸ¬ë‹', 'ì—°êµ¬ì›', 'ëŒ€í•™êµ', 'êµìˆ˜'
        ]
        
        lines = context.split('\n')
        key_info = []
        
        for line in lines:
            line = line.strip()
            
            # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
            if not line:
                continue
            
            # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ê±´ë„ˆë›°ê¸°
            if any(pattern in line for pattern in skip_patterns):
                continue
            
            # ë©”íƒ€ë°ì´í„°ë¡œ ë³´ì´ëŠ” ì¤„ ê±´ë„ˆë›°ê¸°
            if line.startswith('#') or line.startswith('*') or line.startswith('('):
                continue
            
            # ìˆ«ìë§Œ ìˆê±°ë‚˜ ë§¤ìš° ì§§ì€ ì¤„ ê±´ë„ˆë›°ê¸°
            if line.replace(' ', '').replace('.', '').replace('-', '').replace(':', '').isdigit():
                continue
            
            if len(line) < 15:
                continue
            
            # ì˜ë¯¸ ìˆëŠ” ì‹¤ì œ ë‚´ìš©ë§Œ ì„ íƒ
            if any(keyword in line for keyword in ['íƒì§€', 'ë¶„ì„', 'í•™ìŠµ', 'ë°ì´í„°', 'ëª¨ë¸', 'ë°©ë²•', 'í™•ì‚°', 'ê°ì—¼', 'ì†Œë‚˜ë¬´', 'YOLO', 'ë“œë¡ ', 'ì‹ ë¢°ë„', 'GPS']):
                key_info.append(line)
                if len(key_info) >= 4:  # ìµœëŒ€ 4ì¤„
                    break
        
        if key_info:
            # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ì¸íŠ¸ë¡œ ì¶”ê°€
            intro = self._generate_intro(question)
            info_text = '\n\n'.join(key_info)
            return f"{intro}\n\n{info_text}\n\në” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!"
        
        # Fallback - ì»¨í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œê°€ ìˆëŠ” ë¶€ë¶„ ì¶”ì¶œ
        sentences = context.split('.')
        relevant_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20 and not any(pattern in sentence for pattern in skip_patterns):
                if any(keyword in sentence for keyword in ['íƒì§€', 'ë¶„ì„', 'í•™ìŠµ', 'ë°ì´í„°', 'ëª¨ë¸', 'ë°©ë²•', 'í™•ì‚°', 'ì†Œë‚˜ë¬´', 'YOLO', 'ë“œë¡ ']):
                    relevant_sentences.append(sentence)
                    if len(relevant_sentences) >= 3:
                        break
        
        if relevant_sentences:
            intro = self._generate_intro(question)
            return f"{intro}\n\n{'. '.join(relevant_sentences)}.\n\në” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”!"
        
        return f"{question}ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”?"
    
    def _generate_intro(self, question: str) -> str:
        """ì§ˆë¬¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì¸íŠ¸ë¡œ ìƒì„± (í•˜ë“œì½”ë”© ì—†ì´ ë™ì )"""
        q_lower = question.lower()
        
        if 'ë­' in q_lower or 'ë¬´ì—‡' in q_lower or 'ì´ë€' in q_lower:
            return f"**{question}**\n\nì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤:"
        elif 'ì–´ë–»ê²Œ' in q_lower or 'ë°©ë²•' in q_lower:
            return f"**{question}**\n\në‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤:"
        elif 'ì™œ' in q_lower or 'ì´ìœ ' in q_lower:
            return f"**{question}**\n\nì´ìœ ë¥¼ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤:"
        else:
            return f"**{question}**\n\nê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:"
    
    def _generate_knowledge_only_answer(self, question: str, context: str) -> str:
        """[DEPRECATED] ì§€ì‹ë² ì´ìŠ¤ë§Œ ì‚¬ìš© - _extract_natural_answerë¡œ ëŒ€ì²´ë¨"""
        return self._extract_natural_answer(question, context)

# ì „ì—­ RAG ì¸ìŠ¤í„´ìŠ¤
_rag_instance: Optional[SimpleRAG] = None

def get_rag_system() -> SimpleRAG:
    """RAG ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_instance
    
    if _rag_instance is None:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ knowledge_base ì°¾ê¸°
        current_dir = Path(__file__).parent.parent  # api í´ë”ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ
        knowledge_base_path = current_dir / "knowledge_base"
        
        _rag_instance = SimpleRAG(str(knowledge_base_path))
    
    return _rag_instance