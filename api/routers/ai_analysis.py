# AI ë¶„ì„ ë¼ìš°í„° - RAG ê¸°ë°˜ ì±—ë´‡
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from typing import Optional
import pandas as pd
import json
import datetime
from transformers import pipeline
import torch
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from ..rag_system import get_rag_system

router = APIRouter()

# ì „ì—­ ë³€ìˆ˜ë“¤
text_generator = None
rag_system = None

@router.on_event("startup")
async def initialize_ai_system():
    """AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global text_generator, rag_system
    
    try:
        print("ğŸ¤– AI ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        # 1. RAG ì‹œìŠ¤í…œ ë¡œë“œ
        print("ğŸ“š RAG ì‹œìŠ¤í…œ ë¡œë”©...")
        rag_system = get_rag_system()
        
        # 2. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ (CPU ê¸°ë°˜)
        print("ğŸ§  í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ë¡œë”©...")
        try:
            text_generator = pipeline(
                "text-generation",
                model="gpt2",  # ê°€ë²¼ìš´ ê¸°ë³¸ ëª¨ë¸
                device=-1,     # CPU ì‚¬ìš©
                pad_token_id=50256
            )
            print("âœ… GPT-2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ GPT-2 ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ê¸° ì‚¬ìš©: {e}")
            text_generator = None
        
        print("ğŸ¯ AI ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

@router.post("/simple-chat")
async def simple_chat_bot(
    question: str = Form(..., description="ì§ˆë¬¸ (CSV íŒŒì¼ ì—†ì´ ìˆœìˆ˜ ì§ˆë¬¸ë§Œ)")
):
    """
    ğŸ¤– Knowledge Base ê¸°ë°˜ RAG ì±—ë´‡
    
    âœ… 8ê°œ ì „ë¬¸ ë¬¸ì„œ ì§€ì‹ ë² ì´ìŠ¤ í™œìš©
    - PDF ë…¼ë¬¸ (ë¬´ì¸í•­ê³µê¸° ë”¥ëŸ¬ë‹ íƒì§€)
    - ì²˜ë¦¬ ê°€ì´ë“œ (ì‹ ë¢°ë„ë³„ ëŒ€ì‘ë°©ì•ˆ)
    - GPS ë°©ì œ ê³„íš
    - ìƒíƒœí•™ì  íŠ¹ì„± ë“±
    
    ğŸ” FAISS ì˜ë¯¸ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ìë™ ì¶”ì¶œ
    ğŸ¤– GPT-4o-miniê°€ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ ìƒì„±
    
    ì§ˆë¬¸ ì˜ˆì‹œ:
    - "YOLO ëª¨ë¸ì´ ë­ì•¼?"
    - "mAP 70% ì´ìƒì´ë©´ í˜„ì—…ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•´?"
    - "ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ í™•ì‚° íŒ¨í„´ì€?"
    - "ë“œë¡  íƒì§€ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë°©ë²•ì€?"
    """
    
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # 1. ì§ˆë¬¸ì—ì„œ ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
        print(f"ğŸ” ì§ˆë¬¸ ë¶„ì„: {question}")
        relevant_context = ""
        knowledge_sources = 0
        
        try:
            relevant_context = rag_system.get_context_for_question(question)
            relevant_docs = rag_system.simple_search(question, top_k=3)
            knowledge_sources = len(relevant_docs)
            print(f"ğŸ“– {knowledge_sources}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
        except Exception as e:
            print(f"âš ï¸ ì§€ì‹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            relevant_context = "ê¸°ë³¸ ì „ë¬¸ ì§€ì‹ì„ í™œìš©í•©ë‹ˆë‹¤."
        
        # 2. ìˆœìˆ˜ ì§ˆë¬¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        analysis = rag_system.generate_korean_response(
            question=question,
            context=relevant_context,
            data_analysis=""  # CSV ë°ì´í„° ì—†ìŒ
        )
        
        # ë””ë²„ê¹…: ì‘ë‹µ í™•ì¸
        print(f"ğŸ¯ ìµœì¢… ë¶„ì„ ê²°ê³¼ ê¸¸ì´: {len(analysis)} ê¸€ì")
        print(f"ğŸ¯ ìµœì¢… ë¶„ì„ ê²°ê³¼: {analysis[:100]}...")
        
        # ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
        if not analysis or len(analysis.strip()) < 20:
            analysis = "ì§ˆë¬¸ì„ ì´í•´í–ˆì§€ë§Œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
        
        return {
            "success": True,
            "question": question,
            "ai_answer": analysis,
            "knowledge_sources_used": knowledge_sources,
            "context_used": bool(relevant_context),
            "model_info": {
                "type": "Knowledge Base RAG ì±—ë´‡",
                "llm_model": "gpt-4o-mini",
                "knowledge_base_docs": len(rag_system.documents) if rag_system else 0,
                "search_method": "FAISS Semantic Search",
                "mode": "ì§€ì‹ ê¸°ë°˜ ë‹µë³€ (RAG)"
            },
            "timestamp": datetime.datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì±—ë´‡ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@router.post("/chat-analysis")
async def rag_based_chat_analysis(
    csv_file: UploadFile = File(..., description="íƒì§€ ê²°ê³¼ CSV íŒŒì¼"),
    question: str = Form(..., description="ë¶„ì„ ì§ˆë¬¸"),
    use_rag: bool = Form(default=True, description="RAG ì§€ì‹ ë² ì´ìŠ¤ ì‚¬ìš© ì—¬ë¶€")
):
    """
    ğŸ§  RAG ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ì±—ë´‡
    
    ì—…ë¡œë“œëœ íƒì§€ ê²°ê³¼ CSVë¥¼ ë¶„ì„í•˜ê³ , 
    ì „ë¬¸ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    """
    
    try:
        # 1. CSV ë°ì´í„° ë¡œë“œ ë° ë¶„ì„
        print(f"ğŸ“Š CSV ë°ì´í„° ë¶„ì„ ì‹œì‘...")
        df = pd.read_csv(csv_file.file)
        data_summary = analyze_detection_data(df)
        
        # 2. RAG ê¸°ë°˜ ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
        relevant_context = ""
        knowledge_sources = 0
        
        if use_rag and rag_system:
            try:
                print(f"ğŸ” ì§ˆë¬¸ ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰: {question}")
                relevant_context = rag_system.get_context_for_question(question)
                knowledge_sources = len(rag_system.simple_search(question, top_k=3))
                print(f"ğŸ“– {knowledge_sources}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
            except Exception as e:
                print(f"âš ï¸ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                relevant_context = "ì „ë¬¸ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 3. ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¶„ì„ ìƒì„±
        analysis = generate_expert_analysis(
            question=question,
            data_summary=data_summary,
            relevant_context=relevant_context,
            use_llm=(text_generator is not None)
        )
        
        return {
            "success": True,
            "question": question,
            "ai_analysis": analysis,
            "data_summary": data_summary,
            "knowledge_context": relevant_context if use_rag else "RAG ì‚¬ìš© ì•ˆí•¨",
            "knowledge_sources_found": knowledge_sources,
            "rag_enabled": use_rag,
            "model_info": {
                "rag_system": "Simple Keyword-based RAG",
                "text_generator": "DialoGPT-medium + GPT-2 ë°±ì—…",
                "korean_support": "í•œêµ­ì–´ ë§¥ë½ íŠ¹í™” ì²˜ë¦¬",
                "knowledge_base_docs": len(rag_system.documents) if rag_system else 0,
                "type": "ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜"
            },
            "timestamp": datetime.datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@router.post("/generate-report")
async def generate_comprehensive_report(
    csv_file: UploadFile = File(..., description="íƒì§€ ê²°ê³¼ CSV íŒŒì¼")
):
    """
    ğŸ“‹ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
    
    íƒì§€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    try:
        # CSV ë°ì´í„° ë¶„ì„
        df = pd.read_csv(csv_file.file)
        data_summary = analyze_detection_data(df)
        
        # ë¯¸ë¦¬ ì •ì˜ëœ ë¶„ì„ ì§ˆë¬¸ë“¤
        analysis_questions = [
            "ì „ì²´ì ì¸ í”¼í•´ ê·œëª¨ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”",
            "ìš°ì„  ë°©ì œê°€ í•„ìš”í•œ ì§€ì—­ì€ ì–´ë””ì¸ê°€ìš”?",
            "ì‹ ë¢°ë„ ë¶„í¬ë¥¼ í•´ì„í•˜ê³  ê¶Œê³ ì‚¬í•­ì„ ì œì‹œí•´ì£¼ì„¸ìš”",
            "GPS ì¢Œí‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ì‚° íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"
        ]
        
        # ê° ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ì„ ì‹¤í–‰
        report_sections = {}
        for question in analysis_questions:
            try:
                relevant_context = ""
                if rag_system:
                    relevant_context = rag_system.get_context_for_question(question)
                
                analysis = generate_expert_analysis(
                    question=question,
                    data_summary=data_summary,
                    relevant_context=relevant_context,
                    use_llm=(text_generator is not None)
                )
                
                # ì§ˆë¬¸ì„ ì„¹ì…˜ ì œëª©ìœ¼ë¡œ ë³€í™˜
                section_title = question.replace("í•´ì£¼ì„¸ìš”", "").replace("ì¸ê°€ìš”?", "")
                report_sections[section_title] = analysis
                
            except Exception as e:
                report_sections[question] = f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        
        return {
            "success": True,
            "report_sections": report_sections,
            "data_summary": data_summary,
            "generated_at": datetime.datetime.now().isoformat(),
            "report_info": {
                "total_sections": len(report_sections),
                "knowledge_base_used": rag_system is not None,
                "ai_model_used": text_generator is not None
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

def analyze_detection_data(df: pd.DataFrame) -> dict:
    """íƒì§€ ë°ì´í„° ìƒì„¸ ë¶„ì„"""
    try:
        # ê¸°ë³¸ í†µê³„
        total_detections = len(df)
        
        # ì‹ ë¢°ë„ë³„ ë¶„í¬
        high_conf = len(df[df['confidence'] >= 0.7]) if 'confidence' in df.columns else 0
        medium_conf = len(df[(df['confidence'] >= 0.4) & (df['confidence'] < 0.7)]) if 'confidence' in df.columns else 0
        low_conf = len(df[df['confidence'] < 0.4]) if 'confidence' in df.columns else 0
        avg_confidence = round(df['confidence'].mean(), 3) if 'confidence' in df.columns else 0
        
        # ê³µê°„ ì •ë³´
        files_processed = df['filename'].nunique() if 'filename' in df.columns else 1
        has_gps = all(col in df.columns for col in ['tm_x', 'tm_y'])
        
        # GPS ë²”ìœ„ (ìˆëŠ” ê²½ìš°)
        gps_info = {}
        if has_gps:
            gps_info = {
                "x_range": f"{df['tm_x'].min():.0f} ~ {df['tm_x'].max():.0f}",
                "y_range": f"{df['tm_y'].min():.0f} ~ {df['tm_y'].max():.0f}",
                "coverage_area": calculate_coverage_area(df['tm_x'], df['tm_y'])
            }
        
        return {
            "total_detections": total_detections,
            "confidence_distribution": {
                "high_70_plus": high_conf,
                "medium_40_70": medium_conf, 
                "low_below_40": low_conf,
                "average": avg_confidence
            },
            "spatial_analysis": {
                "files_processed": files_processed,
                "detections_per_file": round(total_detections / files_processed, 1) if files_processed > 0 else 0,
                "has_gps_coordinates": has_gps,
                "gps_info": gps_info
            },
            "risk_assessment": assess_risk_level(high_conf, medium_conf, low_conf, total_detections)
        }
        
    except Exception as e:
        return {"error": f"ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}

def calculate_coverage_area(x_coords, y_coords) -> str:
    """GPS ì¢Œí‘œ ê¸°ë°˜ ì»¤ë²„ë¦¬ì§€ ë©´ì  ê³„ì‚°"""
    try:
        if len(x_coords) < 2 or len(y_coords) < 2:
            return "ë©´ì  ê³„ì‚° ë¶ˆê°€"
        
        x_range = x_coords.max() - x_coords.min()
        y_range = y_coords.max() - y_coords.min()
        area_sqm = x_range * y_range
        area_sqkm = area_sqm / 1_000_000
        
        if area_sqkm >= 1:
            return f"ì•½ {area_sqkm:.1f}kmÂ²"
        else:
            return f"ì•½ {area_sqm:.0f}mÂ²"
            
    except Exception:
        return "ë©´ì  ê³„ì‚° ë¶ˆê°€"

def assess_risk_level(high_conf: int, medium_conf: int, low_conf: int, total: int) -> dict:
    """ìœ„í—˜ë„ í‰ê°€"""
    if total == 0:
        return {"level": "ë°ì´í„° ì—†ìŒ", "description": "íƒì§€ ê²°ê³¼ ì—†ìŒ"}
    
    high_ratio = high_conf / total
    medium_ratio = medium_conf / total
    
    if high_ratio >= 0.7:
        level = "ë§¤ìš° ë†’ìŒ"
        description = "ì¦‰ì‹œ ëŒ€ê·œëª¨ ë°©ì œ ì‘ì—… í•„ìš”"
    elif high_ratio >= 0.4:
        level = "ë†’ìŒ" 
        description = "ì‹ ì†í•œ ë°©ì œ ê³„íš ìˆ˜ë¦½ í•„ìš”"
    elif medium_ratio >= 0.5:
        level = "ë³´í†µ"
        description = "ì¬ì¡°ì‚¬ í›„ ë°©ì œ ê³„íš ê²°ì •"
    else:
        level = "ë‚®ìŒ"
        description = "ì§€ì†ì  ëª¨ë‹ˆí„°ë§ í•„ìš”"
    
    return {
        "level": level,
        "description": description,
        "high_confidence_ratio": round(high_ratio * 100, 1)
    }

def generate_expert_analysis(question: str, data_summary: dict, relevant_context: str, use_llm: bool = False) -> str:
    """ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¶„ì„ ìƒì„± - DialoGPT-medium ê¸°ë°˜"""
    
    # ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ë¶„ì„
    base_analysis = generate_rule_based_analysis(question, data_summary, relevant_context)
    
    # RAG ì‹œìŠ¤í…œì˜ í•œêµ­ì–´ ì‘ë‹µ ìƒì„± ì‚¬ìš©
    if rag_system and hasattr(rag_system, 'generate_korean_response'):
        try:
            print("ğŸ¤– DialoGPT-mediumìœ¼ë¡œ í•œêµ­ì–´ ì‘ë‹µ ìƒì„± ì¤‘...")
            enhanced_analysis = rag_system.generate_korean_response(
                question=question, 
                context=relevant_context, 
                data_analysis=base_analysis
            )
            
            # ê¸°ë³¸ ë¶„ì„ê³¼ AI ìƒì„± ì‘ë‹µ ê²°í•©
            if enhanced_analysis and len(enhanced_analysis.strip()) > 50:
                return f"{base_analysis}\n\nğŸ¤– **AI ì „ë¬¸ê°€ ì¶”ê°€ ë¶„ì„**\n{enhanced_analysis}"
            
        except Exception as e:
            print(f"âš ï¸ DialoGPT ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ ì‚¬ìš©: {e}")
    
    # LLM ë°±ì—… (ê¸°ì¡´ GPT-2)
    elif use_llm and text_generator:
        try:
            enhanced_analysis = enhance_with_llm(question, base_analysis, text_generator)
            return enhanced_analysis
        except Exception as e:
            print(f"âš ï¸ ë°±ì—… LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    return base_analysis

def generate_rule_based_analysis(question: str, data_summary: dict, relevant_context: str) -> str:
    """ê·œì¹™ ê¸°ë°˜ ì „ë¬¸ê°€ ë¶„ì„ ìƒì„±"""
    
    # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
    question_lower = question.lower()
    
    analysis_parts = []
    
    # ê¸°ë³¸ ë°ì´í„° í•´ì„
    total = data_summary.get("total_detections", 0)
    high_conf = data_summary.get("confidence_distribution", {}).get("high_70_plus", 0)
    avg_conf = data_summary.get("confidence_distribution", {}).get("average", 0)
    risk_level = data_summary.get("risk_assessment", {}).get("level", "ì•Œ ìˆ˜ ì—†ìŒ")
    
    # í”¼í•´ ê·œëª¨ ê´€ë ¨ ì§ˆë¬¸
    if any(keyword in question_lower for keyword in ["ê·œëª¨", "í‰ê°€", "ì „ì²´", "í”¼í•´"]):
        if total > 3000:
            scale_assessment = "ëŒ€ê·œëª¨ í”¼í•´ ì§€ì—­ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."
        elif total > 1000:
            scale_assessment = "ì¤‘ê·œëª¨ í”¼í•´ê°€ ë°œìƒí•œ ì§€ì—­ì…ë‹ˆë‹¤."
        elif total > 100:
            scale_assessment = "ì†Œê·œëª¨ í”¼í•´ê°€ í™•ì¸ëœ ì§€ì—­ì…ë‹ˆë‹¤."
        else:
            scale_assessment = "ê²½ë¯¸í•œ í”¼í•´ ìˆ˜ì¤€ì…ë‹ˆë‹¤."
        
        analysis_parts.append(f"""
ğŸ¯ **í”¼í•´ ê·œëª¨ í‰ê°€**
- ì´ {total}ê°œ í”¼í•´ëª© íƒì§€: {scale_assessment}
- í‰ê·  ì‹ ë¢°ë„ {avg_conf}: {'ë†’ì€ ì •í™•ë„' if avg_conf >= 0.6 else 'ë³´í†µ ì •í™•ë„' if avg_conf >= 0.4 else 'ì •ë°€ ê²€í†  í•„ìš”'}
- ìœ„í—˜ ë“±ê¸‰: {risk_level}
""")
    
    # ìš°ì„ ìˆœìœ„/ë°©ì œ ê´€ë ¨ ì§ˆë¬¸  
    if any(keyword in question_lower for keyword in ["ìš°ì„ ", "ë°©ì œ", "ì²˜ë¦¬", "ëŒ€ì‘"]):
        analysis_parts.append(f"""
ğŸš¨ **ìš°ì„  ë°©ì œ ê¶Œê³ ì‚¬í•­**
- ê³ ì‹ ë¢°ë„ ì§€ì—­ ({high_conf}ê°œ): ì¦‰ì‹œ ë°©ì œ í•„ìš”
- ì¤‘ì‹ ë¢°ë„ ì§€ì—­: 2ì£¼ ë‚´ ì¬ì¡°ì‚¬ í›„ ë°©ì œ ê²°ì •
- ì €ì‹ ë¢°ë„ ì§€ì—­: ëª¨ë‹ˆí„°ë§ ê°•í™”
- ê¶Œì¥ ì‘ì—… ìˆœì„œ: ê³ ì‹ ë¢°ë„ â†’ í´ëŸ¬ìŠ¤í„° ê²½ê³„ â†’ ë‚´ë¶€ ì§€ì—­
""")
    
    # ì‹ ë¢°ë„ ê´€ë ¨ ì§ˆë¬¸
    if any(keyword in question_lower for keyword in ["ì‹ ë¢°ë„", "ì •í™•ë„", "ë¶„í¬"]):
        conf_dist = data_summary.get("confidence_distribution", {})
        analysis_parts.append(f"""
ğŸ“Š **ì‹ ë¢°ë„ ë¶„í¬ ë¶„ì„**  
- ê³ ì‹ ë¢°ë„ (70%+): {conf_dist.get('high_70_plus', 0)}ê°œ
- ì¤‘ì‹ ë¢°ë„ (40-70%): {conf_dist.get('medium_40_70', 0)}ê°œ  
- ì €ì‹ ë¢°ë„ (40% ë¯¸ë§Œ): {conf_dist.get('low_below_40', 0)}ê°œ
- í‰ê·  ì‹ ë¢°ë„: {conf_dist.get('average', 0)}

ğŸ’¡ **ê¶Œê³ ì‚¬í•­**: {'ì‹ ë¢°ë„ê°€ ë†’ì•„ ì¦‰ì‹œ ë°©ì œ ê°€ëŠ¥' if avg_conf >= 0.6 else 'ì¬ì¡°ì‚¬ë¥¼ í†µí•œ ì •í™•ë„ í–¥ìƒ í•„ìš”'}
""")
    
    # GPS/ì¢Œí‘œ ê´€ë ¨ ì§ˆë¬¸
    if any(keyword in question_lower for keyword in ["gps", "ì¢Œí‘œ", "ìœ„ì¹˜", "í™•ì‚°", "íŒ¨í„´"]):
        spatial_info = data_summary.get("spatial_analysis", {})
        has_gps = spatial_info.get("has_gps_coordinates", False)
        
        if has_gps:
            gps_info = spatial_info.get("gps_info", {})
            analysis_parts.append(f"""
ğŸ“ **GPS ì¢Œí‘œ ë¶„ì„**
- íƒì§€ ë²”ìœ„: X({gps_info.get('x_range', 'N/A')}), Y({gps_info.get('y_range', 'N/A')})
- ì»¤ë²„ë¦¬ì§€: {gps_info.get('coverage_area', 'N/A')}
- íŒŒì¼ë‹¹ í‰ê·  íƒì§€: {spatial_info.get('detections_per_file', 0)}ê°œ

ğŸ—ºï¸ **í™•ì‚° íŒ¨í„´ ê¶Œê³ **: í´ëŸ¬ìŠ¤í„° ë¶„ì„ì„ í†µí•œ ì°¨ë‹¨ì„  ì„¤ì¹˜ ë° ìš°ì„ ìˆœìœ„ ë°©ì œ ê³„íš ìˆ˜ë¦½
""")
        else:
            analysis_parts.append(f"""
âš ï¸ **GPS ì¢Œí‘œ ì •ë³´ ë¶€ì¡±**
- í˜„ì¬ í”½ì…€ ì¢Œí‘œë§Œ ì œê³µë¨
- ì •í™•í•œ í˜„ì¥ ë°©ì œë¥¼ ìœ„í•´ GPS ì¢Œí‘œ ë³€í™˜ í•„ìš”
- ì§€ë¦¬ì°¸ì¡° ì •ë³´(.tfw) íŒŒì¼ í™•ì¸ ê¶Œì¥
""")
    
    # RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if relevant_context and "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" not in relevant_context:
        analysis_parts.append(f"""
ğŸ“š **ì „ë¬¸ ì§€ì‹ ê¸°ë°˜ ì°¸ê³ ì‚¬í•­**
{relevant_context}
""")
    
    # ê¸°ë³¸ ë¶„ì„ì´ ì—†ëŠ” ê²½ìš°
    if not analysis_parts:
        analysis_parts.append(f"""
ğŸ“Š **ê¸°ë³¸ ë¶„ì„ ê²°ê³¼**
- ì´ íƒì§€ ìˆ˜: {total}ê°œ
- í‰ê·  ì‹ ë¢°ë„: {avg_conf}
- ìœ„í—˜ ë“±ê¸‰: {risk_level}

ğŸ’¡ ë” êµ¬ì²´ì ì¸ ë¶„ì„ì„ ìœ„í•´ ì§ˆë¬¸ì„ ëª…í™•íˆ í•´ì£¼ì‹œë©´ ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.
""")
    
    return "\n".join(analysis_parts)

def enhance_with_llm(question: str, base_analysis: str, llm_pipeline) -> str:
    """LLMì„ í™œìš©í•œ ë¶„ì„ í–¥ìƒ (ì„ íƒì )"""
    try:
        prompt = f"""ë‹¤ìŒ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì¡°ì–¸ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question}

ê¸°ë³¸ ë¶„ì„:
{base_analysis}

ì¶”ê°€ ì „ë¬¸ê°€ ì¡°ì–¸:"""
        
        response = llm_pipeline(
            prompt,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True,
            pad_token_id=llm_pipeline.tokenizer.eos_token_id
        )
        
        full_response = response[0]['generated_text']
        enhanced_part = full_response.replace(prompt, "").strip()
        
        if enhanced_part and len(enhanced_part) > 10:
            return base_analysis + f"\n\nğŸ§  **AI ì¶”ê°€ ë¶„ì„**\n{enhanced_part}"
        else:
            return base_analysis
            
    except Exception as e:
        print(f"âš ï¸ LLM í–¥ìƒ ì‹¤íŒ¨: {e}")
        return base_analysis