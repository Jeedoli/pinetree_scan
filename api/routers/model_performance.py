"""
ğŸ¯ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ API
- ì´ë¯¸ì§€ ì—…ë¡œë“œí•˜ì—¬ mAP, Precision, Recall ê°’ ê³„ì‚°
- ë‹¤ì–‘í•œ ì‹ ë¢°ë„ ì„ê³„ê°’ì—ì„œ ì„±ëŠ¥ ì¸¡ì •
- ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì ì¸ ì„±ëŠ¥ ë¶„ì„
"""
from ..config import DEFAULT_MODEL_PATH
from fastapi import APIRouter, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse
from typing import List, Dict, Any, Optional
import os
import tempfile
import shutil
from pathlib import Path
import cv2
import numpy as np
from ultralytics import YOLO
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_recall_curve, average_precision_score
import base64
import io

from ..config import DEFAULT_MODEL_PATH

router = APIRouter()

class SimpleModelAnalyzer:
    """ê°„ë‹¨í•œ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = YOLO(model_path)
        
    def analyze_performance(self, image_paths: List[str]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ë“¤ì˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        
        # ë‹¤ì–‘í•œ ì‹ ë¢°ë„ ì„ê³„ê°’ì—ì„œ í…ŒìŠ¤íŠ¸
        confidence_thresholds = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5]
        
        results = {}
        all_confidences = []
        
        for conf in confidence_thresholds:
            total_detections = 0
            confidences = []
            
            for image_path in image_paths:
                try:
                    # YOLO ì¶”ë¡ 
                    yolo_results = self.model(image_path, conf=conf, verbose=False)
                    result = yolo_results[0]
                    
                    if result.boxes is not None and len(result.boxes) > 0:
                        boxes = result.boxes
                        total_detections += len(boxes)
                        
                        # ì‹ ë¢°ë„ ì ìˆ˜ ìˆ˜ì§‘
                        for i in range(len(boxes)):
                            confidence = float(boxes.conf[i].cpu().numpy())
                            confidences.append(confidence)
                            all_confidences.append(confidence)
                
                except Exception:
                    continue
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            precision = np.mean(confidences) if confidences else 0.0
            recall_estimate = total_detections / len(image_paths) if image_paths else 0.0
            
            # F1-Score ê³„ì‚° (Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· )
            if precision > 0 and recall_estimate > 0:
                f1_score = 2 * (precision * recall_estimate) / (precision + recall_estimate)
            else:
                f1_score = 0.0
            
            results[f"conf_{conf}"] = {
                "confidence_threshold": conf,
                "total_detections": total_detections,
                "precision_estimate": round(precision, 4),
                "recall_estimate": round(recall_estimate, 4),
                "f1_score": round(f1_score, 4),
                "avg_confidence": round(np.mean(confidences), 4) if confidences else 0.0
            }
        
        # mAP ì¶”ì • (ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ precision í‰ê· )
        precisions = [r["precision_estimate"] for r in results.values() if r["precision_estimate"] > 0]
        mAP_estimate = np.mean(precisions) if precisions else 0.0
        
        # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (F1-Score ê¸°ì¤€)
        best_conf = max(results.items(), key=lambda x: x[1]["f1_score"])
        
        return {
            "mAP_estimate": round(mAP_estimate, 4),
            "total_images_analyzed": len(image_paths),
            "best_confidence_threshold": best_conf[1]["confidence_threshold"],
            "best_performance": best_conf[1],
            "all_thresholds": list(results.values()),
            "confidence_statistics": {
                "mean": round(np.mean(all_confidences), 4) if all_confidences else 0.0,
                "std": round(np.std(all_confidences), 4) if all_confidences else 0.0,
                "min": round(np.min(all_confidences), 4) if all_confidences else 0.0,
                "max": round(np.max(all_confidences), 4) if all_confidences else 0.0,
                "count": len(all_confidences)
            }
        }
    
def _get_improvement_suggestions(analysis_result: Dict) -> List[str]:
    """ì„±ëŠ¥ ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±"""
    suggestions = []
    best_perf = analysis_result["best_performance"]
    
    if best_perf["precision_estimate"] < 0.6:
        suggestions.append("ì •ë°€ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì˜¤íƒì§€ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ë†’ì´ê±°ë‚˜ ì¶”ê°€ í•™ìŠµì„ ê³ ë ¤í•˜ì„¸ìš”.")
    
    if best_perf["recall_estimate"] < 0.6:
        suggestions.append("ì¬í˜„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ë†“ì¹œ ê°ì²´ê°€ ë§ìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ë‚®ì¶”ê±°ë‚˜ ë°ì´í„° ì¦ê°•ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
    if best_perf["f1_score"] < 0.5:
        suggestions.append("ì „ì²´ì ì¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë¸ ì¬í•™ìŠµì´ë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
    if analysis_result["confidence_statistics"]["mean"] < 0.3:
        suggestions.append("í‰ê·  ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ì˜ í’ˆì§ˆì„ í™•ì¸í•˜ê±°ë‚˜ ë” ë§ì€ epochë¡œ í•™ìŠµí•˜ì„¸ìš”.")
        
    if not suggestions:
        suggestions.append("í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ìœ ì§€ ë˜ëŠ” ë¯¸ì„¸ ì¡°ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
    return suggestions

def _evaluate_model_status(analysis_result: Dict) -> str:
    """ëª¨ë¸ ì „ì²´ ìƒíƒœ í‰ê°€"""
    best_perf = analysis_result["best_performance"]
    avg_score = (best_perf["precision_estimate"] + best_perf["recall_estimate"] + best_perf["f1_score"]) / 3
    
    if avg_score >= 0.8:
        return "ğŸŸ¢ EXCELLENT - ëª¨ë¸ì´ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤"
    elif avg_score >= 0.6:
        return "ğŸŸ¡ GOOD - ëª¨ë¸ì´ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤"
    elif avg_score >= 0.4:
        return "ğŸŸ  FAIR - ëª¨ë¸ ì„±ëŠ¥ì´ ë³´í†µì…ë‹ˆë‹¤. ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤"
    else:
        return "ğŸ”´ POOR - ëª¨ë¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤"

# ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
analyzer = None

def get_analyzer():
    """ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global analyzer
    if analyzer is None:
        analyzer = SimpleModelAnalyzer(str(DEFAULT_MODEL_PATH))
    return analyzer

@router.post("/analyze", summary="ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")
async def analyze_model_performance(
    files: List[UploadFile] = File(..., description="ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ë“¤")
):
    """
    ì´ë¯¸ì§€ë“¤ì„ ì—…ë¡œë“œí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    
    Returns:
    - mAP ì¶”ì •ê°’
    - Precision, Recall ì¶”ì •ê°’  
    - F1-Score
    - ì‹ ë¢°ë„ í†µê³„
    - ìµœì  ì„ê³„ê°’ ì¶”ì²œ
    """
    
    if not files:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤")
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    temp_dir = Path(tempfile.gettempdir()) / "performance_analysis"
    temp_dir.mkdir(exist_ok=True)
    
    try:
        # íŒŒì¼ ì €ì¥
        image_paths = []
        for file in files:
            if not file.content_type.startswith('image/'):
                continue
                
            temp_path = temp_dir / file.filename
            with open(temp_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            image_paths.append(str(temp_path))
        
        if not image_paths:
            raise HTTPException(status_code=400, detail="ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
        performance_analyzer = get_analyzer()
        analysis_result = performance_analyzer.analyze_performance(image_paths)
        
        # íƒì§€ ê²°ê³¼ ìš”ì•½ ê³„ì‚°
        best_perf = analysis_result["best_performance"]
        conf_stats = analysis_result["confidence_statistics"]
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
        def get_performance_grade(score):
            if score >= 0.8: return "A (ìš°ìˆ˜)"
            elif score >= 0.6: return "B (ì–‘í˜¸)"  
            elif score >= 0.4: return "C (ë³´í†µ)"
            elif score >= 0.2: return "D (ë¯¸í¡)"
            else: return "F (ë¶ˆëŸ‰)"
        
        # ê³ ë„í™”ëœ ì‘ë‹µ ë°ì´í„°
        response = {
            "ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼": {
                "ğŸ¯ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ": {
                    "mAP (Mean Average Precision)": {
                        "ê°’": analysis_result["mAP_estimate"],
                        "ë°±ë¶„ìœ¨": f"{analysis_result['mAP_estimate'] * 100:.1f}%",
                        "ë“±ê¸‰": get_performance_grade(analysis_result["mAP_estimate"]),
                        "ì„¤ëª…": "ì „ì²´ í´ë˜ìŠ¤ì— ëŒ€í•œ í‰ê·  ì •ë°€ë„"
                    },
                    "Precision (ì •ë°€ë„)": {
                        "ê°’": best_perf["precision_estimate"],
                        "ë°±ë¶„ìœ¨": f"{best_perf['precision_estimate'] * 100:.1f}%",
                        "ë“±ê¸‰": get_performance_grade(best_perf["precision_estimate"]),
                        "ì„¤ëª…": "íƒì§€í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì •ë‹µì¸ ë¹„ìœ¨"
                    },
                    "Recall (ì¬í˜„ìœ¨)": {
                        "ê°’": best_perf["recall_estimate"],
                        "ë°±ë¶„ìœ¨": f"{best_perf['recall_estimate'] * 100:.1f}%",
                        "ë“±ê¸‰": get_performance_grade(best_perf["recall_estimate"]),
                        "ì„¤ëª…": "ì „ì²´ ì •ë‹µ ì¤‘ ì‹¤ì œë¡œ ì°¾ì•„ë‚¸ ë¹„ìœ¨"
                    },
                    "F1-Score": {
                        "ê°’": best_perf["f1_score"],
                        "ë°±ë¶„ìœ¨": f"{best_perf['f1_score'] * 100:.1f}%",
                        "ë“±ê¸‰": get_performance_grade(best_perf["f1_score"]),
                        "ì„¤ëª…": "ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· "
                    }
                },
                
                "ğŸ” íƒì§€ ê²°ê³¼ ìš”ì•½": {
                    "ì´ íƒì§€ ê°œìˆ˜": best_perf["total_detections"],
                    "ë¶„ì„ëœ ì´ë¯¸ì§€ ìˆ˜": analysis_result["total_images_analyzed"],
                    "ì´ë¯¸ì§€ë‹¹ í‰ê·  íƒì§€": round(best_perf["total_detections"] / analysis_result["total_images_analyzed"], 1),
                    "ìµœì  ì‹ ë¢°ë„ ì„ê³„ê°’": best_perf["confidence_threshold"],
                    "í‰ê·  ì‹ ë¢°ë„": best_perf["avg_confidence"]
                },
                
                "ğŸ“ˆ ì‹ ë¢°ë„ ë¶„ì„": {
                    "ì „ì²´ íƒì§€ëœ ê°ì²´ ìˆ˜": conf_stats["count"],
                    "ì‹ ë¢°ë„ í†µê³„": {
                        "í‰ê· ": f"{conf_stats['mean']:.3f}",
                        "í‘œì¤€í¸ì°¨": f"{conf_stats['std']:.3f}",
                        "ìµœì†Œê°’": f"{conf_stats['min']:.3f}",
                        "ìµœê³ ê°’": f"{conf_stats['max']:.3f}"
                    },
                    "ì‹ ë¢°ë„ ë¶„í¬": {
                        "ë†’ìŒ (0.5 ì´ìƒ)": sum(1 for r in analysis_result["all_thresholds"] if r["confidence_threshold"] >= 0.5 and r["total_detections"] > 0),
                        "ì¤‘ê°„ (0.2-0.5)": sum(1 for r in analysis_result["all_thresholds"] if 0.2 <= r["confidence_threshold"] < 0.5 and r["total_detections"] > 0),
                        "ë‚®ìŒ (0.2 ë¯¸ë§Œ)": sum(1 for r in analysis_result["all_thresholds"] if r["confidence_threshold"] < 0.2 and r["total_detections"] > 0)
                    }
                },
                
                "âš™ï¸ ìµœì í™” ê¶Œì¥ì‚¬í•­": {
                    "ì¶”ì²œ ì‹ ë¢°ë„ ì„ê³„ê°’": analysis_result["best_confidence_threshold"],
                    "ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸": _get_improvement_suggestions(analysis_result),
                    "ëª¨ë¸ ìƒíƒœ í‰ê°€": _evaluate_model_status(analysis_result)
                }
            },
            
            "ğŸ“‹ ìƒì„¸ ë¶„ì„ ë°ì´í„°": {
                "ì„ê³„ê°’ë³„ ìƒì„¸ ê²°ê³¼": [
                    {
                        "ì‹ ë¢°ë„_ì„ê³„ê°’": r["confidence_threshold"],
                        "íƒì§€_ê°œìˆ˜": r["total_detections"],
                        "ì •ë°€ë„": f"{r['precision_estimate']:.3f}",
                        "ì¬í˜„ìœ¨": f"{r['recall_estimate']:.3f}",
                        "F1_ì ìˆ˜": f"{r['f1_score']:.3f}",
                        "í‰ê· _ì‹ ë¢°ë„": f"{r['avg_confidence']:.3f}"
                    }
                    for r in analysis_result["all_thresholds"]
                ],
                "ë¶„ì„_ë©”íƒ€ë°ì´í„°": {
                    "ë¶„ì„_ì‹œê°„": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "ëª¨ë¸_ê²½ë¡œ": str(DEFAULT_MODEL_PATH),
                    "í…ŒìŠ¤íŠ¸ëœ_ì„ê³„ê°’_ìˆ˜": len(analysis_result["all_thresholds"]),
                    "API_ë²„ì „": "v1.2"
                }
            }
        }
        
        return JSONResponse(content=response)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            shutil.rmtree(temp_dir)
        except:
            pass

@router.get("/model-info", summary="ëª¨ë¸ ì •ë³´")
async def get_model_info():
    """í˜„ì¬ ëª¨ë¸ì˜ ê¸°ë³¸ ì •ë³´ ë°˜í™˜"""
    
    try:
        model_path = str(DEFAULT_MODEL_PATH)
        model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
        
        # ëª¨ë¸ ë¡œë“œí•˜ì—¬ í´ë˜ìŠ¤ ì •ë³´ í™•ì¸
        model = YOLO(model_path)
        
        return {
            "ëª¨ë¸_ê²½ë¡œ": model_path,
            "ëª¨ë¸_í¬ê¸°_MB": round(model_size, 2),
            "í´ë˜ìŠ¤_ìˆ˜": len(model.names),
            "í´ë˜ìŠ¤ëª…": model.names
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.get("/health", summary="ìƒíƒœ í™•ì¸")  
async def health_check():
    """API ìƒíƒœ í™•ì¸"""
    
    try:
        analyzer = get_analyzer()
        return {
            "ìƒíƒœ": "ì •ìƒ",
            "ëª¨ë¸_ë¡œë“œë¨": True,
            "ì‹œê°„": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "ìƒíƒœ": "ì˜¤ë¥˜",
            "ëª¨ë¸_ë¡œë“œë¨": False,
            "ì˜¤ë¥˜": str(e),
            "ì‹œê°„": datetime.now().isoformat()
        }
