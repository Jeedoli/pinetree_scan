# ìœ í‹¸ë¦¬í‹° API ë¼ìš°í„° - ë¶€ê°€ì  ê¸°ëŠ¥ë“¤
# verify_gps_to_pixel.py, csv_to_yolo_label.py, roboflow_csv_to_yolo.py ê¸°ë°˜

from fastapi import APIRouter, File, UploadFile, HTTPException, Form, Query
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import tempfile
import shutil
import pandas as pd
import numpy as np
import cv2
import rasterio
import pyproj
import datetime
import yaml
import zipfile
import random
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from .. import config

router = APIRouter()

# ì‘ë‹µ ëª¨ë¸ ì •ì˜
class GPSVerificationResult(BaseModel):
    latitude: float
    longitude: float
    pixel_x: int
    pixel_y: int
    is_within_bounds: bool

class GPSVerificationResponse(BaseModel):
    success: bool
    message: str
    image_info: Dict[str, Any]
    verification_results: List[GPSVerificationResult]
    output_image_url: Optional[str] = None

class YOLOLabelResult(BaseModel):
    original_coordinates: Dict[str, float]
    yolo_format: str
    is_within_image: bool

class YOLOConversionResponse(BaseModel):
    success: bool
    message: str
    total_objects: int
    converted_objects: int
    label_results: List[YOLOLabelResult]
    download_url: Optional[str] = None

class RoboflowConversionResponse(BaseModel):
    success: bool
    message: str
    processed_images: int
    total_labels: int
    conversion_summary: Dict[str, int]
    download_url: Optional[str] = None

# ê¸°ë³¸ ì„¤ì •
DEFAULT_UTILS_DIR = config.API_OUTPUT_BASE / "utilities"
DEFAULT_COLAB_DIR = config.API_OUTPUT_BASE / "colab_deeplearning"

@router.post("/verify_gps_to_pixel", response_model=GPSVerificationResponse)
async def verify_gps_to_pixel_coordinates(
    image_file: UploadFile = File(..., description="ê²€ì¦í•  ì´ë¯¸ì§€ íŒŒì¼ (GeoTIFF ê¶Œì¥)"),
    csv_file: UploadFile = File(..., description="GPS ì¢Œí‘œê°€ ë‹´ê¸´ CSV íŒŒì¼ (latitude, longitude ì»¬ëŸ¼ í•„ìš”)"),
    box_size: int = Form(default=20, description="í‘œì‹œí•  ë°•ìŠ¤ í¬ê¸° (í”½ì…€)")
):
    """
    GPS ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜í•˜ì—¬ ê²€ì¦í•©ë‹ˆë‹¤.
    ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸: verify_gps_to_pixel.py
    
    - **image_file**: ê²€ì¦í•  ì´ë¯¸ì§€ íŒŒì¼ (GeoTIFF í˜•ì‹ ê¶Œì¥)
    - **csv_file**: GPS ì¢Œí‘œ CSV (latitude, longitude ì»¬ëŸ¼ í•„ìš”)  
    - **box_size**: í‘œì‹œí•  ê²€ì¦ ë°•ìŠ¤ í¬ê¸° (ê¸°ë³¸: 20í”½ì…€)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(tuple(config.ALLOWED_IMAGE_EXTENSIONS)):
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {image_file.filename}"
            )
        
        if not csv_file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = DEFAULT_UTILS_DIR / f"gps_verification_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ì²˜ë¦¬
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # íŒŒì¼ ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            csv_path = os.path.join(temp_dir, csv_file.filename)
            
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            with open(csv_path, "wb") as f:
                shutil.copyfileobj(csv_file.file, f)
            
            # CSV íŒŒì¼ ê²€ì¦ ë° ë¡œë“œ
            try:
                df = pd.read_csv(csv_path)
                required_cols = ['latitude', 'longitude']
                if not all(col in df.columns for col in required_cols):
                    raise HTTPException(
                        status_code=400,
                        detail=f"CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {required_cols}"
                    )
            except pd.errors.EmptyDataError:
                raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì¢Œí‘œ ë³€í™˜
            img = cv2.imread(image_path)
            if img is None:
                raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            verification_results = []
            img_height, img_width = img.shape[:2]
            
            # ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘
            image_info = {
                "filename": image_file.filename,
                "width": img_width,
                "height": img_height,
                "channels": img.shape[2] if len(img.shape) > 2 else 1
            }
            
            # GPS ì¢Œí‘œ ì²˜ë¦¬
            with rasterio.open(image_path) as src:
                # ì´ë¯¸ì§€ì— CRS ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
                has_crs = src.crs is not None
                image_info["has_crs"] = has_crs
                image_info["crs"] = str(src.crs) if has_crs else None
                
                for _, row in df.iterrows():
                    lat, lon = float(row['latitude']), float(row['longitude'])
                    
                    try:
                        if has_crs and not src.crs.to_string().startswith("EPSG:4326"):
                            # CRS ë³€í™˜ í•„ìš”í•œ ê²½ìš°
                            transformer = pyproj.Transformer.from_crs(
                                "EPSG:4326", src.crs, always_xy=True
                            )
                            x_img, y_img = transformer.transform(lon, lat)
                            col, row_idx = src.index(x_img, y_img)
                        else:
                            # ì§ì ‘ ë³€í™˜
                            col, row_idx = src.index(lon, lat)
                        
                        # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ í™•ì¸
                        is_within = (0 <= col < img_width and 0 <= row_idx < img_height)
                        
                        verification_results.append(GPSVerificationResult(
                            latitude=lat,
                            longitude=lon,
                            pixel_x=int(col),
                            pixel_y=int(row_idx),
                            is_within_bounds=is_within
                        ))
                        
                        # ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ê²½ê³„ ë‚´ì— ìˆëŠ” ê²½ìš°ë§Œ)
                        if is_within:
                            x1 = max(0, int(col) - box_size // 2)
                            y1 = max(0, int(row_idx) - box_size // 2)
                            x2 = min(img_width, int(col) + box_size // 2)
                            y2 = min(img_height, int(row_idx) + box_size // 2)
                            
                            # ë¹¨ê°„ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)
                            
                            # ì¢Œí‘œ í…ìŠ¤íŠ¸ ì¶”ê°€
                            cv2.putText(
                                img, f"({lat:.6f}, {lon:.6f})", 
                                (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 
                                0.5, (0, 0, 255), 1
                            )
                        
                    except Exception as e:
                        print(f"ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨ ({lat}, {lon}): {str(e)}")
                        continue
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
            output_filename = f"gps_verification_{os.path.splitext(image_file.filename)[0]}.png"
            output_path = output_dir / output_filename
            cv2.imwrite(str(output_path), img)
            
            # í†µê³„ ê³„ì‚°
            within_bounds_count = sum(1 for r in verification_results if r.is_within_bounds)
            
            return GPSVerificationResponse(
                success=True,
                message=f"GPS ì¢Œí‘œ ê²€ì¦ ì™„ë£Œ: {len(verification_results)}ê°œ ì¢Œí‘œ ì¤‘ {within_bounds_count}ê°œê°€ ì´ë¯¸ì§€ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.",
                image_info=image_info,
                verification_results=verification_results,
                output_image_url=f"/api/v1/utilities/download/verification/{output_filename}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GPS ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.post("/csv_to_yolo_labels", response_model=YOLOConversionResponse)
async def convert_csv_to_yolo_labels(
    csv_file: UploadFile = File(..., description="TM ì¢Œí‘œ CSV íŒŒì¼ (x, y ì»¬ëŸ¼ í•„ìš”)"),
    tfw_file: UploadFile = File(..., description="ì¢Œí‘œ ë³€í™˜ íŒŒì¼ (.tfw)"),
    image_width: int = Form(..., description="ì´ë¯¸ì§€ ë„ˆë¹„ (í”½ì…€)"),
    image_height: int = Form(..., description="ì´ë¯¸ì§€ ë†’ì´ (í”½ì…€)"),
    bbox_size: int = Form(default=16, description="ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° (í”½ì…€)"),
    class_id: int = Form(default=0, description="YOLO í´ë˜ìŠ¤ ID")
):
    """
    CSVì˜ TM ì¢Œí‘œë¥¼ YOLO ë¼ë²¨ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸: csv_to_yolo_label.py
    
    - **csv_file**: TM ì¢Œí‘œê°€ ë‹´ê¸´ CSV íŒŒì¼ (x, y ì»¬ëŸ¼ í•„ìš”)
    - **tfw_file**: ì¢Œí‘œ ë³€í™˜ì„ ìœ„í•œ TFW íŒŒì¼
    - **image_width/height**: ëŒ€ìƒ ì´ë¯¸ì§€ í¬ê¸°
    - **bbox_size**: ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° (ê¸°ë³¸: 16í”½ì…€)
    - **class_id**: YOLO í´ë˜ìŠ¤ ID (ê¸°ë³¸: 0)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not csv_file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not tfw_file.filename.lower().endswith('.tfw'):
            raise HTTPException(status_code=400, detail="TFW íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = DEFAULT_UTILS_DIR / f"yolo_conversion_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ì²˜ë¦¬
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # íŒŒì¼ ì €ì¥
            csv_path = os.path.join(temp_dir, csv_file.filename)
            tfw_path = os.path.join(temp_dir, tfw_file.filename)
            
            with open(csv_path, "wb") as f:
                shutil.copyfileobj(csv_file.file, f)
            with open(tfw_path, "wb") as f:
                shutil.copyfileobj(tfw_file.file, f)
            
            # CSV íŒŒì¼ ë¡œë“œ ë° ê²€ì¦
            try:
                df = pd.read_csv(csv_path)
                if 'x' not in df.columns or 'y' not in df.columns:
                    raise HTTPException(
                        status_code=400,
                        detail="CSV íŒŒì¼ì— 'x', 'y' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    )
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # TFW íŒŒì¼ ë¡œë“œ
            try:
                with open(tfw_path, 'r') as f:
                    tfw_params = [float(line.strip()) for line in f.readlines()]
                if len(tfw_params) != 6:
                    raise HTTPException(status_code=400, detail="ì˜ëª»ëœ TFW íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"TFW íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # YOLO ë¼ë²¨ ë³€í™˜
            label_results = []
            yolo_lines = []
            
            for _, row in df.iterrows():
                x_tm, y_tm = float(row['x']), float(row['y'])
                
                # TM ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                px, py = tm_to_pixel(x_tm, y_tm, tfw_params)
                
                # YOLO í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
                x_center = px / image_width
                y_center = py / image_height
                width_norm = bbox_size / image_width
                height_norm = bbox_size / image_height
                
                # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ í™•ì¸
                is_within = (0 <= px < image_width and 0 <= py < image_height)
                
                yolo_format = f"{class_id} {x_center:.6f} {y_center:.6f} {width_norm:.6f} {height_norm:.6f}"
                
                label_results.append(YOLOLabelResult(
                    original_coordinates={"x": x_tm, "y": y_tm, "px": px, "py": py},
                    yolo_format=yolo_format,
                    is_within_image=is_within
                ))
                
                if is_within:
                    yolo_lines.append(yolo_format)
            
            # YOLO ë¼ë²¨ íŒŒì¼ ì €ì¥
            output_filename = f"converted_labels_{timestamp}.txt"
            output_path = output_dir / output_filename
            
            with open(output_path, 'w') as f:
                f.write('\n'.join(yolo_lines))
            
            return YOLOConversionResponse(
                success=True,
                message=f"YOLO ë¼ë²¨ ë³€í™˜ ì™„ë£Œ: {len(df)}ê°œ ì¢Œí‘œ ì¤‘ {len(yolo_lines)}ê°œê°€ ì´ë¯¸ì§€ ë²”ìœ„ ë‚´ì—ì„œ ë³€í™˜ë¨",
                total_objects=len(df),
                converted_objects=len(yolo_lines),
                label_results=label_results,
                download_url=f"/api/v1/utilities/download/yolo/{output_filename}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"YOLO ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.post("/roboflow_to_yolo", response_model=RoboflowConversionResponse)
async def convert_roboflow_to_yolo(
    images: List[UploadFile] = File(..., description="ì´ë¯¸ì§€ íŒŒì¼ë“¤"),
    labels_csv: UploadFile = File(..., description="Roboflow ë¼ë²¨ CSV (filename, label, xmin, ymin, xmax, ymax)"),
    create_zip: bool = Form(default=True, description="ê²°ê³¼ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•í• ì§€ ì—¬ë¶€")
):
    """
    Roboflow ë¼ë²¨ CSVë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸: roboflow_csv_to_yolo.py
    
    - **images**: ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤
    - **labels_csv**: Roboflow í˜•ì‹ì˜ ë¼ë²¨ CSV (filename, label, xmin, ymin, xmax, ymax)
    - **create_zip**: ê²°ê³¼ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•í• ì§€ ì—¬ë¶€
    """
    
    try:
        # CSV íŒŒì¼ ê²€ì¦
        if not labels_csv.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="ë¼ë²¨ CSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = DEFAULT_UTILS_DIR / f"roboflow_conversion_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ì²˜ë¦¬
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì €ì¥
            image_paths = {}
            for img_file in images:
                if img_file.filename.lower().endswith(tuple(config.ALLOWED_IMAGE_EXTENSIONS)):
                    img_path = os.path.join(temp_dir, img_file.filename)
                    with open(img_path, "wb") as f:
                        shutil.copyfileobj(img_file.file, f)
                    image_paths[img_file.filename] = img_path
            
            # CSV íŒŒì¼ ì €ì¥ ë° ë¡œë“œ
            csv_path = os.path.join(temp_dir, labels_csv.filename)
            with open(csv_path, "wb") as f:
                shutil.copyfileobj(labels_csv.file, f)
            
            try:
                df = pd.read_csv(csv_path)
                required_cols = ['filename', 'label', 'xmin', 'ymin', 'xmax', 'ymax']
                if not all(col in df.columns for col in required_cols):
                    raise HTTPException(
                        status_code=400,
                        detail=f"CSV íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {required_cols}"
                    )
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # ë³€í™˜ í†µê³„
            conversion_summary = {}
            total_labels = 0
            processed_images = 0
            
            # ê° ì´ë¯¸ì§€ë³„ ë¼ë²¨ ë³€í™˜
            for filename in df['filename'].unique():
                if filename not in image_paths:
                    continue
                
                img_path = image_paths[filename]
                img = cv2.imread(img_path)
                if img is None:
                    continue
                
                h, w = img.shape[:2]
                image_labels = df[df['filename'] == filename]
                
                yolo_lines = []
                for _, row in image_labels.iterrows():
                    class_id = int(row['label'])
                    xmin, ymin, xmax, ymax = map(float, [
                        row['xmin'], row['ymin'], row['xmax'], row['ymax']
                    ])
                    
                    # YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì •ê·œí™”ëœ ì¤‘ì‹¬ì¢Œí‘œì™€ í¬ê¸°)
                    x_center = ((xmin + xmax) / 2) / w
                    y_center = ((ymin + ymax) / 2) / h
                    width_norm = (xmax - xmin) / w
                    height_norm = (ymax - ymin) / h
                    
                    yolo_lines.append(f"{class_id} {x_center} {y_center} {width_norm} {height_norm}")
                    total_labels += 1
                
                # YOLO ë¼ë²¨ íŒŒì¼ ì €ì¥
                label_filename = os.path.splitext(filename)[0] + '.txt'
                label_path = output_dir / label_filename
                
                with open(label_path, 'w') as f:
                    f.write('\n'.join(yolo_lines))
                
                conversion_summary[filename] = len(yolo_lines)
                processed_images += 1
            
            # ZIP íŒŒì¼ ìƒì„± (ì˜µì…˜)
            download_url = None
            if create_zip and processed_images > 0:
                import zipfile
                zip_filename = f"roboflow_to_yolo_{timestamp}.zip"
                zip_path = output_dir.parent / zip_filename
                
                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for txt_file in output_dir.glob("*.txt"):
                        zipf.write(txt_file, txt_file.name)
                
                download_url = f"/api/v1/utilities/download/roboflow/{zip_filename}"
            
            return RoboflowConversionResponse(
                success=True,
                message=f"Roboflow â†’ YOLO ë³€í™˜ ì™„ë£Œ: {processed_images}ê°œ ì´ë¯¸ì§€, {total_labels}ê°œ ë¼ë²¨",
                processed_images=processed_images,
                total_labels=total_labels,
                conversion_summary=conversion_summary,
                download_url=download_url
            )
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Roboflow ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def tm_to_pixel(x: float, y: float, tfw: List[float]) -> tuple:
    """TM ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜"""
    A, D, B, E, C, F = tfw
    px = (x - C) / A
    py = (y - F) / E
    return px, py


# ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ë“¤
@router.get("/download/verification/{filename}")
async def download_verification_result(filename: str):
    """GPS ê²€ì¦ ê²°ê³¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    # ìµœì‹  ê²€ì¦ ê²°ê³¼ í´ë”ì—ì„œ íŒŒì¼ ì°¾ê¸°
    verification_dirs = [d for d in DEFAULT_UTILS_DIR.iterdir() 
                        if d.is_dir() and d.name.startswith("gps_verification_")]
    
    for verification_dir in sorted(verification_dirs, reverse=True):
        file_path = verification_dir / filename
        if file_path.exists():
            return FileResponse(
                path=str(file_path),
                filename=filename,
                media_type='image/png'
            )
    
    raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


@router.get("/download/yolo/{filename}")
async def download_yolo_labels(filename: str):
    """YOLO ë¼ë²¨ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    # ìµœì‹  ë³€í™˜ ê²°ê³¼ í´ë”ì—ì„œ íŒŒì¼ ì°¾ê¸°
    conversion_dirs = [d for d in DEFAULT_UTILS_DIR.iterdir()
                      if d.is_dir() and d.name.startswith("yolo_conversion_")]
    
    for conversion_dir in sorted(conversion_dirs, reverse=True):
        file_path = conversion_dir / filename
        if file_path.exists():
            return FileResponse(
                path=str(file_path),
                filename=filename,
                media_type='text/plain'
            )
    
    raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


@router.get("/download/roboflow/{filename}")
async def download_roboflow_conversion(filename: str):
    """Roboflow ë³€í™˜ ê²°ê³¼ ZIP ë‹¤ìš´ë¡œë“œ"""
    file_path = DEFAULT_UTILS_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/zip'
    )


@router.get("/download/colab/{filename}")
async def download_colab_training_dataset(filename: str):
    """Google Colab ë”¥ëŸ¬ë‹ í›ˆë ¨ìš© ZIP ë‹¤ìš´ë¡œë“œ"""
    file_path = DEFAULT_COLAB_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/zip'
    )


@router.get("/status")
async def utilities_status():
    """ìœ í‹¸ë¦¬í‹° ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´"""
    
    # ìµœê·¼ ì‘ì—… ë‚´ì—­ ì¡°íšŒ
    recent_operations = []
    
    if DEFAULT_UTILS_DIR.exists():
        for operation_dir in sorted(DEFAULT_UTILS_DIR.iterdir(), reverse=True)[:10]:
            if operation_dir.is_dir():
                file_count = len(list(operation_dir.glob("*")))
                recent_operations.append({
                    "operation": operation_dir.name,
                    "timestamp": operation_dir.stat().st_mtime,
                    "file_count": file_count
                })
    
    return {
        "service": "utilities",
        "status": "active",
        "available_tools": [
            "GPS ì¢Œí‘œ ê²€ì¦ (verify_gps_to_pixel)",
            "CSV â†’ YOLO ë¼ë²¨ ë³€í™˜ (csv_to_yolo_labels)",
            "Roboflow â†’ YOLO ë³€í™˜ (roboflow_to_yolo)",
            "ë”¥ëŸ¬ë‹ í›ˆë ¨ìš© ZIP ìƒì„± (create_training_zip)"
        ],
        "recent_operations": recent_operations,
        "supported_formats": {
            "images": list(config.ALLOWED_IMAGE_EXTENSIONS),
            "coordinates": [".csv", ".tfw"],
            "output": [".png", ".txt", ".zip"]
        }
    }


# ë”¥ëŸ¬ë‹ í›ˆë ¨ìš© ZIP ìƒì„± ì‘ë‹µ ëª¨ë¸
class TrainingZipResponse(BaseModel):
    success: bool
    message: str
    zip_filename: str
    dataset_info: Dict[str, Any]  # intì—ì„œ Anyë¡œ ë³€ê²½í•˜ì—¬ ì¶”ê°€ ì •ë³´ í¬í•¨ ê°€ëŠ¥
    download_url: str

@router.post("/create_training_zip", response_model=TrainingZipResponse)
async def create_training_dataset_zip(
    train_split: float = Form(default=0.8, description="í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (0.0~1.0)"),
    auto_split: bool = Form(default=True, description="YOLO ìë™ ë¶„í•  ì‚¬ìš© ì—¬ë¶€"),
    class_names: str = Form(default="damaged_tree", description="í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    shuffle_data: bool = Form(default=True, description="ë°ì´í„° ì…”í”Œ ì—¬ë¶€"),
    max_files: int = Form(default=0, description="ìµœëŒ€ íŒŒì¼ ìˆ˜ (0=ì „ì²´, ì–‘ìˆ˜=ì œí•œ)"),
    source_prefix: str = Form(default="", description="íŒŒì¼ëª… ì ‘ë‘ì‚¬ í•„í„° (ì˜ˆ: A20250915, B20250915)")
):
    """
    íƒ€ì¼ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ Google Colab ë”¥ëŸ¬ë‹ í›ˆë ¨ìš© ZIP íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    
    - **train_split**: í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (0.8 = 80% í›ˆë ¨, 20% ê²€ì¦)
    - **auto_split**: Trueë©´ YOLOê°€ ìë™ìœ¼ë¡œ ë¶„í•  (ê°„ë‹¨í•œ êµ¬ì¡°)
    - **class_names**: í´ë˜ìŠ¤ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)
    - **shuffle_data**: ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì„ì„ì§€ ì—¬ë¶€
    - **max_files**: ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (0=ì „ì²´ ì‚¬ìš©)
    - **source_prefix**: íŠ¹ì • ë‚ ì§œ/ë°°ì¹˜ì˜ íƒ€ì¼ë§Œ ì‚¬ìš© (ì˜ˆ: "A20250915", "B20250915")
    
    íŒŒì¼ëª… í˜•ì‹: {ì ‘ë‘ì‚¬}_tile_{x}_{y}.tif (ì˜ˆ: A20250915_tile_0_0.tif)
    """
    
    try:
        import zipfile
        import random
        import yaml
        from pathlib import Path
        
        # ì†ŒìŠ¤ ë°ì´í„° ê²½ë¡œ - API ì¶œë ¥ ë””ë ‰í† ë¦¬ì—ì„œ ìµœì‹  íƒ€ì¼ ì°¾ê¸°
        project_root = Path(__file__).parent.parent.parent
        api_tiles_base = project_root / "data" / "api_outputs" / "tiles"
        
        # ìµœì‹  íƒ€ì¼ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        if not api_tiles_base.exists():
            raise HTTPException(status_code=404, detail="API íƒ€ì¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íƒ€ì¼ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # íƒ€ì¼ ë””ë ‰í† ë¦¬ë“¤ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìµœì‹  ê²ƒ ì°¾ê¸°
        tile_dirs = [d for d in api_tiles_base.iterdir() if d.is_dir() and d.name.startswith('tiles_')]
        if not tile_dirs:
            raise HTTPException(status_code=404, detail="ìƒì„±ëœ íƒ€ì¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íƒ€ì¼ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ê°€ì¥ ìµœê·¼ íƒ€ì¼ ë””ë ‰í† ë¦¬ ì„ íƒ
        latest_tile_dir = max(tile_dirs, key=lambda x: x.stat().st_mtime)
        tiles_images_dir = latest_tile_dir / "images"
        tiles_labels_dir = latest_tile_dir / "labels"
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ - Colab ë”¥ëŸ¬ë‹ ì „ìš© í´ë”
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = DEFAULT_COLAB_DIR / f"training_dataset_{timestamp}"
        zip_filename = f"pinetree_training_dataset_{timestamp}.zip"
        zip_path = DEFAULT_COLAB_DIR / zip_filename
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        DEFAULT_COLAB_DIR.mkdir(parents=True, exist_ok=True)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ìˆ˜ì§‘ (ê°œì„ ëœ ë¡œì§)
        image_files = []
        
        # ì§€ì›í•˜ëŠ” ì´ë¯¸ì§€ í™•ì¥ì
        image_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']
        
        # ìµœì¢… ê²½ë¡œ ê²€ì¦
        if not tiles_images_dir.exists():
            raise HTTPException(status_code=404, detail=f"íƒ€ì¼ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tiles_images_dir}")
        
        if not tiles_labels_dir.exists():
            raise HTTPException(status_code=404, detail=f"íƒ€ì¼ ë¼ë²¨ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tiles_labels_dir}")
        
        # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘ (í•„í„°ë§ ì ìš©)
        for ext in image_extensions:
            if source_prefix:
                # íŠ¹ì • ì ‘ë‘ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ë§Œ
                pattern = f"{source_prefix}*{ext}"
                image_files.extend(list(tiles_images_dir.glob(pattern)))
            else:
                # ëª¨ë“  íŒŒì¼
                image_files.extend(list(tiles_images_dir.glob(f"*{ext}")))
        
        if len(image_files) == 0:
            if source_prefix:
                raise HTTPException(status_code=404, detail=f"'{source_prefix}' ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íƒ€ì¼ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ìµœì‹  íŒŒì¼ë“¤ë§Œ ì‚¬ìš© (íŒŒì¼ ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬)
        image_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        # ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ ì ìš©
        if max_files > 0 and len(image_files) > max_files:
            image_files = image_files[:max_files]
            print(f"ğŸ“Š íŒŒì¼ ìˆ˜ë¥¼ {max_files}ê°œë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.")
        
        # 2. ì´ë¯¸ì§€-ë¼ë²¨ ë§¤ì¹­
        matched_pairs = []
        unmatched_images = []
        
        for image_file in image_files:
            label_file = tiles_labels_dir / f"{image_file.stem}.txt"
            
            if label_file.exists():
                matched_pairs.append((image_file, label_file))
            else:
                unmatched_images.append(image_file)
        
        if len(matched_pairs) == 0:
            raise HTTPException(status_code=404, detail="ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€-ë¼ë²¨ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # 3. ë°ì´í„° ì…”í”Œ
        if shuffle_data:
            random.shuffle(matched_pairs)
        
        # 4. í´ë˜ìŠ¤ ì´ë¦„ íŒŒì‹±
        class_list = [name.strip() for name in class_names.split(',')]
        if len(class_list) == 0:
            class_list = ['damaged_tree']
        
        # 5. ë°ì´í„°ì…‹ êµ¬ì¡° ìƒì„±
        if auto_split:
            # ê°„ë‹¨í•œ êµ¬ì¡° (YOLO ìë™ ë¶„í• )
            images_dir = output_dir / "images"
            labels_dir = output_dir / "labels"
            
            images_dir.mkdir()
            labels_dir.mkdir()
            
            # ëª¨ë“  íŒŒì¼ ë³µì‚¬
            for img_file, label_file in matched_pairs:
                shutil.copy2(img_file, images_dir / img_file.name)
                shutil.copy2(label_file, labels_dir / label_file.name)
            
            # ì˜ˆìƒ ë¶„í•  ìˆ˜ ê³„ì‚° (YOLO ìë™ë¶„í•  ê¸°ì¤€)
            expected_train_count = int(len(matched_pairs) * train_split)
            expected_val_count = len(matched_pairs) - expected_train_count
            
            dataset_info = {
                "train_images": expected_train_count,
                "train_labels": expected_train_count,
                "val_images": expected_val_count,
                "val_labels": expected_val_count,
                "total_pairs": len(matched_pairs),
                "split_ratio": f"{train_split:.1f}:{1-train_split:.1f}",
                "auto_split": True
            }
            
            # data.yaml ìƒì„± (ìë™ ë¶„í• )
            data_yaml = {
                'path': '/content/dataset',
                'train': 'images',
                'val': 'images',  # YOLOê°€ ìë™ìœ¼ë¡œ ë¶„í• 
                'nc': len(class_list),
                'names': class_list
            }
            
        else:
            # ìˆ˜ë™ ë¶„í•  êµ¬ì¡°
            split_idx = int(len(matched_pairs) * train_split)
            train_pairs = matched_pairs[:split_idx]
            val_pairs = matched_pairs[split_idx:]
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            train_images_dir = output_dir / "images"
            train_labels_dir = output_dir / "labels"
            val_images_dir = output_dir / "val_images"
            val_labels_dir = output_dir / "val_labels"
            
            for directory in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:
                directory.mkdir(parents=True)
            
            # í›ˆë ¨ ë°ì´í„° ë³µì‚¬
            for img_file, label_file in train_pairs:
                shutil.copy2(img_file, train_images_dir / img_file.name)
                shutil.copy2(label_file, train_labels_dir / label_file.name)
            
            # ê²€ì¦ ë°ì´í„° ë³µì‚¬
            for img_file, label_file in val_pairs:
                shutil.copy2(img_file, val_images_dir / img_file.name)
                shutil.copy2(label_file, val_labels_dir / label_file.name)
            
            dataset_info = {
                "train_images": len(train_pairs),
                "train_labels": len(train_pairs),
                "val_images": len(val_pairs),
                "val_labels": len(val_pairs),
                "total_pairs": len(matched_pairs),
                "split_ratio": f"{train_split:.1f}:{1-train_split:.1f}",
                "auto_split": False
            }
            
            # data.yaml ìƒì„± (ìˆ˜ë™ ë¶„í• )
            data_yaml = {
                'path': '/content/dataset',
                'train': 'images',
                'val': 'val_images',
                'nc': len(class_list),
                'names': class_list
            }
        
        # 6. data.yaml íŒŒì¼ ì €ì¥
        with open(output_dir / "data.yaml", 'w', encoding='utf-8') as f:
            yaml.dump(data_yaml, f, default_flow_style=False, allow_unicode=True)
        
        # 7. README.txt ìƒì„± (ì‚¬ìš©ë²• ì•ˆë‚´)
        readme_content = f"""ğŸŒ² ì†Œë‚˜ë¬´ì¬ì„ ì¶©ë³‘ ë”¥ëŸ¬ë‹ í›ˆë ¨ ë°ì´í„°ì…‹

ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:
- ì´ ì´ë¯¸ì§€-ë¼ë²¨ ìŒ: {dataset_info['total_pairs']}ê°œ
- í›ˆë ¨ ë°ì´í„°: {dataset_info['train_images']}ê°œ
- ê²€ì¦ ë°ì´í„°: {dataset_info['val_images']}ê°œ {'(ìë™ ë¶„í• )' if auto_split else ''}
- í´ë˜ìŠ¤ ìˆ˜: {len(class_list)}ê°œ
- í´ë˜ìŠ¤: {', '.join(class_list)}

ğŸš€ Google Colab ì‚¬ìš©ë²•:
1. ì´ ZIP íŒŒì¼ì„ Google Driveì— ì—…ë¡œë“œ
2. Colabì—ì„œ ë‹¤ìŒ ì½”ë“œ ì‹¤í–‰:

```python
# Google Drive ë§ˆìš´íŠ¸
from google.colab import drive
drive.mount('/content/drive')

# ZIP íŒŒì¼ ì••ì¶• í•´ì œ
!unzip -q "/content/drive/MyDrive/{zip_filename}" -d /content/dataset

# ë°ì´í„° í™•ì¸
!ls -la /content/dataset/

# YOLOv8 ì„¤ì¹˜ ë° í›ˆë ¨
!pip install ultralytics
from ultralytics import YOLO

model = YOLO('yolov8s.pt')
results = model.train(
    data='/content/dataset/data.yaml',
    epochs=200,
    imgsz=1024,
    batch=16,
    device=0,
    cache=True
)
```

ğŸ“‹ data.yaml ì„¤ì •:
{yaml.dump(data_yaml, default_flow_style=False)}

ìƒì„± ì‹œê°„: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        with open(output_dir / "README.txt", 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        # 8. ZIP íŒŒì¼ ìƒì„±
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    file_path = Path(root) / file
                    arcname = file_path.relative_to(output_dir)
                    zipf.write(file_path, arcname)
        
        # 9. ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        shutil.rmtree(output_dir)
        
        # 10. ë‹¤ìš´ë¡œë“œ URL ìƒì„± (Colab ì „ìš© ì—”ë“œí¬ì¸íŠ¸)
        download_url = f"/api/v1/utilities/download/colab/{zip_filename}"
        
        return TrainingZipResponse(
            success=True,
            message=f"ë”¥ëŸ¬ë‹ í›ˆë ¨ìš© ZIP íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {dataset_info['total_pairs']}ê°œì˜ ì´ë¯¸ì§€-ë¼ë²¨ ìŒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
            zip_filename=zip_filename,
            dataset_info=dataset_info,
            download_url=download_url
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ZIP íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


# íƒ€ì¼ ë°ì´í„° ê´€ë¦¬ ì‘ë‹µ ëª¨ë¸
class TilesManagementResponse(BaseModel):
    success: bool
    message: str
    tiles_info: Dict[str, Any]

@router.get("/tiles_info", response_model=TilesManagementResponse)
async def get_tiles_information():
    """
    í˜„ì¬ íƒ€ì¼ ë°ì´í„° ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    íŒŒì¼ ìˆ˜, í¬ê¸°, ìƒì„± ì‹œê°„ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        project_root = Path(__file__).parent.parent.parent
        tiles_images_dir = project_root / "data" / "tiles" / "images"
        tiles_labels_dir = project_root / "data" / "tiles" / "labels"
        
        tiles_info = {
            "images": {"count": 0, "size_mb": 0, "files": []},
            "labels": {"count": 0, "size_mb": 0, "files": []},
            "matched_pairs": 0,
            "directories_exist": {
                "images": tiles_images_dir.exists(),
                "labels": tiles_labels_dir.exists()
            }
        }
        
        # ì´ë¯¸ì§€ íŒŒì¼ ì •ë³´
        if tiles_images_dir.exists():
            image_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']
            image_files = []
            
            for ext in image_extensions:
                image_files.extend(list(tiles_images_dir.glob(f"*{ext}")))
            
            total_size = sum(f.stat().st_size for f in image_files)
            
            # íŒŒì¼ë³„ ìƒì„¸ ì •ë³´ (ìµœì‹  10ê°œë§Œ)
            image_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            file_details = []
            for f in image_files[:10]:
                file_details.append({
                    "name": f.name,
                    "size_mb": round(f.stat().st_size / (1024*1024), 2),
                    "created": datetime.datetime.fromtimestamp(f.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S")
                })
            
            tiles_info["images"] = {
                "count": len(image_files),
                "size_mb": round(total_size / (1024*1024), 2),
                "files": file_details
            }
        
        # ë¼ë²¨ íŒŒì¼ ì •ë³´
        if tiles_labels_dir.exists():
            label_files = list(tiles_labels_dir.glob("*.txt"))
            total_size = sum(f.stat().st_size for f in label_files)
            
            label_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            file_details = []
            for f in label_files[:10]:
                file_details.append({
                    "name": f.name,
                    "size_kb": round(f.stat().st_size / 1024, 2),
                    "created": datetime.datetime.fromtimestamp(f.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S")
                })
            
            tiles_info["labels"] = {
                "count": len(label_files),
                "size_mb": round(total_size / (1024*1024), 2),
                "files": file_details
            }
            
            # ë§¤ì¹­ëœ ìŒ ê³„ì‚°
            if tiles_images_dir.exists():
                image_stems = {f.stem for f in tiles_images_dir.glob("*") if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif', '.tiff']}
                label_stems = {f.stem for f in label_files}
                tiles_info["matched_pairs"] = len(image_stems & label_stems)
        
        return TilesManagementResponse(
            success=True,
            message="íƒ€ì¼ ë°ì´í„° ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
            tiles_info=tiles_info
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íƒ€ì¼ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.delete("/clear_tiles")
async def clear_tiles_data(
    confirm: bool = Query(..., description="ì •ë§ë¡œ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (true ì…ë ¥ í•„ìˆ˜)")
):
    """
    ëˆ„ì ëœ íƒ€ì¼ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤.
    âš ï¸ ì£¼ì˜: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!
    """
    try:
        if not confirm:
            raise HTTPException(status_code=400, detail="ì‚­ì œë¥¼ í™•ì¸í•˜ë ¤ë©´ confirm=trueë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        
        project_root = Path(__file__).parent.parent.parent
        tiles_dir = project_root / "data" / "tiles"
        
        deleted_info = {
            "images_deleted": 0,
            "labels_deleted": 0,
            "total_size_mb": 0
        }
        
        if tiles_dir.exists():
            # ì‚­ì œ ì „ ì •ë³´ ìˆ˜ì§‘
            images_dir = tiles_dir / "images"
            labels_dir = tiles_dir / "labels"
            
            if images_dir.exists():
                image_files = list(images_dir.glob("*"))
                deleted_info["images_deleted"] = len(image_files)
                deleted_info["total_size_mb"] += sum(f.stat().st_size for f in image_files if f.is_file()) / (1024*1024)
            
            if labels_dir.exists():
                label_files = list(labels_dir.glob("*.txt"))
                deleted_info["labels_deleted"] = len(label_files)
                deleted_info["total_size_mb"] += sum(f.stat().st_size for f in label_files) / (1024*1024)
            
            # ë””ë ‰í† ë¦¬ ì „ì²´ ì‚­ì œ
            shutil.rmtree(tiles_dir)
            
            # ë¹ˆ ë””ë ‰í† ë¦¬ ì¬ìƒì„±
            tiles_dir.mkdir(parents=True, exist_ok=True)
            (tiles_dir / "images").mkdir(exist_ok=True)
            (tiles_dir / "labels").mkdir(exist_ok=True)
        
        return {
            "success": True,
            "message": f"íƒ€ì¼ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ {deleted_info['images_deleted']}ê°œ, ë¼ë²¨ {deleted_info['labels_deleted']}ê°œ (ì´ {deleted_info['total_size_mb']:.1f}MB)",
            "deleted_info": deleted_info
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íƒ€ì¼ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
