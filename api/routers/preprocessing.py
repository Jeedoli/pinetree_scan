# ì „ì²˜ë¦¬ API ë¼ìš°í„° - tile_and_label.py ê¸°ë°˜  
# ëŒ€ìš©ëŸ‰ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  + YOLO ë¼ë²¨ ìë™ ìƒì„±

from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import os
import tempfile
import shutil
import pandas as pd
import numpy as np
import rasterio
from rasterio.transform import Affine
from rasterio.windows import Window
import datetime
import zipfile
import yaml
import random
import sys
import logging
from pathlib import Path
from api import config

router = APIRouter()# Pydantic ëª¨ë¸ ì •ì˜

# ì „ì²˜ë¦¬ API ë¼ìš°í„° - tile_and_label.py ê¸°ë°˜
# ëŒ€ìš©ëŸ‰ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  + YOLO ë¼ë²¨ ìë™ ìƒì„±

from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import os
import tempfile
import shutil
import pandas as pd
import numpy as np
import rasterio
from rasterio.transform import Affine
from rasterio.windows import Window
import datetime
import zipfile
import yaml
import random
import sys
import logging
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from .. import config

router = APIRouter()

# ì‘ë‹µ ëª¨ë¸ ì •ì˜
class TileInfo(BaseModel):
    tile_name: str
    labels_count: int
    tile_size: tuple
    position: tuple  # (tx, ty)

class InferenceTileInfo(BaseModel):
    tile_name: str
    tile_size: tuple
    position: tuple  # (tx, ty)
    has_georeference: bool

class IntegratedTrainingResponse(BaseModel):
    success: bool
    message: str
    preprocessing_info: Dict
    training_dataset_info: Dict
    download_url: str
    zip_filename: str

class InferenceTilingResponse(BaseModel):
    success: bool
    message: str
    total_tiles: int
    original_size: tuple
    tile_size: int
    tile_info: List[InferenceTileInfo]
    download_url: Optional[str] = None

# ê¸°ë³¸ ì„¤ì •
DEFAULT_TILE_SIZE = config.DEFAULT_TILE_SIZE
# DEFAULT_BBOX_SIZE = config.DEFAULT_BBOX_SIZE  # âŒ ì œê±° - Multi-Scale Detection ì§€ì›
DEFAULT_CLASS_ID = config.DEFAULT_CLASS_ID
DEFAULT_OUTPUT_DIR = Path(config.API_TILES_DIR)  # Path ê°ì²´ë¡œ ë³€ê²½

# ğŸŒ² Multi-Scale Dynamic Bounding Box ì„¤ì •
MIN_BBOX_SIZE = config.MIN_BBOX_SIZE  # 10px
MAX_BBOX_SIZE = config.MAX_BBOX_SIZE  # 200px
DEFAULT_BBOX_SIZES = config.DEFAULT_BBOX_SIZES  # [16, 32, 64, 128]

# í—¬í¼ í•¨ìˆ˜ë“¤
def load_tfw(tfw_path):
    """TFW íŒŒì¼ì—ì„œ ë³€í™˜ íŒŒë¼ë¯¸í„° ì½ê¸°"""
    with open(tfw_path) as f:
        vals = [float(x.strip()) for x in f.readlines()]
    return vals

def tm_to_pixel(x, y, tfw):
    """TM ì¢Œí‘œ(x, y)ë¥¼ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œ(px, py)ë¡œ ë³€í™˜"""
    A, D, B, E, C, F = tfw
    px = (x - C) / A
    py = (y - F) / E
    return px, py

def calculate_dynamic_bbox_size(row, default_size=32):
    """
    ğŸŒ² CSV í–‰ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ê³„ì‚°
    
    Args:
        row: CSV í–‰ ë°ì´í„° (pandas Series)
        default_size: ê¸°ë³¸ í¬ê¸° (ì •ë³´ê°€ ì—†ì„ ê²½ìš°)
    
    Returns:
        int: ê³„ì‚°ëœ ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° (MIN_BBOX_SIZE ~ MAX_BBOX_SIZE ë²”ìœ„)
    """
    base_size = default_size
    
    try:
        # ğŸ¯ í”¼í•´ëª© íŠ¹ì„± ê¸°ë°˜ í¬ê¸° ì¡°ì • ë¡œì§
        
        # 1. ë‚˜ì´/í¬ê¸° ì •ë³´ê°€ ìˆëŠ” ê²½ìš° (age, dbh, height ë“±)
        if 'age' in row and pd.notna(row.get('age')):
            age = float(row['age'])
            # ë‚˜ì´ì— ë”°ë¥¸ í¬ê¸° ì¡°ì • (1ë…„ = 1.5px ì¶”ê°€)
            age_factor = min(age * 1.5, 80)  # ìµœëŒ€ 80pxê¹Œì§€ ì¦ê°€
            base_size += age_factor
        
        elif 'dbh' in row and pd.notna(row.get('dbh')):  # ì§ê²½ ì •ë³´
            dbh = float(row['dbh'])
            # DBH(cm) * 2 = bbox í¬ê¸° ì¡°ì •
            dbh_factor = min(dbh * 2, 100)
            base_size += dbh_factor
            
        elif 'height' in row and pd.notna(row.get('height')):  # ë†’ì´ ì •ë³´
            height = float(row['height'])
            # ë†’ì´(m) * 3 = bbox í¬ê¸° ì¡°ì •
            height_factor = min(height * 3, 90)
            base_size += height_factor
        
        # 2. í”¼í•´ ì •ë„ì— ë”°ë¥¸ í¬ê¸° ì¡°ì •
        if 'damage_level' in row and pd.notna(row.get('damage_level')):
            damage = row['damage_level']
            if isinstance(damage, str):
                # í…ìŠ¤íŠ¸ ê¸°ë°˜ í”¼í•´ ë“±ê¸‰
                damage_multiplier = {
                    'light': 0.8, 'mild': 0.85, 'moderate': 1.0,
                    'severe': 1.2, 'heavy': 1.3, 'critical': 1.4,
                    'ê²½ë¯¸': 0.8, 'ë³´í†µ': 1.0, 'ì‹¬í•¨': 1.2, 'ë§¤ìš°ì‹¬í•¨': 1.4
                }.get(damage.lower(), 1.0)
            else:
                # ìˆ«ì ê¸°ë°˜ í”¼í•´ ë“±ê¸‰ (0-5 ìŠ¤ì¼€ì¼)
                damage_multiplier = 0.7 + (float(damage) * 0.15)
            
            base_size *= damage_multiplier
        
        # 3. ìˆ˜ì¢…ì— ë”°ë¥¸ í¬ê¸° ì¡°ì • (ì†Œë‚˜ë¬´ ê³„ì—´)
        if 'species' in row and pd.notna(row.get('species')):
            species = str(row['species']).lower()
            species_multiplier = {
                'pine': 1.0, 'pinus': 1.0, 'ì†Œë‚˜ë¬´': 1.0,
                'red_pine': 1.1, 'ì ì†¡': 1.1,
                'black_pine': 1.2, 'í‘ì†¡': 1.2,
                'young': 0.7, 'ìœ ëª©': 0.7,
                'mature': 1.3, 'ì„±ëª©': 1.3,
                'old': 1.5, 'ê³ ëª©': 1.5
            }.get(species, 1.0)
            base_size *= species_multiplier
        
        # 4. ê¸°ë³¸ í¬ê¸° ë‹¤ì–‘ì„± ì¶”ê°€ (ë™ì¼ í¬ê¸° ë°©ì§€)
        # ìœ„ì¹˜ ê¸°ë°˜ ì•½ê°„ì˜ ëœë¤ì„± (ì¬í˜„ ê°€ëŠ¥)
        if 'x' in row and 'y' in row:
            position_hash = hash(f"{row.get('x', 0)}{row.get('y', 0)}") % 100
            size_variation = (position_hash / 100 - 0.5) * 8  # Â±4px ë³€ë™
            base_size += size_variation
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í¬ê¸° ì‚¬ìš©
        logging.warning(f"Dynamic bbox size calculation error: {e}")
        pass
    
    # ìµœì¢… í¬ê¸° ì œí•œ ë° ì •ìˆ˜ ë³€í™˜
    final_size = int(max(MIN_BBOX_SIZE, min(base_size, MAX_BBOX_SIZE)))
    
    return final_size

def get_multi_scale_bbox_sizes(row, num_scales=3):
    """
    ğŸ¯ Multi-Scale Detectionì„ ìœ„í•œ ë‹¤ì¤‘ ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ìƒì„±
    
    Args:
        row: CSV í–‰ ë°ì´í„°
        num_scales: ìƒì„±í•  ìŠ¤ì¼€ì¼ ìˆ˜
    
    Returns:
        list: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ë°”ìš´ë”©ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    base_size = calculate_dynamic_bbox_size(row)
    
    # ê¸°ë³¸ í¬ê¸°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ìƒì„±
    scales = [0.7, 1.0, 1.4]  # ì‘ì€, ê¸°ë³¸, í° í¬ê¸°
    if num_scales >= 4:
        scales = [0.6, 0.8, 1.0, 1.3, 1.6]  # ë” ì„¸ë°€í•œ ìŠ¤ì¼€ì¼
    
    bbox_sizes = []
    for scale in scales[:num_scales]:
        size = int(base_size * scale)
        size = max(MIN_BBOX_SIZE, min(size, MAX_BBOX_SIZE))
        bbox_sizes.append(size)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    bbox_sizes = sorted(list(set(bbox_sizes)))
    
    return bbox_sizes

def process_tiles_and_labels(image_path, tfw_params, df, output_images, output_labels, 
                           tile_size, bbox_size, class_id, file_prefix):
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ (deprecated)"""
    return process_tiles_and_labels_multiscale(
        image_path, tfw_params, df, output_images, output_labels, 
        tile_size, class_id, file_prefix, enable_multiscale=False
    )

def process_tiles_and_labels_multiscale(image_path, tfw_params, df, output_images, output_labels, 
                                       tile_size, class_id, file_prefix, enable_multiscale=True):
    """
    ğŸŒ² Multi-Scale ì´ë¯¸ì§€ íƒ€ì¼ ë¶„í•  ë° ë™ì  YOLO ë¼ë²¨ ìƒì„±
    
    Args:
        enable_multiscale: Trueì‹œ ë™ì  ë°”ìš´ë”©ë°•ìŠ¤, Falseì‹œ ê¸°ë³¸ 32px
    """
    tile_info = []
    
    with rasterio.open(image_path) as src:
        width, height = src.width, src.height
        n_tiles_x = int(np.ceil(width / tile_size))
        n_tiles_y = int(np.ceil(height / tile_size))
        
        total_tiles = n_tiles_x * n_tiles_y
        processed_tiles = 0
        
        print(f"ğŸ¯ íƒ€ì¼ ë¶„í•  ì‹œì‘: {width}x{height} â†’ {n_tiles_x}x{n_tiles_y} = {total_tiles}ê°œ íƒ€ì¼", flush=True)
        print(f"ğŸ“Š GPS í¬ì¸íŠ¸ {len(df)}ê°œë¥¼ {total_tiles}ê°œ íƒ€ì¼ì— ë¶„ë°° ì¤‘...", flush=True)
        
        for ty in range(n_tiles_y):
            for tx in range(n_tiles_x):
                x0 = tx * tile_size
                y0 = ty * tile_size
                w = min(tile_size, width - x0)
                h = min(tile_size, height - y0)
                window = Window(x0, y0, w, h)
                
                # RGB 3ì±„ë„ë§Œ ì¶”ì¶œí•˜ì—¬ íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„±
                tile_img = src.read([1, 2, 3], window=window)
                tile_name = f"{file_prefix}_{tx}_{ty}.tif"
                
                # ğŸ¯ Multi-Scale íƒ€ì¼ ë‚´ bbox ë¼ë²¨ ìƒì„±
                lines = []
                
                # ğŸš€ ì„±ëŠ¥ ìµœì í™”: íƒ€ì¼ ì˜ì—­ì— í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì¢Œí‘œë§Œ í•„í„°ë§
                # ì „ì²´ ì¢Œí‘œë¥¼ í”½ì…€ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ ê³„ì‚°)
                if not hasattr(process_tiles_and_labels_multiscale, '_pixel_coords'):
                    print("ğŸ”„ GPS ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜ ì¤‘... (ìµœì´ˆ 1íšŒ)", flush=True)
                    pixel_coords = []
                    for _, row in df.iterrows():
                        px, py = tm_to_pixel(row["x"], row["y"], tfw_params)
                        pixel_coords.append((px, py, row))
                    process_tiles_and_labels_multiscale._pixel_coords = pixel_coords
                    print(f"âœ… {len(pixel_coords)}ê°œ ì¢Œí‘œ ë³€í™˜ ì™„ë£Œ", flush=True)
                
                # í˜„ì¬ íƒ€ì¼ ì˜ì—­ì— í¬í•¨ë˜ëŠ” ì¢Œí‘œë§Œ ì²˜ë¦¬
                for px, py, row in process_tiles_and_labels_multiscale._pixel_coords:
                    # íƒ€ì¼ ë‚´ ìƒëŒ€ì¢Œí‘œë¡œ ë³€í™˜
                    rel_x = px - x0
                    rel_y = py - y0
                    if 0 <= rel_x < w and 0 <= rel_y < h:
                        x_center = rel_x / w
                        y_center = rel_y / h
                        
                        if enable_multiscale:
                            # ğŸŒ² ë™ì  ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ê³„ì‚°
                            dynamic_size = calculate_dynamic_bbox_size(row, default_size=32)
                            
                            # Multi-Scale ë¼ë²¨ ìƒì„± (ì£¼ ìŠ¤ì¼€ì¼ë§Œ ì‚¬ìš© - ì¤‘ë³µ ë°©ì§€)
                            bw = dynamic_size / w
                            bh = dynamic_size / h
                            
                            lines.append(
                                f"{class_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}"
                            )
                        else:
                            # ê¸°ì¡´ ê³ ì • í¬ê¸° (í˜¸í™˜ì„±)
                            bbox_size = 32
                            bw = bbox_size / w
                            bh = bbox_size / h
                            lines.append(
                                f"{class_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}"
                            )
                
                # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
                labels_count = len(lines)
                saved = labels_count > 0
                
                # ì§„í–‰ìƒí™© ì¶œë ¥ (10% ë‹¨ìœ„)
                processed_tiles += 1
                if processed_tiles % max(1, total_tiles // 10) == 0 or processed_tiles == total_tiles:
                    progress_pct = (processed_tiles / total_tiles) * 100
                    tiles_with_labels = sum(1 for t in tile_info if hasattr(t, 'labels_count') and t.labels_count > 0) + (1 if saved else 0)
                    print(f"  ğŸ“Š ì§„í–‰ìƒí™©: {processed_tiles}/{total_tiles} ({progress_pct:.1f}%) - ë¼ë²¨ ìˆëŠ” íƒ€ì¼: {tiles_with_labels}ê°œ", flush=True)
                
                if saved:
                    # ë¼ë²¨ íŒŒì¼ ì €ì¥
                    out_lbl_path = os.path.join(output_labels, tile_name.replace(".tif", ".txt"))
                    with open(out_lbl_path, "w") as f:
                        f.write("\n".join(lines))
                    
                    # íƒ€ì¼ ì´ë¯¸ì§€ ì €ì¥
                    out_img_path = os.path.join(output_images, tile_name)
                    orig_affine = src.transform
                    tile_affine = orig_affine * Affine.translation(x0, y0)
                    with rasterio.open(
                        out_img_path,
                        "w",
                        driver="GTiff",
                        height=h,
                        width=w,
                        count=3,
                        dtype=src.dtypes[0],
                        crs=src.crs,
                        transform=tile_affine,
                    ) as dst:
                        dst.write(tile_img)
                
                # íƒ€ì¼ ì •ë³´ ì €ì¥ (TileInfo ëª¨ë¸ì— ë§ê²Œ ì¡°ì •)
                tile_info.append(TileInfo(
                    tile_name=tile_name,
                    labels_count=labels_count,
                    tile_size=(w, h),
                    position=(tx, ty)
                ))
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    tiles_with_labels = sum(1 for t in tile_info if t.labels_count > 0)
    total_labels = sum(t.labels_count for t in tile_info)
    print(f"âœ… íƒ€ì¼ ë¶„í•  ì™„ë£Œ: {total_tiles}ê°œ íƒ€ì¼ ìƒì„±, {tiles_with_labels}ê°œ ë¼ë²¨ íƒ€ì¼, {total_labels}ê°œ ì´ ë¼ë²¨", flush=True)
    
    return tile_info

@router.get("/download/{filename}")
async def download_tiles_zip(filename: str):
    """
    ìƒì„±ëœ íƒ€ì¼ê³¼ ë¼ë²¨ ZIP íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    file_path = DEFAULT_OUTPUT_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type='application/zip'
    )


@router.get("/status")
async def preprocessing_status():
    """
    ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ì˜ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ìµœê·¼ ìƒì„±ëœ íƒ€ì¼ í´ë”ë“¤ ì¡°íšŒ
    recent_tiles = []
    if DEFAULT_OUTPUT_DIR.exists():
        for item in sorted(os.listdir(DEFAULT_OUTPUT_DIR), reverse=True)[:5]:
            item_path = DEFAULT_OUTPUT_DIR / item
            if item_path.is_dir():
                images_dir = item_path / "images"
                labels_dir = item_path / "labels"
                images_count = len([f for f in os.listdir(images_dir) 
                                 if f.endswith('.tif')]) if images_dir.exists() else 0
                labels_count = len([f for f in os.listdir(labels_dir)
                                 if f.endswith('.txt')]) if labels_dir.exists() else 0
                
                recent_tiles.append({
                    "folder_name": item,
                    "images_count": images_count,
                    "labels_count": labels_count
                })
    
    return {
        "service": "preprocessing",
        "status": "active",
        "recent_tiles": recent_tiles,
        "default_settings": {
            "tile_size": DEFAULT_TILE_SIZE,
            "bbox_sizes": DEFAULT_BBOX_SIZES,  # Multi-Scale ì§€ì›
            "bbox_range": f"{MIN_BBOX_SIZE}-{MAX_BBOX_SIZE}px",
            "class_id": DEFAULT_CLASS_ID
        }
    }


@router.post("/tile-for-inference", response_model=InferenceTilingResponse)
async def create_inference_tiles(
    image_file: UploadFile = File(..., description="ì›ë³¸ GeoTIFF ì´ë¯¸ì§€"),
    tile_size: int = Form(default=DEFAULT_TILE_SIZE, description="íƒ€ì¼ í¬ê¸° (í”½ì…€)")
):
    """
    ì¶”ë¡ ìš© íƒ€ì¼ ìƒì„±: ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•´ íƒ€ì¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ì›ë³¸ ì´ë¯¸ì§€ë§Œ ë¶„í• í•˜ë©°, ì§€ë¦¬ì°¸ì¡° ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
    
    - **image_file**: ì›ë³¸ GeoTIFF ì´ë¯¸ì§€ íŒŒì¼ (.tif/.tiff)
    - **tile_size**: íƒ€ì¼ í•œ ë³€ í¬ê¸° (ê¸°ë³¸: 1024í”½ì…€)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(('.tif', '.tiff')):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ì€ TIFF í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            
            # ë‚ ì§œ ê¸°ë°˜ ì ‘ë‘ì‚¬ ìƒì„±
            today = datetime.datetime.now().strftime("%Y%m%d")
            
            # ê°™ì€ ë‚ ì§œì˜ ê¸°ì¡´ í´ë”ë“¤ í™•ì¸í•˜ì—¬ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì •
            existing_prefixes = []
            if os.path.exists(DEFAULT_OUTPUT_DIR):
                for folder in os.listdir(DEFAULT_OUTPUT_DIR):
                    if folder.startswith(f"inference_tiles_") and today in folder:
                        # inference_tiles_A20250915_ í˜•íƒœì—ì„œ ì ‘ë‘ì‚¬ ì¶”ì¶œ
                        parts = folder.split('_')
                        if len(parts) >= 3 and parts[2].startswith(('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')):
                            prefix_char = parts[2][0]
                            existing_prefixes.append(prefix_char)
            
            # ë‹¤ìŒ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì • (A, B, C, ... Z ìˆœì„œ)
            next_prefix = 'A'
            for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
                if char not in existing_prefixes:
                    next_prefix = char
                    break
            
            # íŒŒì¼ ì ‘ë‘ì‚¬ ìƒì„±
            file_prefix = f"{next_prefix}{today}"
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_base = DEFAULT_OUTPUT_DIR / f"inference_tiles_{file_prefix}_{timestamp}"
            output_images = output_base / "images"
            
            output_images.mkdir(parents=True, exist_ok=True)
            
            # ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ì‹¤í–‰
            tile_info, original_size = process_inference_tiles(
                image_path, str(output_images), tile_size, file_prefix
            )
            
            # ZIP íŒŒì¼ ìƒì„±
            zip_path = create_inference_tiles_zip(output_base, timestamp)
            
            # í†µê³„ ê³„ì‚°
            total_tiles = len(tile_info)
            
            return InferenceTilingResponse(
                success=True,
                message=f"ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ì™„ë£Œ: {total_tiles}ê°œ íƒ€ì¼ ìƒì„±",
                total_tiles=total_tiles,
                original_size=original_size,
                tile_size=tile_size,
                tile_info=tile_info,
                download_url=f"/api/v1/preprocessing/download/{os.path.basename(zip_path)}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¶”ë¡ ìš© íƒ€ì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def process_inference_tiles(
    image_path: str, output_images: str, tile_size: int, file_prefix: str
) -> tuple[List[InferenceTileInfo], tuple]:
    """ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ë©”ì¸ ë¡œì§ (ë¼ë²¨ë§ ì—†ìŒ)"""
    
    tile_info = []
    
    with rasterio.open(image_path) as src:
        width, height = src.width, src.height
        original_size = (width, height)
        
        n_tiles_x = int(np.ceil(width / tile_size))
        n_tiles_y = int(np.ceil(height / tile_size))
        
        for ty in range(n_tiles_y):
            for tx in range(n_tiles_x):
                x0 = tx * tile_size
                y0 = ty * tile_size
                w = min(tile_size, width - x0)
                h = min(tile_size, height - y0)
                
                window = Window(x0, y0, w, h)
                
                # RGB 3ì±„ë„ë§Œ ì¶”ì¶œí•˜ì—¬ íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„±
                tile_img = src.read([1, 2, 3], window=window)
                tile_name = f"{file_prefix}_tile_{tx}_{ty}.tif"
                
                # íƒ€ì¼ ì´ë¯¸ì§€ ì €ì¥ (ì§€ë¦¬ì°¸ì¡° ì •ë³´ ë³´ì¡´)
                tile_path = os.path.join(output_images, tile_name)
                orig_affine = src.transform
                tile_affine = orig_affine * Affine.translation(x0, y0)
                
                has_georeference = src.crs is not None and src.transform is not None
                
                with rasterio.open(
                    tile_path, 'w',
                    driver='GTiff',
                    height=h, width=w, count=3,
                    dtype=src.dtypes[0],
                    crs=src.crs,
                    transform=tile_affine
                ) as dst:
                    dst.write(tile_img)
                
                # íƒ€ì¼ ì •ë³´ ì¶”ê°€
                tile_info.append(InferenceTileInfo(
                    tile_name=tile_name,
                    tile_size=(w, h),
                    position=(tx, ty),
                    has_georeference=has_georeference
                ))
    
    return tile_info, original_size


def create_inference_tiles_zip(output_base: Path, timestamp: str) -> str:
    """ìƒì„±ëœ ì¶”ë¡ ìš© íƒ€ì¼ì„ ZIP íŒŒì¼ë¡œ ì••ì¶•"""
    zip_filename = f"inference_tiles_{timestamp}.zip"
    zip_path = DEFAULT_OUTPUT_DIR / zip_filename
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # images í´ë” ì••ì¶•
        images_dir = output_base / "images"
        if images_dir.exists():
            for root, dirs, files in os.walk(images_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    archive_name = os.path.relpath(file_path, output_base)
                    zipf.write(file_path, archive_name)
    
    return str(zip_path)


@router.post("/create-dataset", response_model=IntegratedTrainingResponse)
async def create_dataset(
    image_file: UploadFile = File(..., description="ì²˜ë¦¬í•  GeoTIFF ì´ë¯¸ì§€ íŒŒì¼"),
    csv_file: UploadFile = File(..., description="GPS ì¢Œí‘œ CSV íŒŒì¼ (x, y ë˜ëŠ” longitude, latitude ì»¬ëŸ¼)"),
    tfw_file: UploadFile = File(..., description="ì§€ë¦¬ì°¸ì¡°ë¥¼ ìœ„í•œ TFW íŒŒì¼"),
    file_prefix: str = Form(..., description="ìƒì„±ë  íƒ€ì¼ íŒŒì¼ëª… ì ‘ë‘ì‚¬"),
    tile_size: int = Form(default=config.DEFAULT_TILE_SIZE, description="íƒ€ì¼ í¬ê¸° (í”½ì…€)"),
    enable_multiscale: bool = Form(default=True, description="Multi-Scale ë™ì  ë°”ìš´ë”©ë°•ìŠ¤ ì‚¬ìš© (True: ë™ì , False: 32px ê³ ì •)"),
    base_bbox_size: int = Form(default=32, description="ê¸°ë³¸ ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° (Multi-Scale ë¹„í™œì„±í™”ì‹œë§Œ ì‚¬ìš©)"),
    class_id: int = Form(default=0, description="YOLO í´ë˜ìŠ¤ ID"),
    train_split: float = Form(default=0.8, description="í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (0.0-1.0)"),
    class_names: str = Form(default="damaged_tree", description="í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    max_files: int = Form(default=1000, description="ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (0=ë¬´ì œí•œ)"),
    shuffle_data: bool = Form(default=True, description="ë°ì´í„° ì…”í”Œ ì—¬ë¶€"),
    auto_split: bool = Form(default=True, description="ìë™ train/val ë¶„í•  ì—¬ë¶€")
):
    """
    ğŸš€ **í†µí•© ë”¥ëŸ¬ë‹ ë°ì´í„°ì…‹ ìƒì„± API** (ê¶Œì¥)
    
    **ì´ APIëŠ” ë‹¤ìŒ ì‘ì—…ì„ í•œë²ˆì— ìˆ˜í–‰í•©ë‹ˆë‹¤:**
    1. ğŸ–¼ï¸ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  (ëª¨ë“  ì˜ì—­ í¬í•¨)
    2. ğŸ·ï¸ GPS ì¢Œí‘œ ê¸°ë°˜ YOLO ë¼ë²¨ ìë™ ìƒì„±  
    3. ï¿½ Positive/Negative ìƒ˜í”Œ ê· í˜• ë°ì´í„°ì…‹ ìƒì„±
    4. ï¿½ğŸ“¦ Google Colab ìµœì í™” ë”¥ëŸ¬ë‹ìš© ZIP íŒŒì¼ ìƒì„±
    5. ğŸ“Š Train/Validation ë°ì´í„°ì…‹ ìë™ ë¶„í• 
    6. ğŸ“‹ README ë° ì‚¬ìš©ë²• ê°€ì´ë“œ í¬í•¨
    
    **ğŸ¯ ê°œì„ ëœ ë°ì´í„°ì…‹ íŠ¹ì§•:**
    - âœ… **ê· í˜• ì¡íŒ ë°ì´í„°**: í”¼í•´ëª©ì´ ìˆëŠ” ì˜ì—­ + ê±´ê°•í•œ ì‚°ë¦¼ ì˜ì—­
    - âœ… **Negative ìƒ˜í”Œ í¬í•¨**: ì˜¤íƒì§€ ë°©ì§€ë¥¼ ìœ„í•œ ìŒì„± ìƒ˜í”Œ ìë™ ìƒì„±
    - âœ… **í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°**: ì–‘ì„±/ìŒì„± ìƒ˜í”Œ ë¹„ìœ¨ ì •ë³´ ì œê³µ
    - âœ… **YOLO ì™„ë²½ í˜¸í™˜**: ë¹ˆ ë¼ë²¨ íŒŒì¼ë¡œ negative ìƒ˜í”Œ ì²˜ë¦¬
    
    **ğŸ“‹ ë§¤ê°œë³€ìˆ˜:**
    5. ğŸ“‹ README ë° ì‚¬ìš©ë²• ê°€ì´ë“œ í¬í•¨
    
    **ğŸ“‹ ë§¤ê°œë³€ìˆ˜:**
    - **image_file**: ì²˜ë¦¬í•  GeoTIFF ì´ë¯¸ì§€ íŒŒì¼ (.tif/.tiff)
    - **csv_file**: GPS ì¢Œí‘œ CSV íŒŒì¼ (x,y ë˜ëŠ” longitude,latitude ì»¬ëŸ¼ í•„ìš”)
    - **tfw_file**: ì§€ë¦¬ì°¸ì¡°ë¥¼ ìœ„í•œ TFW íŒŒì¼ (.tfw)
    - **file_prefix**: ìƒì„±ë  íƒ€ì¼ íŒŒì¼ëª… ì ‘ë‘ì‚¬ (ì˜ˆ: "A20250919")
    - **tile_size**: íƒ€ì¼ í¬ê¸° (ê¸°ë³¸: 1024px, ì„¸ë°€í•œ íƒì§€ ìµœì í™”)
    - **enable_multiscale**: ğŸŒ² Multi-Scale ë™ì  ë°”ìš´ë”©ë°•ìŠ¤ ì‚¬ìš© (ê¸°ë³¸: True)
      - `True`: CSV ë°ì´í„° ê¸°ë°˜ ë™ì  í¬ê¸° ê³„ì‚° (10-200px ë²”ìœ„)
      - `False`: ê³ ì • í¬ê¸° ì‚¬ìš© (í˜¸í™˜ì„± ëª¨ë“œ)
    - **base_bbox_size**: ê³ ì • ëª¨ë“œì‹œ ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° (ê¸°ë³¸: 32px)
    - **class_id**: YOLO í´ë˜ìŠ¤ ID (ê¸°ë³¸: 0)
    - **train_split**: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.8 = 80% í•™ìŠµ, 20% ê²€ì¦)
    - **class_names**: í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸: "damaged_tree")
    - **max_files**: ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (ê¸°ë³¸: 1000ê°œ, 0=ë¬´ì œí•œ)
    - **shuffle_data**: ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ê¸°ë³¸: True)
    - **auto_split**: ìë™ train/val ë¶„í•  ì—¬ë¶€ (ê¸°ë³¸: True, YOLO í˜¸í™˜)
    
    **ğŸ¯ ì¶œë ¥:**
    - ZIP íŒŒì¼ì— í¬í•¨: images/, labels/, data.yaml, README.txt
    - Google Colabì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
    - YOLOv8/YOLOv11 í˜¸í™˜ í˜•ì‹
    
    **ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:**
    ```python
    # Google Colabì—ì„œ ì‚¬ìš©ë²•
    !unzip -q "/content/drive/MyDrive/pinetree_training_dataset_*.zip" -d /content/dataset
    from ultralytics import YOLO
    model = YOLO('yolo11s.pt')
    results = model.train(data='/content/dataset/data.yaml', epochs=200)
    ```
    
    **ğŸ“‹ ì¶”ê°€ ë§¤ê°œë³€ìˆ˜:**
    - **class_names**: í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸: "damaged_tree")
    - **max_files**: ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (ê¸°ë³¸: 1000ê°œ, 0=ë¬´ì œí•œ)
    - **shuffle_data**: ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ê¸°ë³¸: True)
    - **auto_split**: ìë™ train/val ë¶„í•  ì—¬ë¶€ (ê¸°ë³¸: True, YOLO í˜¸í™˜)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(tuple(config.ALLOWED_IMAGE_EXTENSIONS)):
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {image_file.filename}"
            )
        
        if not csv_file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not tfw_file.filename.lower().endswith('.tfw'):
            raise HTTPException(status_code=400, detail="TFW íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_base = DEFAULT_OUTPUT_DIR / f"complete_training_{timestamp}"
        output_base.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        tiles_images_dir = output_base / "tiles" / "images"
        tiles_labels_dir = output_base / "tiles" / "labels" 
        tiles_images_dir.mkdir(parents=True, exist_ok=True)
        tiles_labels_dir.mkdir(parents=True, exist_ok=True)
        
        # ìµœì¢… ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        dataset_dir = output_base / "training_dataset"
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            csv_path = os.path.join(temp_dir, csv_file.filename)
            tfw_path = os.path.join(temp_dir, tfw_file.filename)
            
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            with open(csv_path, "wb") as f:
                shutil.copyfileobj(csv_file.file, f)
            with open(tfw_path, "wb") as f:
                shutil.copyfileobj(tfw_file.file, f)
            
            # TFW íŒŒë¼ë¯¸í„° íŒŒì‹±
            tfw_params = []
            with open(tfw_path, 'r') as f:
                tfw_params = [float(line.strip()) for line in f.readlines()]
            
            if len(tfw_params) != 6:
                raise HTTPException(status_code=400, detail="TFW íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(csv_path)
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # ì¢Œí‘œ ì»¬ëŸ¼ í™•ì¸
            x_col, y_col = None, None
            if 'x' in df.columns and 'y' in df.columns:
                x_col, y_col = 'x', 'y'
            elif 'longitude' in df.columns and 'latitude' in df.columns:
                x_col, y_col = 'longitude', 'latitude'
            else:
                raise HTTPException(
                    status_code=400, 
                    detail="CSV íŒŒì¼ì— 'x,y' ë˜ëŠ” 'longitude,latitude' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            
            # ì¢Œí‘œ ì»¬ëŸ¼ ì´ë¦„ í†µì¼
            df = df.rename(columns={x_col: 'x', y_col: 'y'})
            
            print(f"ğŸ“Š GPS ì¢Œí‘œ ë°ì´í„°: {len(df)}ê°œ í¬ì¸íŠ¸", flush=True)
            
            # Step 1: ğŸŒ² Multi-Scale íƒ€ì¼ë§ ë° ë¼ë²¨ ìƒì„±
            print(f"ğŸ”„ Step 1: {'Multi-Scale ë™ì ' if enable_multiscale else 'ê³ ì • í¬ê¸°'} íƒ€ì¼ë§ ë° ë¼ë²¨ ìƒì„± ì‹œì‘...", flush=True)
            
            if enable_multiscale:
                print("ğŸ¯ Multi-Scale ë™ì  ë°”ìš´ë”©ë°•ìŠ¤ ëª¨ë“œ í™œì„±í™”", flush=True)
                tile_info = process_tiles_and_labels_multiscale(
                    image_path=image_path,
                    tfw_params=tfw_params,
                    df=df,
                    output_images=str(tiles_images_dir),
                    output_labels=str(tiles_labels_dir),
                    tile_size=tile_size,
                    class_id=class_id,
                    file_prefix=file_prefix,
                    enable_multiscale=True
                )
            else:
                print(f"ğŸ“¦ ê³ ì • í¬ê¸° ë°”ìš´ë”©ë°•ìŠ¤ ëª¨ë“œ: {base_bbox_size}px", flush=True)
                tile_info = process_tiles_and_labels(
                    image_path=image_path,
                    tfw_params=tfw_params,
                    df=df,
                    output_images=str(tiles_images_dir),
                    output_labels=str(tiles_labels_dir),
                    tile_size=tile_size,
                    bbox_size=base_bbox_size,
                    class_id=class_id,
                    file_prefix=file_prefix
                )
            
            tiles_with_labels = sum(1 for t in tile_info if t.labels_count > 0)
            total_labels = sum(t.labels_count for t in tile_info)
            
            preprocessing_info = {
                "total_tiles": len(tile_info),
                "tiles_with_labels": tiles_with_labels,
                "total_labels": total_labels,
                "tile_size": tile_size,
                "multiscale_enabled": enable_multiscale,
                "bbox_mode": "Multi-Scale Dynamic" if enable_multiscale else f"Fixed {base_bbox_size}px",
                "bbox_range": f"{MIN_BBOX_SIZE}-{MAX_BBOX_SIZE}px" if enable_multiscale else f"{base_bbox_size}px",
                "file_prefix": file_prefix
            }
            
            print(f"âœ… Step 1 ì™„ë£Œ: {len(tile_info)}ê°œ íƒ€ì¼, {tiles_with_labels}ê°œ ë¼ë²¨ íƒ€ì¼, {total_labels}ê°œ ì´ ë¼ë²¨", flush=True)
            
            # Step 2: ë”¥ëŸ¬ë‹ìš© ë°ì´í„°ì…‹ ìƒì„±
            print("ğŸ”„ Step 2: ë”¥ëŸ¬ë‹ìš© ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...", flush=True)
            
            # íƒ€ì¼ ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ë§¤ì¹­ (ëª¨ë“  ì´ë¯¸ì§€ í¬í•¨)
            image_files = list(tiles_images_dir.glob("*.tif"))
            label_files = list(tiles_labels_dir.glob("*.txt"))
            
            # ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ ë¼ë²¨ íŒŒì¼ ë§¤ì¹­ (ì—†ìœ¼ë©´ ë¹ˆ ë¼ë²¨ë¡œ ì²˜ë¦¬)
            matched_files = []
            positive_samples = 0  # ë¼ë²¨ì´ ìˆëŠ” ìƒ˜í”Œ ìˆ˜
            negative_samples = 0  # ë¼ë²¨ì´ ì—†ëŠ” ìƒ˜í”Œ ìˆ˜
            
            for img_file in image_files:
                label_file = tiles_labels_dir / f"{img_file.stem}.txt"
                
                # ë¼ë²¨ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¼ë²¨ íŒŒì¼ ìƒì„±
                if not label_file.exists():
                    with open(label_file, 'w') as f:
                        pass  # ë¹ˆ íŒŒì¼ ìƒì„±
                    negative_samples += 1
                else:
                    # ë¼ë²¨ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ë‚´ìš© í™•ì¸
                    with open(label_file, 'r') as f:
                        content = f.read().strip()
                    if content:
                        positive_samples += 1
                    else:
                        negative_samples += 1
                
                matched_files.append((img_file, label_file))
            
            print(f"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {len(matched_files)}ê°œ")
            print(f"ğŸ“Š ì–‘ì„± ìƒ˜í”Œ (í”¼í•´ëª© ìˆìŒ): {positive_samples}ê°œ")
            print(f"ğŸ“Š ìŒì„± ìƒ˜í”Œ (í”¼í•´ëª© ì—†ìŒ): {negative_samples}ê°œ")
            print(f"ğŸ“Š í´ë˜ìŠ¤ ë¹„ìœ¨: Positive={positive_samples/(positive_samples+negative_samples)*100:.1f}%, Negative={negative_samples/(positive_samples+negative_samples)*100:.1f}%")
            
            if len(matched_files) == 0:
                raise HTTPException(
                    status_code=400, 
                    detail="íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            # ë°ì´í„° ì…”í”Œ
            if shuffle_data:
                import random
                random.shuffle(matched_files)
            
            # íŒŒì¼ ìˆ˜ ì œí•œ
            if max_files > 0:
                matched_files = matched_files[:max_files]
            
            # train/val ë¶„í• 
            split_idx = int(len(matched_files) * train_split) if auto_split else len(matched_files)
            train_files = matched_files[:split_idx]
            val_files = matched_files[split_idx:] if auto_split else []
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            for split_name in ['train', 'val'] if auto_split else ['train']:
                (dataset_dir / split_name / 'images').mkdir(parents=True, exist_ok=True)
                (dataset_dir / split_name / 'labels').mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ë³µì‚¬
            for img_file, label_file in train_files:
                shutil.copy2(img_file, dataset_dir / 'train' / 'images')
                shutil.copy2(label_file, dataset_dir / 'train' / 'labels')
            
            if auto_split and val_files:
                for img_file, label_file in val_files:
                    shutil.copy2(img_file, dataset_dir / 'val' / 'images')
                    shutil.copy2(label_file, dataset_dir / 'val' / 'labels')
            
            # data.yaml ìƒì„±
            class_list = [name.strip() for name in class_names.split(',')]
            data_yaml = {
                'path': '.',
                'train': 'train/images',
                'val': 'val/images' if auto_split else 'train/images',
                'nc': len(class_list),
                'names': class_list,
                
                # ë©”íƒ€ë°ì´í„°
                'created_date': datetime.datetime.now().isoformat(),
                'source_info': {
                    'image_file': image_file.filename,
                    'csv_file': csv_file.filename,
                    'total_coordinates': len(df),
                    'tile_size': tile_size,
                    'multiscale_enabled': enable_multiscale,
                    'bbox_mode': "Multi-Scale Dynamic" if enable_multiscale else f"Fixed {base_bbox_size}px"
                },
                'dataset_stats': {
                    'total_files': len(matched_files),
                    'train_files': len(train_files),
                    'val_files': len(val_files),
                    'total_labels': total_labels
                }
            }
            
            import yaml
            with open(dataset_dir / 'data.yaml', 'w') as f:
                yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)
            
            # Step 3: ZIP íŒŒì¼ ìƒì„±
            print("ğŸ”„ Step 3: ZIP íŒŒì¼ ìƒì„± ì‹œì‘...", flush=True)
            zip_filename = f"complete_training_dataset_{file_prefix}_{timestamp}.zip"
            zip_path = DEFAULT_OUTPUT_DIR / zip_filename
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ì „ì²´ë¥¼ ì••ì¶•
                for root, dirs, files in os.walk(dataset_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        archive_name = os.path.relpath(file_path, dataset_dir)
                        zipf.write(file_path, archive_name)
            
            # ìµœì¢… ì •ë³´ êµ¬ì„±
            dataset_info = {
                "zip_filename": zip_filename,
                "total_files": len(matched_files),
                "train_files": len(train_files),
                "val_files": len(val_files),
                "total_labels": total_labels,
                "classes": class_list,
                "data_yaml_created": True,
                "file_size_mb": round(zip_path.stat().st_size / (1024 * 1024), 2)
            }
            
            download_url = f"/api/v1/preprocessing/download/{zip_filename}"
            
            print(f"âœ… í†µí•© ì²˜ë¦¬ ì™„ë£Œ!", flush=True)
            print(f"ğŸ“¦ ZIP íŒŒì¼: {zip_filename} ({dataset_info['file_size_mb']}MB)", flush=True)
            
            return IntegratedTrainingResponse(
                success=True,
                message=f"í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ! ì´ {len(matched_files)}ê°œ íŒŒì¼, {total_labels}ê°œ ë¼ë²¨",
                preprocessing_info=preprocessing_info,
                training_dataset_info=dataset_info,
                download_url=download_url,
                zip_filename=zip_filename
            )
            
    except Exception as e:
        print(f"âŒ í†µí•© ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", flush=True)
        raise HTTPException(status_code=500, detail=f"í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.get("/download/{filename}")
async def download_processed_file(filename: str):
    """ì²˜ë¦¬ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    file_path = DEFAULT_OUTPUT_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/octet-stream'
    )
