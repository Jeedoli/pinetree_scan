# ì „ì²˜ë¦¬ API ë¼ìš°í„° - tile_and_label.py ê¸°ë°˜  
# ëŒ€ìš©ëŸ‰ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  + YOLO ë¼ë²¨ ìë™ ìƒì„±

from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import os
import tempfile
import shutil
import pandas as pd
import numpy as np
import rasterio
from rasterio.transform import Affine
from rasterio.windows import Window
import datetime
import zipfile
import yaml
import random
import sys
import logging
from pathlib import Path
from api import config

router = APIRouter()# Pydantic ëª¨ë¸ ì •ì˜

# ì „ì²˜ë¦¬ API ë¼ìš°í„° - tile_and_label.py ê¸°ë°˜
# ëŒ€ìš©ëŸ‰ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  + YOLO ë¼ë²¨ ìë™ ìƒì„±

from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import os
import tempfile
import shutil
import pandas as pd
import numpy as np
import rasterio
from rasterio.transform import Affine
from rasterio.windows import Window
import datetime
import zipfile
import yaml
import random
import sys
import logging
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from .. import config

router = APIRouter()

# ì‘ë‹µ ëª¨ë¸ ì •ì˜
class TileInfo(BaseModel):
    tile_name: str
    labels_count: int
    tile_size: tuple
    position: tuple  # (tx, ty)

class InferenceTileInfo(BaseModel):
    tile_name: str
    tile_size: tuple
    position: tuple  # (tx, ty)
    has_georeference: bool

class PreprocessingResponse(BaseModel):
    success: bool
    message: str
    total_tiles: int
    tiles_with_labels: int
    total_labels: int
    tile_info: List[TileInfo]
    download_url: Optional[str] = None

class IntegratedTrainingResponse(BaseModel):
    success: bool
    message: str
    preprocessing_info: Dict
    training_dataset_info: Dict
    download_url: str
    zip_filename: str

class InferenceTilingResponse(BaseModel):
    success: bool
    message: str
    total_tiles: int
    original_size: tuple
    tile_size: int
    tile_info: List[InferenceTileInfo]
    download_url: Optional[str] = None

# ê¸°ë³¸ ì„¤ì •
DEFAULT_TILE_SIZE = config.DEFAULT_TILE_SIZE
DEFAULT_BBOX_SIZE = config.DEFAULT_BBOX_SIZE
DEFAULT_CLASS_ID = config.DEFAULT_CLASS_ID
DEFAULT_OUTPUT_DIR = Path(config.API_TILES_DIR)  # Path ê°ì²´ë¡œ ë³€ê²½

@router.post("/tile_and_label", response_model=PreprocessingResponse)
async def create_tiles_and_labels(
    image_file: UploadFile = File(..., description="ì›ë³¸ GeoTIFF ì´ë¯¸ì§€"),
    tfw_file: UploadFile = File(..., description="ì¢Œí‘œ ë³€í™˜ íŒŒì¼ (.tfw)"),
    csv_file: UploadFile = File(..., description="í”¼í•´ëª© ìœ„ì¹˜ CSV íŒŒì¼"),
    tile_size: int = Form(default=DEFAULT_TILE_SIZE, description="íƒ€ì¼ í¬ê¸° (í”½ì…€)"),
    bbox_size: int = Form(default=DEFAULT_BBOX_SIZE, description="ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° (í”½ì…€)"),
    class_id: int = Form(default=DEFAULT_CLASS_ID, description="YOLO í´ë˜ìŠ¤ ID")
):
    """
    âš ï¸ **ë ˆê±°ì‹œ API**: ì´ APIëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë©ë‹ˆë‹¤.
    **ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ëŠ” `/create_complete_training_dataset` API ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.**
    
    ëŒ€ìš©ëŸ‰ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í• í•˜ê³  YOLO ë¼ë²¨ì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
    (ë”¥ëŸ¬ë‹ìš© ZIP íŒŒì¼ ìƒì„±ì€ ë³„ë„ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨)
    
    **ğŸ“‹ ë§¤ê°œë³€ìˆ˜:**
    - **image_file**: ì›ë³¸ GeoTIFF ì´ë¯¸ì§€ íŒŒì¼ (.tif/.tiff)
    - **tfw_file**: ì¢Œí‘œ ë³€í™˜ íŒŒì¼ (.tfw)
    - **csv_file**: í”¼í•´ëª© ìœ„ì¹˜ê°€ ë‹´ê¸´ CSV íŒŒì¼ (x, y ì»¬ëŸ¼ í•„ìš”)
    - **tile_size**: íƒ€ì¼ í•œ ë³€ í¬ê¸° (ê¸°ë³¸: 1024í”½ì…€, ì„¸ë°€í•œ íƒì§€ ìµœì í™”)
    - **bbox_size**: ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° (ê¸°ë³¸: 24í”½ì…€, ê°œë³„ ë‚˜ë¬´ íƒì§€ìš©)
    - **class_id**: YOLO í´ë˜ìŠ¤ ID (ê¸°ë³¸: 0)
    
    **ğŸš€ ê¶Œì¥ ì‚¬ìš©ë²•:**
    í†µí•© API `/create_complete_training_dataset`ë¥¼ ì‚¬ìš©í•˜ë©´:
    - íƒ€ì¼ë§ + YOLO ë¼ë²¨ ìƒì„± + ë”¥ëŸ¬ë‹ìš© ZIP ìƒì„±ì„ í•œ ë²ˆì— ì²˜ë¦¬
    - Google Colab ìµœì í™”ëœ ë°ì´í„°ì…‹ êµ¬ì¡°
    - ìë™ train/validation ë¶„í• 
    - README ë° ì‚¬ìš©ë²• ê°€ì´ë“œ í¬í•¨
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(('.tif', '.tiff')):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ì€ TIFF í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        if not tfw_file.filename.lower().endswith('.tfw'):
            raise HTTPException(status_code=400, detail="ì¢Œí‘œ ë³€í™˜ íŒŒì¼ì€ .tfw í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        if not csv_file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="í”¼í•´ëª© ìœ„ì¹˜ íŒŒì¼ì€ CSV í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # ì—…ë¡œë“œ íŒŒì¼ë“¤ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            tfw_path = os.path.join(temp_dir, tfw_file.filename)
            csv_path = os.path.join(temp_dir, csv_file.filename)
            
            # íŒŒì¼ ì €ì¥
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            with open(tfw_path, "wb") as f:
                shutil.copyfileobj(tfw_file.file, f)
            with open(csv_path, "wb") as f:
                shutil.copyfileobj(csv_file.file, f)
            
            # TFW íŒŒì¼ ë¡œë“œ
            tfw_params = load_tfw(tfw_path)
            
            # CSV íŒŒì¼ ë¡œë“œ ë° ê²€ì¦
            try:
                df = pd.read_csv(csv_path)
                if 'x' not in df.columns or 'y' not in df.columns:
                    raise HTTPException(
                        status_code=400, 
                        detail="CSV íŒŒì¼ì— 'x', 'y' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    )
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # ë‚ ì§œ ê¸°ë°˜ ì ‘ë‘ì‚¬ ìƒì„±
            today = datetime.datetime.now().strftime("%Y%m%d")
            
            # ê°™ì€ ë‚ ì§œì˜ ê¸°ì¡´ í´ë”ë“¤ í™•ì¸í•˜ì—¬ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì •
            existing_prefixes = []
            if os.path.exists(DEFAULT_OUTPUT_DIR):
                for folder in os.listdir(DEFAULT_OUTPUT_DIR):
                    if folder.startswith(f"tiles_") and today in folder:
                        # tiles_A20250915_ í˜•íƒœì—ì„œ ì ‘ë‘ì‚¬ ì¶”ì¶œ
                        parts = folder.split('_')
                        if len(parts) >= 2 and parts[1].startswith(('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')):
                            prefix_char = parts[1][0]
                            existing_prefixes.append(prefix_char)
            
            # ë‹¤ìŒ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì • (A, B, C, ... Z ìˆœì„œ)
            next_prefix = 'A'
            for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
                if char not in existing_prefixes:
                    next_prefix = char
                    break
            
            # íŒŒì¼ ì ‘ë‘ì‚¬ ìƒì„±
            file_prefix = f"{next_prefix}{today}"
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_base = DEFAULT_OUTPUT_DIR / f"tiles_{file_prefix}_{timestamp}"
            output_images = output_base / "images"
            output_labels = output_base / "labels"
            
            output_images.mkdir(parents=True, exist_ok=True)
            output_labels.mkdir(parents=True, exist_ok=True)
            
            # íƒ€ì¼ ë¶„í•  ë° ë¼ë²¨ ìƒì„± ì‹¤í–‰
            tile_info = process_tiles_and_labels(
                image_path, tfw_params, df, 
                str(output_images), str(output_labels),
                tile_size, bbox_size, class_id, file_prefix
            )
            
            # ZIP íŒŒì¼ ìƒì„±
            zip_path = create_tiles_zip(output_base, timestamp)
            
            # í†µê³„ ê³„ì‚°
            total_tiles = len(tile_info)
            tiles_with_labels = len([t for t in tile_info if t.labels_count > 0])
            total_labels = sum(t.labels_count for t in tile_info)
            
            return PreprocessingResponse(
                success=True,
                message=f"íƒ€ì¼ ë¶„í•  ì™„ë£Œ: {total_tiles}ê°œ íƒ€ì¼ ìƒì„±, {total_labels}ê°œ ë¼ë²¨ ìƒì„±",
                total_tiles=total_tiles,
                tiles_with_labels=tiles_with_labels,
                total_labels=total_labels,
                tile_info=tile_info,
                download_url=f"/api/v1/preprocessing/download/{os.path.basename(zip_path)}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def load_tfw(tfw_path: str) -> List[float]:
    """TFW íŒŒì¼ì—ì„œ ë³€í™˜ íŒŒë¼ë¯¸í„° ì½ê¸°"""
    with open(tfw_path, 'r') as f:
        return [float(line.strip()) for line in f.readlines()]


def tm_to_pixel(x: float, y: float, tfw: List[float]) -> tuple:
    """TM ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜"""
    A, D, B, E, C, F = tfw
    px = (x - C) / A
    py = (y - F) / E
    return px, py


def process_tiles_and_labels(
    image_path: str, tfw_params: List[float], df: pd.DataFrame,
    output_images: str, output_labels: str,
    tile_size: int, bbox_size: int, class_id: int, file_prefix: str
) -> List[TileInfo]:
    """íƒ€ì¼ ë¶„í•  ë° ë¼ë²¨ ìƒì„± ë©”ì¸ ë¡œì§"""
    
    tile_info = []
    
    with rasterio.open(image_path) as src:
        width, height = src.width, src.height
        n_tiles_x = int(np.ceil(width / tile_size))
        n_tiles_y = int(np.ceil(height / tile_size))
        total_tiles = n_tiles_x * n_tiles_y
        
        print(f"ğŸ¯ ì´ë¯¸ì§€ í¬ê¸°: {width}x{height} í”½ì…€", flush=True)
        print(f"ğŸ“Š ìƒì„±í•  íƒ€ì¼: {n_tiles_x}x{n_tiles_y} = {total_tiles}ê°œ", flush=True)
        print(f"ğŸ”„ íƒ€ì¼ë§ ì‹œì‘...", flush=True)
        
        processed_tiles = 0
        tiles_with_labels = 0
        
        for ty in range(n_tiles_y):
            for tx in range(n_tiles_x):
                processed_tiles += 1
                
                # ì§„í–‰ë¥  ì¶œë ¥ (10% ë‹¨ìœ„ ë˜ëŠ” 100íƒ€ì¼ë§ˆë‹¤)
                if processed_tiles % max(1, total_tiles // 10) == 0 or processed_tiles % 100 == 0:
                    progress = (processed_tiles / total_tiles) * 100
                    print(f"ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}% ({processed_tiles}/{total_tiles})", flush=True)
                
                x0 = tx * tile_size
                y0 = ty * tile_size
                w = min(tile_size, width - x0)
                h = min(tile_size, height - y0)
                
                window = Window(x0, y0, w, h)
                
                # RGB 3ì±„ë„ë§Œ ì¶”ì¶œí•˜ì—¬ íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„±
                tile_img = src.read([1, 2, 3], window=window)
                tile_name = f"{file_prefix}_tile_{tx}_{ty}.tif"
                
                # íƒ€ì¼ ë‚´ ë¼ë²¨ ìƒì„±
                labels = []
                for _, row in df.iterrows():
                    px, py = tm_to_pixel(row['x'], row['y'], tfw_params)
                    
                    # íƒ€ì¼ ë‚´ ìƒëŒ€ì¢Œí‘œë¡œ ë³€í™˜
                    rel_x = px - x0
                    rel_y = py - y0
                    
                    if 0 <= rel_x < w and 0 <= rel_y < h:
                        # YOLO í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
                        x_center = rel_x / w
                        y_center = rel_y / h
                        bw = bbox_size / w
                        bh = bbox_size / h
                        
                        labels.append(f"{class_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}")
                
                # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
                if labels:
                    tiles_with_labels += 1
                    
                    # ë¼ë²¨ íŒŒì¼ ì €ì¥
                    label_path = os.path.join(output_labels, tile_name.replace(".tif", ".txt"))
                    with open(label_path, 'w') as f:
                        f.write('\n'.join(labels))
                    
                    # ë¼ë²¨ì´ ìˆëŠ” íƒ€ì¼ ë¡œê·¸ (ì²˜ìŒ ëª‡ ê°œë§Œ)
                    if tiles_with_labels <= 5:
                        print(f"ğŸ·ï¸ ë¼ë²¨ íƒ€ì¼ ìƒì„±: {tile_name} ({len(labels)}ê°œ ë¼ë²¨)", flush=True)
                    elif tiles_with_labels % 50 == 0:  # 50ê°œë§ˆë‹¤ ë¡œê·¸
                        print(f"ğŸ·ï¸ ë¼ë²¨ íƒ€ì¼ ëˆ„ì : {tiles_with_labels}ê°œ", flush=True)
                    
                    # íƒ€ì¼ ì´ë¯¸ì§€ ì €ì¥
                    tile_path = os.path.join(output_images, tile_name)
                    orig_affine = src.transform
                    tile_affine = orig_affine * Affine.translation(x0, y0)
                    
                    with rasterio.open(
                        tile_path, 'w',
                        driver='GTiff',
                        height=h, width=w, count=3,
                        dtype=src.dtypes[0],
                        crs=src.crs,
                        transform=tile_affine
                    ) as dst:
                        dst.write(tile_img)
                
                # íƒ€ì¼ ì •ë³´ ì¶”ê°€ (ë¼ë²¨ ìœ ë¬´ ê´€ê³„ì—†ì´)
                tile_info.append(TileInfo(
                    tile_name=tile_name,
                    labels_count=len(labels),
                    tile_size=(w, h),
                    position=(tx, ty)
                ))
    
    return tile_info


def create_tiles_zip(output_base: Path, timestamp: str) -> str:
    """ìƒì„±ëœ íƒ€ì¼ê³¼ ë¼ë²¨ì„ ZIP íŒŒì¼ë¡œ ì••ì¶•"""
    zip_filename = f"tiles_and_labels_{timestamp}.zip"
    zip_path = DEFAULT_OUTPUT_DIR / zip_filename
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # images í´ë” ì••ì¶•
        images_dir = output_base / "images"
        if images_dir.exists():
            for root, dirs, files in os.walk(images_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, output_base)
                    zipf.write(file_path, arcname)
        
        # labels í´ë” ì••ì¶•
        labels_dir = output_base / "labels"
        if labels_dir.exists():
            for root, dirs, files in os.walk(labels_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, output_base)
                    zipf.write(file_path, arcname)
    
    return str(zip_path)


@router.get("/download/{filename}")
async def download_tiles_zip(filename: str):
    """
    ìƒì„±ëœ íƒ€ì¼ê³¼ ë¼ë²¨ ZIP íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    file_path = DEFAULT_OUTPUT_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type='application/zip'
    )


@router.get("/status")
async def preprocessing_status():
    """
    ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ì˜ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ìµœê·¼ ìƒì„±ëœ íƒ€ì¼ í´ë”ë“¤ ì¡°íšŒ
    recent_tiles = []
    if DEFAULT_OUTPUT_DIR.exists():
        for item in sorted(os.listdir(DEFAULT_OUTPUT_DIR), reverse=True)[:5]:
            item_path = DEFAULT_OUTPUT_DIR / item
            if item_path.is_dir():
                images_dir = item_path / "images"
                labels_dir = item_path / "labels"
                images_count = len([f for f in os.listdir(images_dir) 
                                 if f.endswith('.tif')]) if images_dir.exists() else 0
                labels_count = len([f for f in os.listdir(labels_dir)
                                 if f.endswith('.txt')]) if labels_dir.exists() else 0
                
                recent_tiles.append({
                    "folder_name": item,
                    "images_count": images_count,
                    "labels_count": labels_count
                })
    
    return {
        "service": "preprocessing",
        "status": "active",
        "recent_tiles": recent_tiles,
        "default_settings": {
            "tile_size": DEFAULT_TILE_SIZE,
            "bbox_size": DEFAULT_BBOX_SIZE,
            "class_id": DEFAULT_CLASS_ID
        }
    }


@router.post("/tile-for-inference", response_model=InferenceTilingResponse)
async def create_inference_tiles(
    image_file: UploadFile = File(..., description="ì›ë³¸ GeoTIFF ì´ë¯¸ì§€"),
    tile_size: int = Form(default=DEFAULT_TILE_SIZE, description="íƒ€ì¼ í¬ê¸° (í”½ì…€)")
):
    """
    ì¶”ë¡ ìš© íƒ€ì¼ ìƒì„±: ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•´ íƒ€ì¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ì›ë³¸ ì´ë¯¸ì§€ë§Œ ë¶„í• í•˜ë©°, ì§€ë¦¬ì°¸ì¡° ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
    
    - **image_file**: ì›ë³¸ GeoTIFF ì´ë¯¸ì§€ íŒŒì¼ (.tif/.tiff)
    - **tile_size**: íƒ€ì¼ í•œ ë³€ í¬ê¸° (ê¸°ë³¸: 1024í”½ì…€)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(('.tif', '.tiff')):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ì€ TIFF í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            
            # ë‚ ì§œ ê¸°ë°˜ ì ‘ë‘ì‚¬ ìƒì„±
            today = datetime.datetime.now().strftime("%Y%m%d")
            
            # ê°™ì€ ë‚ ì§œì˜ ê¸°ì¡´ í´ë”ë“¤ í™•ì¸í•˜ì—¬ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì •
            existing_prefixes = []
            if os.path.exists(DEFAULT_OUTPUT_DIR):
                for folder in os.listdir(DEFAULT_OUTPUT_DIR):
                    if folder.startswith(f"inference_tiles_") and today in folder:
                        # inference_tiles_A20250915_ í˜•íƒœì—ì„œ ì ‘ë‘ì‚¬ ì¶”ì¶œ
                        parts = folder.split('_')
                        if len(parts) >= 3 and parts[2].startswith(('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')):
                            prefix_char = parts[2][0]
                            existing_prefixes.append(prefix_char)
            
            # ë‹¤ìŒ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì • (A, B, C, ... Z ìˆœì„œ)
            next_prefix = 'A'
            for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
                if char not in existing_prefixes:
                    next_prefix = char
                    break
            
            # íŒŒì¼ ì ‘ë‘ì‚¬ ìƒì„±
            file_prefix = f"{next_prefix}{today}"
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_base = DEFAULT_OUTPUT_DIR / f"inference_tiles_{file_prefix}_{timestamp}"
            output_images = output_base / "images"
            
            output_images.mkdir(parents=True, exist_ok=True)
            
            # ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ì‹¤í–‰
            tile_info, original_size = process_inference_tiles(
                image_path, str(output_images), tile_size, file_prefix
            )
            
            # ZIP íŒŒì¼ ìƒì„±
            zip_path = create_inference_tiles_zip(output_base, timestamp)
            
            # í†µê³„ ê³„ì‚°
            total_tiles = len(tile_info)
            
            return InferenceTilingResponse(
                success=True,
                message=f"ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ì™„ë£Œ: {total_tiles}ê°œ íƒ€ì¼ ìƒì„±",
                total_tiles=total_tiles,
                original_size=original_size,
                tile_size=tile_size,
                tile_info=tile_info,
                download_url=f"/api/v1/preprocessing/download/{os.path.basename(zip_path)}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¶”ë¡ ìš© íƒ€ì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def process_inference_tiles(
    image_path: str, output_images: str, tile_size: int, file_prefix: str
) -> tuple[List[InferenceTileInfo], tuple]:
    """ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ë©”ì¸ ë¡œì§ (ë¼ë²¨ë§ ì—†ìŒ)"""
    
    tile_info = []
    
    with rasterio.open(image_path) as src:
        width, height = src.width, src.height
        original_size = (width, height)
        
        n_tiles_x = int(np.ceil(width / tile_size))
        n_tiles_y = int(np.ceil(height / tile_size))
        
        for ty in range(n_tiles_y):
            for tx in range(n_tiles_x):
                x0 = tx * tile_size
                y0 = ty * tile_size
                w = min(tile_size, width - x0)
                h = min(tile_size, height - y0)
                
                window = Window(x0, y0, w, h)
                
                # RGB 3ì±„ë„ë§Œ ì¶”ì¶œí•˜ì—¬ íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„±
                tile_img = src.read([1, 2, 3], window=window)
                tile_name = f"{file_prefix}_tile_{tx}_{ty}.tif"
                
                # íƒ€ì¼ ì´ë¯¸ì§€ ì €ì¥ (ì§€ë¦¬ì°¸ì¡° ì •ë³´ ë³´ì¡´)
                tile_path = os.path.join(output_images, tile_name)
                orig_affine = src.transform
                tile_affine = orig_affine * Affine.translation(x0, y0)
                
                has_georeference = src.crs is not None and src.transform is not None
                
                with rasterio.open(
                    tile_path, 'w',
                    driver='GTiff',
                    height=h, width=w, count=3,
                    dtype=src.dtypes[0],
                    crs=src.crs,
                    transform=tile_affine
                ) as dst:
                    dst.write(tile_img)
                
                # íƒ€ì¼ ì •ë³´ ì¶”ê°€
                tile_info.append(InferenceTileInfo(
                    tile_name=tile_name,
                    tile_size=(w, h),
                    position=(tx, ty),
                    has_georeference=has_georeference
                ))
    
    return tile_info, original_size


def create_inference_tiles_zip(output_base: Path, timestamp: str) -> str:
    """ìƒì„±ëœ ì¶”ë¡ ìš© íƒ€ì¼ì„ ZIP íŒŒì¼ë¡œ ì••ì¶•"""
    zip_filename = f"inference_tiles_{timestamp}.zip"
    zip_path = DEFAULT_OUTPUT_DIR / zip_filename
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # images í´ë” ì••ì¶•
        images_dir = output_base / "images"
        if images_dir.exists():
            for root, dirs, files in os.walk(images_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    archive_name = os.path.relpath(file_path, output_base)
                    zipf.write(file_path, archive_name)
    
    return str(zip_path)


@router.post("/create_complete_training_dataset", response_model=IntegratedTrainingResponse)
async def create_complete_training_dataset(
    image_file: UploadFile = File(..., description="ì²˜ë¦¬í•  GeoTIFF ì´ë¯¸ì§€ íŒŒì¼"),
    csv_file: UploadFile = File(..., description="GPS ì¢Œí‘œ CSV íŒŒì¼ (x, y ë˜ëŠ” longitude, latitude ì»¬ëŸ¼)"),
    tfw_file: UploadFile = File(..., description="ì§€ë¦¬ì°¸ì¡°ë¥¼ ìœ„í•œ TFW íŒŒì¼"),
    file_prefix: str = Form(..., description="ìƒì„±ë  íƒ€ì¼ íŒŒì¼ëª… ì ‘ë‘ì‚¬"),
    tile_size: int = Form(default=1024, description="íƒ€ì¼ í¬ê¸° (í”½ì…€)"),
    bbox_size: int = Form(default=config.DEFAULT_BBOX_SIZE, description="ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° (í”½ì…€)"),
    class_id: int = Form(default=0, description="YOLO í´ë˜ìŠ¤ ID"),
    train_split: float = Form(default=0.8, description="í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (0.0-1.0)"),
    class_names: str = Form(default="damaged_tree", description="í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    max_files: int = Form(default=1000, description="ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (0=ë¬´ì œí•œ)"),
    shuffle_data: bool = Form(default=True, description="ë°ì´í„° ì…”í”Œ ì—¬ë¶€"),
    auto_split: bool = Form(default=True, description="ìë™ train/val ë¶„í•  ì—¬ë¶€")
):
    """
    ğŸš€ **í†µí•© ë”¥ëŸ¬ë‹ ë°ì´í„°ì…‹ ìƒì„± API** (ê¶Œì¥)
    
    **ì´ APIëŠ” ë‹¤ìŒ ì‘ì—…ì„ í•œë²ˆì— ìˆ˜í–‰í•©ë‹ˆë‹¤:**
    1. ğŸ–¼ï¸ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í• 
    2. ğŸ·ï¸ GPS ì¢Œí‘œ ê¸°ë°˜ YOLO ë¼ë²¨ ìë™ ìƒì„±  
    3. ğŸ“¦ Google Colab ìµœì í™” ë”¥ëŸ¬ë‹ìš© ZIP íŒŒì¼ ìƒì„±
    4. ğŸ“Š Train/Validation ë°ì´í„°ì…‹ ìë™ ë¶„í• 
    5. ğŸ“‹ README ë° ì‚¬ìš©ë²• ê°€ì´ë“œ í¬í•¨
    
    **ğŸ“‹ ë§¤ê°œë³€ìˆ˜:**
    - **image_file**: ì²˜ë¦¬í•  GeoTIFF ì´ë¯¸ì§€ íŒŒì¼ (.tif/.tiff)
    - **csv_file**: GPS ì¢Œí‘œ CSV íŒŒì¼ (x,y ë˜ëŠ” longitude,latitude ì»¬ëŸ¼ í•„ìš”)
    - **tfw_file**: ì§€ë¦¬ì°¸ì¡°ë¥¼ ìœ„í•œ TFW íŒŒì¼ (.tfw)
    - **file_prefix**: ìƒì„±ë  íƒ€ì¼ íŒŒì¼ëª… ì ‘ë‘ì‚¬ (ì˜ˆ: "A20250919")
    - **tile_size**: íƒ€ì¼ í¬ê¸° (ê¸°ë³¸: 1024px, ì„¸ë°€í•œ íƒì§€ ìµœì í™”)
    - **bbox_size**: ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° (ê¸°ë³¸: 24px, ê°œë³„ ë‚˜ë¬´ íƒì§€ ìµœì í™”)
    - **class_id**: YOLO í´ë˜ìŠ¤ ID (ê¸°ë³¸: 0)
    - **train_split**: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.8 = 80% í•™ìŠµ, 20% ê²€ì¦)
    - **class_names**: í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸: "damaged_tree")
    - **max_files**: ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (ê¸°ë³¸: 1000ê°œ, 0=ë¬´ì œí•œ)
    - **shuffle_data**: ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ê¸°ë³¸: True)
    - **auto_split**: ìë™ train/val ë¶„í•  ì—¬ë¶€ (ê¸°ë³¸: True, YOLO í˜¸í™˜)
    
    **ğŸ¯ ì¶œë ¥:**
    - ZIP íŒŒì¼ì— í¬í•¨: images/, labels/, data.yaml, README.txt
    - Google Colabì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
    - YOLOv8/YOLOv11 í˜¸í™˜ í˜•ì‹
    
    **ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:**
    ```python
    # Google Colabì—ì„œ ì‚¬ìš©ë²•
    !unzip -q "/content/drive/MyDrive/pinetree_training_dataset_*.zip" -d /content/dataset
    from ultralytics import YOLO
    model = YOLO('yolo11s.pt')
    results = model.train(data='/content/dataset/data.yaml', epochs=200)
    ```
    
    **âš ï¸ ê¸°ì¡´ APIì™€ì˜ ì°¨ì´ì :**
    - `/tile_and_label`: íƒ€ì¼ë§ê³¼ ë¼ë²¨ë§ë§Œ ìˆ˜í–‰ (ë ˆê±°ì‹œ)
    - `/create_complete_training_dataset`: ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í•œë²ˆì— ìˆ˜í–‰ (ê¶Œì¥)
    
    **ğŸ“‹ ì¶”ê°€ ë§¤ê°œë³€ìˆ˜:**
    - **class_names**: í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸: "damaged_tree")
    - **max_files**: ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (ê¸°ë³¸: 1000ê°œ, 0=ë¬´ì œí•œ)
    - **shuffle_data**: ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ê¸°ë³¸: True)
    - **auto_split**: ìë™ train/val ë¶„í•  ì—¬ë¶€ (ê¸°ë³¸: True, YOLO í˜¸í™˜)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(tuple(config.ALLOWED_IMAGE_EXTENSIONS)):
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {image_file.filename}"
            )
        
        if not csv_file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not tfw_file.filename.lower().endswith('.tfw'):
            raise HTTPException(status_code=400, detail="TFW íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_base = DEFAULT_OUTPUT_DIR / f"complete_training_{timestamp}"
        output_base.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        tiles_images_dir = output_base / "tiles" / "images"
        tiles_labels_dir = output_base / "tiles" / "labels" 
        tiles_images_dir.mkdir(parents=True, exist_ok=True)
        tiles_labels_dir.mkdir(parents=True, exist_ok=True)
        
        # ìµœì¢… ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        dataset_dir = output_base / "training_dataset"
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            csv_path = os.path.join(temp_dir, csv_file.filename)
            tfw_path = os.path.join(temp_dir, tfw_file.filename)
            
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            with open(csv_path, "wb") as f:
                shutil.copyfileobj(csv_file.file, f)
            with open(tfw_path, "wb") as f:
                shutil.copyfileobj(tfw_file.file, f)
            
            # TFW íŒŒë¼ë¯¸í„° íŒŒì‹±
            tfw_params = []
            with open(tfw_path, 'r') as f:
                tfw_params = [float(line.strip()) for line in f.readlines()]
            
            if len(tfw_params) != 6:
                raise HTTPException(status_code=400, detail="TFW íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(csv_path)
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # ì¢Œí‘œ ì»¬ëŸ¼ í™•ì¸
            x_col, y_col = None, None
            if 'x' in df.columns and 'y' in df.columns:
                x_col, y_col = 'x', 'y'
            elif 'longitude' in df.columns and 'latitude' in df.columns:
                x_col, y_col = 'longitude', 'latitude'
            else:
                raise HTTPException(
                    status_code=400, 
                    detail="CSV íŒŒì¼ì— 'x,y' ë˜ëŠ” 'longitude,latitude' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            
            # ì¢Œí‘œ ì»¬ëŸ¼ ì´ë¦„ í†µì¼
            df = df.rename(columns={x_col: 'x', y_col: 'y'})
            
            print(f"ğŸ“Š GPS ì¢Œí‘œ ë°ì´í„°: {len(df)}ê°œ í¬ì¸íŠ¸", flush=True)
            
            # Step 1: íƒ€ì¼ë§ ë° ë¼ë²¨ ìƒì„±
            print("ğŸ”„ Step 1: íƒ€ì¼ë§ ë° ë¼ë²¨ ìƒì„± ì‹œì‘...", flush=True)
            tile_info = process_tiles_and_labels(
                image_path=image_path,
                tfw_params=tfw_params,
                df=df,
                output_images=str(tiles_images_dir),
                output_labels=str(tiles_labels_dir),
                tile_size=tile_size,
                bbox_size=bbox_size,
                class_id=class_id,
                file_prefix=file_prefix
            )
            
            tiles_with_labels = sum(1 for t in tile_info if t.labels_count > 0)
            total_labels = sum(t.labels_count for t in tile_info)
            
            preprocessing_info = {
                "total_tiles": len(tile_info),
                "tiles_with_labels": tiles_with_labels,
                "total_labels": total_labels,
                "tile_size": tile_size,
                "bbox_size": bbox_size,
                "file_prefix": file_prefix
            }
            
            print(f"âœ… Step 1 ì™„ë£Œ: {len(tile_info)}ê°œ íƒ€ì¼, {tiles_with_labels}ê°œ ë¼ë²¨ íƒ€ì¼, {total_labels}ê°œ ì´ ë¼ë²¨", flush=True)
            
            # Step 2: ë”¥ëŸ¬ë‹ìš© ë°ì´í„°ì…‹ ìƒì„±
            print("ğŸ”„ Step 2: ë”¥ëŸ¬ë‹ìš© ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...", flush=True)
            
            # íƒ€ì¼ ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ë§¤ì¹­
            image_files = list(tiles_images_dir.glob("*.tif"))
            label_files = list(tiles_labels_dir.glob("*.txt"))
            
            # ë§¤ì¹­ë˜ëŠ” íŒŒì¼ë“¤ë§Œ ì„ íƒ
            matched_files = []
            for img_file in image_files:
                label_file = tiles_labels_dir / f"{img_file.stem}.txt"
                if label_file.exists():
                    matched_files.append((img_file, label_file))
            
            print(f"ğŸ“Š ë§¤ì¹­ëœ íŒŒì¼ ìŒ: {len(matched_files)}ê°œ", flush=True)
            
            if len(matched_files) == 0:
                raise HTTPException(
                    status_code=400, 
                    detail="ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€-ë¼ë²¨ íŒŒì¼ ìŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            # ë°ì´í„° ì…”í”Œ
            if shuffle_data:
                import random
                random.shuffle(matched_files)
            
            # íŒŒì¼ ìˆ˜ ì œí•œ
            if max_files > 0:
                matched_files = matched_files[:max_files]
            
            # train/val ë¶„í• 
            split_idx = int(len(matched_files) * train_split) if auto_split else len(matched_files)
            train_files = matched_files[:split_idx]
            val_files = matched_files[split_idx:] if auto_split else []
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            for split_name in ['train', 'val'] if auto_split else ['train']:
                (dataset_dir / split_name / 'images').mkdir(parents=True, exist_ok=True)
                (dataset_dir / split_name / 'labels').mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ë³µì‚¬
            for img_file, label_file in train_files:
                shutil.copy2(img_file, dataset_dir / 'train' / 'images')
                shutil.copy2(label_file, dataset_dir / 'train' / 'labels')
            
            if auto_split and val_files:
                for img_file, label_file in val_files:
                    shutil.copy2(img_file, dataset_dir / 'val' / 'images')
                    shutil.copy2(label_file, dataset_dir / 'val' / 'labels')
            
            # data.yaml ìƒì„±
            class_list = [name.strip() for name in class_names.split(',')]
            data_yaml = {
                'path': '.',
                'train': 'train/images',
                'val': 'val/images' if auto_split else 'train/images',
                'nc': len(class_list),
                'names': class_list,
                
                # ë©”íƒ€ë°ì´í„°
                'created_date': datetime.datetime.now().isoformat(),
                'source_info': {
                    'image_file': image_file.filename,
                    'csv_file': csv_file.filename,
                    'total_coordinates': len(df),
                    'tile_size': tile_size,
                    'bbox_size': bbox_size
                },
                'dataset_stats': {
                    'total_files': len(matched_files),
                    'train_files': len(train_files),
                    'val_files': len(val_files),
                    'total_labels': total_labels
                }
            }
            
            import yaml
            with open(dataset_dir / 'data.yaml', 'w') as f:
                yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)
            
            # Step 3: ZIP íŒŒì¼ ìƒì„±
            print("ğŸ”„ Step 3: ZIP íŒŒì¼ ìƒì„± ì‹œì‘...", flush=True)
            zip_filename = f"complete_training_dataset_{file_prefix}_{timestamp}.zip"
            zip_path = DEFAULT_OUTPUT_DIR / zip_filename
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ì „ì²´ë¥¼ ì••ì¶•
                for root, dirs, files in os.walk(dataset_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        archive_name = os.path.relpath(file_path, dataset_dir)
                        zipf.write(file_path, archive_name)
            
            # ìµœì¢… ì •ë³´ êµ¬ì„±
            dataset_info = {
                "zip_filename": zip_filename,
                "total_files": len(matched_files),
                "train_files": len(train_files),
                "val_files": len(val_files),
                "total_labels": total_labels,
                "classes": class_list,
                "data_yaml_created": True,
                "file_size_mb": round(zip_path.stat().st_size / (1024 * 1024), 2)
            }
            
            download_url = f"/api/v1/preprocessing/download/{zip_filename}"
            
            print(f"âœ… í†µí•© ì²˜ë¦¬ ì™„ë£Œ!", flush=True)
            print(f"ğŸ“¦ ZIP íŒŒì¼: {zip_filename} ({dataset_info['file_size_mb']}MB)", flush=True)
            
            return IntegratedTrainingResponse(
                success=True,
                message=f"í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ! ì´ {len(matched_files)}ê°œ íŒŒì¼, {total_labels}ê°œ ë¼ë²¨",
                preprocessing_info=preprocessing_info,
                training_dataset_info=dataset_info,
                download_url=download_url,
                zip_filename=zip_filename
            )
            
    except Exception as e:
        print(f"âŒ í†µí•© ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", flush=True)
        raise HTTPException(status_code=500, detail=f"í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.get("/download/{filename}")
async def download_processed_file(filename: str):
    """ì²˜ë¦¬ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    file_path = DEFAULT_OUTPUT_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/octet-stream'
    )
