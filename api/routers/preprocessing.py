# ì „ì²˜ë¦¬ API ë¼ìš°í„° - tile_and_label.py ê¸°ë°˜  
# ëŒ€ìš©ëŸ‰ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  + YOLO ë¼ë²¨ ìë™ ìƒì„±

from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import os
import tempfile
import shutil
import pandas as pd
import numpy as np
import rasterio
from rasterio.transform import Affine
from rasterio.windows import Window
import datetime
import zipfile
import yaml
import random
import sys
import logging
from pathlib import Path
from api import config

router = APIRouter()# Pydantic ëª¨ë¸ ì •ì˜

# ì „ì²˜ë¦¬ API ë¼ìš°í„° - tile_and_label.py ê¸°ë°˜
# ëŒ€ìš©ëŸ‰ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  + YOLO ë¼ë²¨ ìë™ ìƒì„±

from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import os
import tempfile
import shutil
import pandas as pd
import numpy as np
import rasterio
from rasterio.transform import Affine
from rasterio.windows import Window
import datetime
import zipfile
import yaml
import random
import sys
import logging
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from .. import config

router = APIRouter()

# ì‘ë‹µ ëª¨ë¸ ì •ì˜
class TileInfo(BaseModel):
    tile_name: str
    labels_count: int
    tile_size: tuple
    position: tuple  # (tx, ty)

class InferenceTileInfo(BaseModel):
    tile_name: str
    tile_size: tuple
    position: tuple  # (tx, ty)
    has_georeference: bool

class IntegratedTrainingResponse(BaseModel):
    success: bool
    message: str
    preprocessing_info: Dict
    training_dataset_info: Dict
    download_url: str
    zip_filename: str
    validation_samples_url: Optional[str] = None  # ë¼ë²¨ë§ ê²€ì¦ ìƒ˜í”Œ ZIP URL
    validation_info: Optional[Dict] = None  # ê²€ì¦ ì •ë³´
    coordinate_validation: Optional[Dict] = None  # ğŸ” GPS ì¢Œí‘œ ë§¤í•‘ ê²€ì¦ ì •ë³´

class InferenceTilingResponse(BaseModel):
    success: bool
    message: str
    total_tiles: int
    original_size: tuple
    tile_size: int
    tile_info: List[InferenceTileInfo]
    download_url: Optional[str] = None

class MergedDatasetResponse(BaseModel):
    success: bool
    message: str
    merged_dataset_info: Dict
    download_url: str
    zip_filename: str
    validation_samples_url: Optional[str] = None
    source_datasets: List[Dict]  # ì›ë³¸ ZIPë“¤ì˜ ì •ë³´

# ê¸°ë³¸ ì„¤ì •
DEFAULT_TILE_SIZE = config.DEFAULT_TILE_SIZE
DEFAULT_CLASS_ID = config.DEFAULT_CLASS_ID
DEFAULT_OUTPUT_DIR = Path(config.API_TILES_DIR)  # Path ê°ì²´ë¡œ ë³€ê²½

# ğŸ“¦ ë©€í‹°ìŠ¤ì¼€ì¼ ë°”ìš´ë”©ë°•ìŠ¤ - ì„¤ì •ê°’ì€ config.pyì—ì„œ ê´€ë¦¬

# ğŸ“Š ë¼ë²¨ë§ ê²€ì¦ì„ ìœ„í•œ ì‹œê°í™” í•¨ìˆ˜ë“¤
def create_labeling_validation_samples(tiles_images_dir: Path, tiles_labels_dir: Path, 
                                      output_dir: Path, num_samples: int = 30) -> List[str]:
    """
    ğŸ” ë¼ë²¨ë§ í’ˆì§ˆ ê²€ì¦ì„ ìœ„í•œ ì‹œê°í™” ìƒ˜í”Œ ìƒì„±
    
    Args:
        tiles_images_dir: íƒ€ì¼ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        tiles_labels_dir: íƒ€ì¼ ë¼ë²¨ ë””ë ‰í† ë¦¬  
        output_dir: ê²€ì¦ ìƒ˜í”Œ ì¶œë ¥ ë””ë ‰í† ë¦¬
        num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
    
    Returns:
        List[str]: ìƒì„±ëœ ê²€ì¦ ìƒ˜í”Œ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
    """
    import cv2
    import random
    
    # ê²€ì¦ ìƒ˜í”Œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    validation_dir = output_dir / "validation_samples"
    validation_dir.mkdir(parents=True, exist_ok=True)
    
    # ë¼ë²¨ì´ ìˆëŠ” íƒ€ì¼ë“¤ ì°¾ê¸°
    image_files = list(tiles_images_dir.glob("*.tif"))
    labeled_tiles = []
    unlabeled_tiles = []
    
    for img_file in image_files:
        label_file = tiles_labels_dir / f"{img_file.stem}.txt"
        if label_file.exists():
            # ë¼ë²¨ íŒŒì¼ ë‚´ìš© í™•ì¸
            with open(label_file, 'r') as f:
                content = f.read().strip()
            if content:  # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°
                labeled_tiles.append((img_file, label_file))
            else:  # ë¹ˆ ë¼ë²¨ íŒŒì¼ (negative sample)
                unlabeled_tiles.append((img_file, label_file))
    
    print(f"ğŸ” ë¼ë²¨ë§ ê²€ì¦ ìƒ˜í”Œ ìƒì„±: {len(labeled_tiles)}ê°œ ì–‘ì„±, {len(unlabeled_tiles)}ê°œ ìŒì„± íƒ€ì¼ ë°œê²¬")
    
    sample_files = []
    
    # ì–‘ì„± ìƒ˜í”Œ (ë¼ë²¨ì´ ìˆëŠ” íƒ€ì¼) ì‹œê°í™”
    # ìŒì„± ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ëª¨ë“  ìƒ˜í”Œì„ ì–‘ì„±ìœ¼ë¡œ, ìˆìœ¼ë©´ ë°˜ë°˜ìœ¼ë¡œ ë°°ë¶„
    if len(unlabeled_tiles) == 0:
        positive_samples = min(num_samples, len(labeled_tiles))
    else:
        positive_samples = min(num_samples // 2, len(labeled_tiles))
    
    if positive_samples > 0:
        selected_positive = random.sample(labeled_tiles, positive_samples)
        
        for i, (img_file, label_file) in enumerate(selected_positive):
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ
                img = cv2.imread(str(img_file))
                if img is None:
                    continue
                    
                h, w = img.shape[:2]
                
                # ë¼ë²¨ íŒŒì¼ ì½ê¸°
                with open(label_file, 'r') as f:
                    lines = f.readlines()
                
                # ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                for line in lines:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id, x_center, y_center, width, height = map(float, parts[:5])
                        
                        # YOLO ì •ê·œí™” ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                        x_center_px = int(x_center * w)
                        y_center_px = int(y_center * h)
                        width_px = int(width * w)
                        height_px = int(height * h)
                        
                        # ë°”ìš´ë”©ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚°
                        x1 = max(0, x_center_px - width_px // 2)
                        y1 = max(0, y_center_px - height_px // 2)
                        x2 = min(w, x_center_px + width_px // 2)
                        y2 = min(h, y_center_px + height_px // 2)
                        
                        # ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë…¹ìƒ‰)
                        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        
                        # ì¤‘ì‹¬ì  í‘œì‹œ (ë¹¨ê°„ ì›)
                        cv2.circle(img, (x_center_px, y_center_px), 3, (0, 0, 255), -1)
                        
                        # í¬ê¸° ì •ë³´ í‘œì‹œ
                        cv2.putText(img, f"{width_px}x{height_px}", 
                                  (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # íƒ€ì¼ ì •ë³´ í‘œì‹œ
                cv2.putText(img, f"POSITIVE: {img_file.name}", 
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(img, f"Labels: {len(lines)}", 
                          (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
                
                # ì €ì¥
                sample_filename = f"validation_positive_{i+1:02d}_{img_file.stem}.jpg"
                sample_path = validation_dir / sample_filename
                cv2.imwrite(str(sample_path), img)
                sample_files.append(sample_filename)
                
            except Exception as e:
                print(f"âš ï¸ ì–‘ì„± ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨ {img_file.name}: {e}")
    
    # ìŒì„± ìƒ˜í”Œ (ë¼ë²¨ì´ ì—†ëŠ” íƒ€ì¼) ì‹œê°í™”  
    negative_samples = min(num_samples - len(sample_files), len(unlabeled_tiles))
    if negative_samples > 0 and unlabeled_tiles:
        selected_negative = random.sample(unlabeled_tiles, negative_samples)
        
        for i, (img_file, label_file) in enumerate(selected_negative):
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ
                img = cv2.imread(str(img_file))
                if img is None:
                    continue
                
                # íƒ€ì¼ ì •ë³´ í‘œì‹œ
                cv2.putText(img, f"NEGATIVE: {img_file.name}", 
                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                cv2.putText(img, "No Labels (Background)", 
                          (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
                
                # ì €ì¥
                sample_filename = f"validation_negative_{i+1:02d}_{img_file.stem}.jpg"
                sample_path = validation_dir / sample_filename
                cv2.imwrite(str(sample_path), img)
                sample_files.append(sample_filename)
                
            except Exception as e:
                print(f"âš ï¸ ìŒì„± ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨ {img_file.name}: {e}")
    
    print(f"âœ… ë¼ë²¨ë§ ê²€ì¦ ìƒ˜í”Œ {len(sample_files)}ê°œ ìƒì„± ì™„ë£Œ")
    return sample_files

def create_validation_samples_zip(output_base: Path, timestamp: str, sample_files: List[str]) -> str:
    """ê²€ì¦ ìƒ˜í”Œë“¤ì„ ZIP íŒŒì¼ë¡œ ì••ì¶•"""
    zip_filename = f"labeling_validation_samples_{timestamp}.zip"
    zip_path = DEFAULT_OUTPUT_DIR / zip_filename
    
    validation_dir = output_base / "validation_samples"
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # ê²€ì¦ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ ì••ì¶•
        for sample_file in sample_files:
            sample_path = validation_dir / sample_file
            if sample_path.exists():
                zipf.write(sample_path, f"validation_samples/{sample_file}")
        
        # README íŒŒì¼ ìƒì„± ë° ì¶”ê°€
        readme_content = f"""# ë¼ë²¨ë§ ê²€ì¦ ìƒ˜í”Œ

ìƒì„± ì‹œê°„: {datetime.datetime.now().isoformat()}
ì´ ìƒ˜í”Œ ìˆ˜: {len(sample_files)}ê°œ

## íŒŒì¼ ì„¤ëª…:
- validation_positive_XX_*.jpg: í”¼í•´ëª© ë¼ë²¨ì´ ìˆëŠ” íƒ€ì¼ (ë…¹ìƒ‰ ë°”ìš´ë”©ë°•ìŠ¤)
- validation_negative_XX_*.jpg: í”¼í•´ëª©ì´ ì—†ëŠ” ë°°ê²½ íƒ€ì¼

## ê²€ì¦ ë°©ë²•:
1. ì–‘ì„± ìƒ˜í”Œ: ë…¹ìƒ‰ ë°”ìš´ë”©ë°•ìŠ¤ê°€ ì‹¤ì œ í”¼í•´ëª© ìœ„ì¹˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
2. ìŒì„± ìƒ˜í”Œ: ì‹¤ì œë¡œ í”¼í•´ëª©ì´ ì—†ëŠ” ê¹¨ë—í•œ ë°°ê²½ì¸ì§€ í™•ì¸
3. ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸°ê°€ ì ì ˆí•œì§€ í™•ì¸ (ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì§€ ì•Šì€ì§€)

## ë¬¸ì œ ë°œê²¬ì‹œ:
- GPS ì¢Œí‘œ ì •í™•ë„ í™•ì¸ í•„ìš”
- ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ì¡°ì • í•„ìš”  
- ë°ì´í„°ì…‹ ì¬ìƒì„± ê¶Œì¥
"""
        
        readme_path = validation_dir / "README.txt"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        zipf.write(readme_path, "validation_samples/README.txt")
    
    return str(zip_path)

# í—¬í¼ í•¨ìˆ˜ë“¤
def load_tfw(tfw_path):
    """TFW íŒŒì¼ì—ì„œ ë³€í™˜ íŒŒë¼ë¯¸í„° ì½ê¸°"""
    with open(tfw_path) as f:
        vals = [float(x.strip()) for x in f.readlines()]
    return vals

def tm_to_pixel(x, y, tfw):
    """TM ì¢Œí‘œ(x, y)ë¥¼ ì´ë¯¸ì§€ í”½ì…€ ì¢Œí‘œ(px, py)ë¡œ ë³€í™˜"""
    A, D, B, E, C, F = tfw
    px = (x - C) / A
    py = (y - F) / E
    return px, py

# ğŸ¯ ë©€í‹°ìŠ¤ì¼€ì¼ ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ê³„ì‚° í•¨ìˆ˜
def calculate_adaptive_bbox_size(target_px, target_py, all_pixel_coords, min_size=16, max_size=128):
    """
    GPS ì¢Œí‘œ ë°€ë„ì™€ ê±°ë¦¬ ê¸°ë°˜ ì ì‘ì  ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ê³„ì‚°
    
    Args:
        target_px, target_py: ëŒ€ìƒ í”½ì…€ ì¢Œí‘œ
        all_pixel_coords: ëª¨ë“  í”½ì…€ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ [(px, py, row, idx), ...]
        min_size, max_size: ìµœì†Œ/ìµœëŒ€ ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸°
    
    Returns:
        int: ê³„ì‚°ëœ ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸°
    """
    import math
    
    # ì£¼ë³€ ë°˜ê²½ ë‚´ ì¢Œí‘œ ê°œìˆ˜ë¡œ ë°€ë„ ê³„ì‚°
    search_radius = 50  # 50í”½ì…€ ë°˜ê²½
    nearby_points = 0
    
    for px, py, row, idx in all_pixel_coords:
        distance = math.sqrt((target_px - px)**2 + (target_py - py)**2)
        if distance <= search_radius:
            nearby_points += 1
    
    # ë°€ë„ê°€ ë†’ì„ìˆ˜ë¡ ì‘ì€ ë°”ìš´ë”©ë°•ìŠ¤, ë‚®ì„ìˆ˜ë¡ í° ë°”ìš´ë”©ë°•ìŠ¤
    if nearby_points <= 1:  # ë§¤ìš° ë‚®ì€ ë°€ë„ (ì™¸ë”´ í”¼í•´ëª©)
        bbox_size = max_size  # 128px
    elif nearby_points <= 3:  # ë‚®ì€ ë°€ë„
        bbox_size = int(max_size * 0.75)  # 96px
    elif nearby_points <= 6:  # ì¤‘ê°„ ë°€ë„
        bbox_size = int(max_size * 0.5)  # 64px
    elif nearby_points <= 10:  # ë†’ì€ ë°€ë„
        bbox_size = int(max_size * 0.375)  # 48px
    else:  # ë§¤ìš° ë†’ì€ ë°€ë„ (ë°€ì§‘ëœ í”¼í•´ëª©)
        bbox_size = min_size  # 16px
    
    return bbox_size

def process_tiles_and_labels(image_path, tfw_params, df, output_images, output_labels, 
                           tile_size, class_id, file_prefix):
    """ğŸ“¦ ë©€í‹°ìŠ¤ì¼€ì¼ ë°”ìš´ë”©ë°•ìŠ¤ë¥¼ ì‚¬ìš©í•œ íƒ€ì¼ë§ ë° ë¼ë²¨ ìƒì„± (ì ì‘ì  í¬ê¸°)"""
    tile_info, coordinate_tracking = process_tiles_and_labels_simple(
        image_path, tfw_params, df, output_images, output_labels, 
        tile_size, class_id, file_prefix, use_adaptive_bbox=True
    )
    return tile_info, coordinate_tracking

def process_tiles_and_labels_simple(image_path, tfw_params, df, output_images, output_labels, 
                                   tile_size, class_id, file_prefix, use_adaptive_bbox=True):
    """
    ğŸ“¦ ì´ë¯¸ì§€ íƒ€ì¼ ë¶„í•  ë° ë©€í‹°ìŠ¤ì¼€ì¼ YOLO ë¼ë²¨ ìƒì„± (GPS ì¢Œí‘œ ì¶”ì  í¬í•¨)
    
    Args:
        use_adaptive_bbox: ì ì‘ì  ë°”ìš´ë”©ë°•ìŠ¤ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)
        
    Returns:
        tuple: (tile_info, coordinate_tracking_info)
    """
    tile_info = []
    
    # ğŸ§¹ ì´ì „ ì§€ë„ì˜ ìºì‹œëœ í”½ì…€ ì¢Œí‘œ ì´ˆê¸°í™” (ê° ì§€ë„ë§ˆë‹¤ ë…ë¦½ì  ì²˜ë¦¬)
    if hasattr(process_tiles_and_labels_simple, '_pixel_coords'):
        delattr(process_tiles_and_labels_simple, '_pixel_coords')
        print("ğŸ”„ ì´ì „ ì§€ë„ì˜ í”½ì…€ ì¢Œí‘œ ìºì‹œ ì´ˆê¸°í™”", flush=True)
    
    # ğŸ” GPS ì¢Œí‘œ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    total_coordinates = len(df)
    processed_coordinates = set()  # ì²˜ë¦¬ëœ ì¢Œí‘œ ì¸ë±ìŠ¤ë“¤
    out_of_bounds_coordinates = []  # ì´ë¯¸ì§€ ë²”ìœ„ ë°– ì¢Œí‘œë“¤
    labels_created = 0  # ìƒì„±ëœ ì´ ë¼ë²¨ ìˆ˜
    
    with rasterio.open(image_path) as src:
        width, height = src.width, src.height
        n_tiles_x = int(np.ceil(width / tile_size))
        n_tiles_y = int(np.ceil(height / tile_size))
        
        total_tiles = n_tiles_x * n_tiles_y
        processed_tiles = 0
        
        print(f"ğŸ¯ íƒ€ì¼ ë¶„í•  ì‹œì‘: {width}x{height} â†’ {n_tiles_x}x{n_tiles_y} = {total_tiles}ê°œ íƒ€ì¼", flush=True)
        print(f"ğŸ“Š GPS í¬ì¸íŠ¸ {len(df)}ê°œë¥¼ {total_tiles}ê°œ íƒ€ì¼ì— ë¶„ë°° ì¤‘...", flush=True)
        
        for ty in range(n_tiles_y):
            for tx in range(n_tiles_x):
                x0 = tx * tile_size
                y0 = ty * tile_size
                w = min(tile_size, width - x0)
                h = min(tile_size, height - y0)
                window = Window(x0, y0, w, h)
                
                # RGB 3ì±„ë„ë§Œ ì¶”ì¶œí•˜ì—¬ íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„±
                tile_img = src.read([1, 2, 3], window=window)
                tile_name = f"{file_prefix}_{tx}_{ty}.tif"
                
                # ğŸ¯ Multi-Scale íƒ€ì¼ ë‚´ bbox ë¼ë²¨ ìƒì„±
                lines = []
                
                # ğŸš€ ì„±ëŠ¥ ìµœì í™”: íƒ€ì¼ ì˜ì—­ì— í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì¢Œí‘œë§Œ í•„í„°ë§
                # ì „ì²´ ì¢Œí‘œë¥¼ í”½ì…€ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ ê³„ì‚°) + ì¶”ì  ì •ë³´ í¬í•¨
                if not hasattr(process_tiles_and_labels_simple, '_pixel_coords'):
                    print("ğŸ”„ GPS ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜ ì¤‘... (ìµœì´ˆ 1íšŒ)", flush=True)
                    pixel_coords = []
                    for idx, row in df.iterrows():
                        px, py = tm_to_pixel(row["x"], row["y"], tfw_params)
                        
                        # ì´ë¯¸ì§€ ë²”ìœ„ ì²´í¬
                        if px < 0 or py < 0 or px >= width or py >= height:
                            out_of_bounds_coordinates.append({
                                'index': idx,
                                'original_coords': (row["x"], row["y"]),
                                'pixel_coords': (px, py),
                                'reason': 'out_of_image_bounds'
                            })
                        else:
                            pixel_coords.append((px, py, row, idx))  # ì¸ë±ìŠ¤ ì¶”ê°€
                    
                    process_tiles_and_labels_simple._pixel_coords = pixel_coords
                    print(f"âœ… {len(pixel_coords)}ê°œ ì¢Œí‘œ ë³€í™˜ ì™„ë£Œ (ë²”ìœ„ ì™¸: {len(out_of_bounds_coordinates)}ê°œ)", flush=True)
                
                # í˜„ì¬ íƒ€ì¼ ì˜ì—­ì— í¬í•¨ë˜ëŠ” ì¢Œí‘œë§Œ ì²˜ë¦¬ (ì¶”ì  í¬í•¨)
                for px, py, row, coord_idx in process_tiles_and_labels_simple._pixel_coords:
                    # íƒ€ì¼ ë‚´ ìƒëŒ€ì¢Œí‘œë¡œ ë³€í™˜
                    rel_x = px - x0
                    rel_y = py - y0
                    if 0 <= rel_x < w and 0 <= rel_y < h:
                        x_center = rel_x / w
                        y_center = rel_y / h
                        
                        # ğŸ¯ ë©€í‹°ìŠ¤ì¼€ì¼ ì ì‘ì  ë°”ìš´ë”©ë°•ìŠ¤ ìƒì„±
                        # GPS ì¢Œí‘œ ë°€ë„ ê¸°ë°˜ ì ì‘ì  ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ê³„ì‚°
                        adaptive_size = calculate_adaptive_bbox_size(px, py, process_tiles_and_labels_simple._pixel_coords, 
                                                                  min_size=config.ADAPTIVE_BBOX_MIN_SIZE, 
                                                                  max_size=config.ADAPTIVE_BBOX_MAX_SIZE)
                        bw = adaptive_size / w
                        bh = adaptive_size / h
                        lines.append(
                            f"{class_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}"
                        )
                        
                        # ğŸ” ì²˜ë¦¬ëœ ì¢Œí‘œ ì¶”ì 
                        processed_coordinates.add(coord_idx)
                        labels_created += 1
                
                # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
                labels_count = len(lines)
                saved = labels_count > 0
                
                # ì§„í–‰ìƒí™© ì¶œë ¥ (10% ë‹¨ìœ„)
                processed_tiles += 1
                if processed_tiles % max(1, total_tiles // 10) == 0 or processed_tiles == total_tiles:
                    progress_pct = (processed_tiles / total_tiles) * 100
                    tiles_with_labels = sum(1 for t in tile_info if hasattr(t, 'labels_count') and t.labels_count > 0) + (1 if saved else 0)
                    print(f"  ğŸ“Š ì§„í–‰ìƒí™©: {processed_tiles}/{total_tiles} ({progress_pct:.1f}%) - ë¼ë²¨ ìˆëŠ” íƒ€ì¼: {tiles_with_labels}ê°œ", flush=True)
                
                if saved:
                    # ë¼ë²¨ íŒŒì¼ ì €ì¥
                    out_lbl_path = os.path.join(output_labels, tile_name.replace(".tif", ".txt"))
                    with open(out_lbl_path, "w") as f:
                        f.write("\n".join(lines))
                    
                    # íƒ€ì¼ ì´ë¯¸ì§€ ì €ì¥
                    out_img_path = os.path.join(output_images, tile_name)
                    orig_affine = src.transform
                    tile_affine = orig_affine * Affine.translation(x0, y0)
                    with rasterio.open(
                        out_img_path,
                        "w",
                        driver="GTiff",
                        height=h,
                        width=w,
                        count=3,
                        dtype=src.dtypes[0],
                        crs=src.crs,
                        transform=tile_affine,
                    ) as dst:
                        dst.write(tile_img)
                
                # íƒ€ì¼ ì •ë³´ ì €ì¥ (TileInfo ëª¨ë¸ì— ë§ê²Œ ì¡°ì •)
                tile_info.append(TileInfo(
                    tile_name=tile_name,
                    labels_count=labels_count,
                    tile_size=(w, h),
                    position=(tx, ty)
                ))
    
    # ğŸ” GPS ì¢Œí‘œ ì¶”ì  í†µê³„ ê³„ì‚°
    missing_coordinates = len(out_of_bounds_coordinates) if out_of_bounds_coordinates else 0
    total_processed = len(processed_coordinates) if processed_coordinates else 0
    
    # ìµœì¢… í†µê³„ ì¶œë ¥ (GPS ì¶”ì  í¬í•¨)
    tiles_with_labels = sum(1 for t in tile_info if t.labels_count > 0)
    total_labels = sum(t.labels_count for t in tile_info)
    csv_total = len(process_tiles_and_labels_simple._pixel_coords) if hasattr(process_tiles_and_labels_simple, '_pixel_coords') else 0
    success_rate = (total_processed/csv_total * 100) if csv_total > 0 else 0
    
    print(f"âœ… íƒ€ì¼ ë¶„í•  ì™„ë£Œ: {total_tiles}ê°œ íƒ€ì¼ ìƒì„±, {tiles_with_labels}ê°œ ë¼ë²¨ íƒ€ì¼, {total_labels}ê°œ ì´ ë¼ë²¨", flush=True)
    print(f"ğŸ” GPS ì¢Œí‘œ ë§¤í•‘: CSV {csv_total}ê°œ â†’ ë¼ë²¨ {total_processed}ê°œ ìƒì„± ({success_rate:.1f}%), ë²”ìœ„ì™¸ {missing_coordinates}ê°œ", flush=True)
    
    # ì¢Œí‘œ ì¶”ì  ì •ë³´ í¬í•¨í•˜ì—¬ ë°˜í™˜
    coordinate_tracking = {
        "csv_total": csv_total,
        "labels_created": total_processed,
        "out_of_bounds": missing_coordinates,
        "success_rate": f"{success_rate:.2f}%"
    }
    
    return tile_info, coordinate_tracking

@router.get("/download/{filename}")
async def download_tiles_zip(filename: str):
    """
    ìƒì„±ëœ íƒ€ì¼ê³¼ ë¼ë²¨ ZIP íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    file_path = DEFAULT_OUTPUT_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type='application/zip'
    )


@router.get("/status")
async def preprocessing_status():
    """
    ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ì˜ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ìµœê·¼ ìƒì„±ëœ íƒ€ì¼ í´ë”ë“¤ ì¡°íšŒ
    recent_tiles = []
    if DEFAULT_OUTPUT_DIR.exists():
        for item in sorted(os.listdir(DEFAULT_OUTPUT_DIR), reverse=True)[:5]:
            item_path = DEFAULT_OUTPUT_DIR / item
            if item_path.is_dir():
                images_dir = item_path / "images"
                labels_dir = item_path / "labels"
                images_count = len([f for f in os.listdir(images_dir) 
                                 if f.endswith('.tif')]) if images_dir.exists() else 0
                labels_count = len([f for f in os.listdir(labels_dir)
                                 if f.endswith('.txt')]) if labels_dir.exists() else 0
                
                recent_tiles.append({
                    "folder_name": item,
                    "images_count": images_count,
                    "labels_count": labels_count
                })
    
    return {
        "service": "preprocessing",
        "status": "active",
        "recent_tiles": recent_tiles,
        "default_settings": {
            "tile_size": DEFAULT_TILE_SIZE,
            "bbox_mode": f"Multiscale Adaptive {config.ADAPTIVE_BBOX_MIN_SIZE}~{config.ADAPTIVE_BBOX_MAX_SIZE}px",
            "class_id": DEFAULT_CLASS_ID
        }
    }


@router.post("/tile-for-inference", response_model=InferenceTilingResponse)
async def create_inference_tiles(
    image_file: UploadFile = File(..., description="ì›ë³¸ GeoTIFF ì´ë¯¸ì§€"),
    tile_size: int = Form(default=DEFAULT_TILE_SIZE, description="íƒ€ì¼ í¬ê¸° (í”½ì…€)")
):
    """
    ì¶”ë¡ ìš© íƒ€ì¼ ìƒì„±: ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•´ íƒ€ì¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ì›ë³¸ ì´ë¯¸ì§€ë§Œ ë¶„í• í•˜ë©°, ì§€ë¦¬ì°¸ì¡° ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
    
    - **image_file**: ì›ë³¸ GeoTIFF ì´ë¯¸ì§€ íŒŒì¼ (.tif/.tiff)
    - **tile_size**: íƒ€ì¼ í•œ ë³€ í¬ê¸° (ê¸°ë³¸: 1024í”½ì…€)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(('.tif', '.tiff')):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ì€ TIFF í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            
            # ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            
            # ë‚ ì§œ ê¸°ë°˜ ì ‘ë‘ì‚¬ ìƒì„±
            today = datetime.datetime.now().strftime("%Y%m%d")
            
            # ê°™ì€ ë‚ ì§œì˜ ê¸°ì¡´ í´ë”ë“¤ í™•ì¸í•˜ì—¬ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì •
            existing_prefixes = []
            if os.path.exists(DEFAULT_OUTPUT_DIR):
                for folder in os.listdir(DEFAULT_OUTPUT_DIR):
                    if folder.startswith(f"inference_tiles_") and today in folder:
                        # inference_tiles_A20250915_ í˜•íƒœì—ì„œ ì ‘ë‘ì‚¬ ì¶”ì¶œ
                        parts = folder.split('_')
                        if len(parts) >= 3 and parts[2].startswith(('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')):
                            prefix_char = parts[2][0]
                            existing_prefixes.append(prefix_char)
            
            # ë‹¤ìŒ ìˆœì°¨ ì ‘ë‘ì‚¬ ê²°ì • (A, B, C, ... Z ìˆœì„œ)
            next_prefix = 'A'
            for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
                if char not in existing_prefixes:
                    next_prefix = char
                    break
            
            # íŒŒì¼ ì ‘ë‘ì‚¬ ìƒì„±
            file_prefix = f"{next_prefix}{today}"
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_base = DEFAULT_OUTPUT_DIR / f"inference_tiles_{file_prefix}_{timestamp}"
            output_images = output_base / "images"
            
            output_images.mkdir(parents=True, exist_ok=True)
            
            # ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ì‹¤í–‰
            tile_info, original_size = process_inference_tiles(
                image_path, str(output_images), tile_size, file_prefix
            )
            
            # ZIP íŒŒì¼ ìƒì„±
            zip_path = create_inference_tiles_zip(output_base, timestamp)
            
            # í†µê³„ ê³„ì‚°
            total_tiles = len(tile_info)
            
            return InferenceTilingResponse(
                success=True,
                message=f"ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ì™„ë£Œ: {total_tiles}ê°œ íƒ€ì¼ ìƒì„±",
                total_tiles=total_tiles,
                original_size=original_size,
                tile_size=tile_size,
                tile_info=tile_info,
                download_url=f"/api/v1/preprocessing/download/{os.path.basename(zip_path)}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¶”ë¡ ìš© íƒ€ì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def process_inference_tiles(
    image_path: str, output_images: str, tile_size: int, file_prefix: str
) -> tuple[List[InferenceTileInfo], tuple]:
    """ì¶”ë¡ ìš© íƒ€ì¼ ë¶„í•  ë©”ì¸ ë¡œì§ (ë¼ë²¨ë§ ì—†ìŒ)"""
    
    tile_info = []
    
    with rasterio.open(image_path) as src:
        width, height = src.width, src.height
        original_size = (width, height)
        
        n_tiles_x = int(np.ceil(width / tile_size))
        n_tiles_y = int(np.ceil(height / tile_size))
        
        for ty in range(n_tiles_y):
            for tx in range(n_tiles_x):
                x0 = tx * tile_size
                y0 = ty * tile_size
                w = min(tile_size, width - x0)
                h = min(tile_size, height - y0)
                
                window = Window(x0, y0, w, h)
                
                # RGB 3ì±„ë„ë§Œ ì¶”ì¶œí•˜ì—¬ íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„±
                tile_img = src.read([1, 2, 3], window=window)
                tile_name = f"{file_prefix}_tile_{tx}_{ty}.tif"
                
                # íƒ€ì¼ ì´ë¯¸ì§€ ì €ì¥ (ì§€ë¦¬ì°¸ì¡° ì •ë³´ ë³´ì¡´)
                tile_path = os.path.join(output_images, tile_name)
                orig_affine = src.transform
                tile_affine = orig_affine * Affine.translation(x0, y0)
                
                has_georeference = src.crs is not None and src.transform is not None
                
                with rasterio.open(
                    tile_path, 'w',
                    driver='GTiff',
                    height=h, width=w, count=3,
                    dtype=src.dtypes[0],
                    crs=src.crs,
                    transform=tile_affine
                ) as dst:
                    dst.write(tile_img)
                
                # íƒ€ì¼ ì •ë³´ ì¶”ê°€
                tile_info.append(InferenceTileInfo(
                    tile_name=tile_name,
                    tile_size=(w, h),
                    position=(tx, ty),
                    has_georeference=has_georeference
                ))
    
    return tile_info, original_size


def create_inference_tiles_zip(output_base: Path, timestamp: str) -> str:
    """ìƒì„±ëœ ì¶”ë¡ ìš© íƒ€ì¼ì„ ZIP íŒŒì¼ë¡œ ì••ì¶•"""
    zip_filename = f"inference_tiles_{timestamp}.zip"
    zip_path = DEFAULT_OUTPUT_DIR / zip_filename
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # images í´ë” ì••ì¶•
        images_dir = output_base / "images"
        if images_dir.exists():
            for root, dirs, files in os.walk(images_dir):
                # macOS ì‹œìŠ¤í…œ í´ë” ì œì™¸
                if '__MACOSX' in root:
                    continue
                for file in files:
                    # macOS ìˆ¨ê¹€ íŒŒì¼ ë° ì‹œìŠ¤í…œ íŒŒì¼ ì œì™¸
                    if file.startswith('.') or file.startswith('._'):
                        continue
                    file_path = os.path.join(root, file)
                    archive_name = os.path.relpath(file_path, output_base)
                    zipf.write(file_path, archive_name)
    
    return str(zip_path)


@router.post("/create-dataset", response_model=IntegratedTrainingResponse)
async def create_dataset(
    image_file: UploadFile = File(..., description="ì²˜ë¦¬í•  GeoTIFF ì´ë¯¸ì§€ íŒŒì¼"),
    csv_file: UploadFile = File(..., description="GPS ì¢Œí‘œ CSV íŒŒì¼ (x, y ë˜ëŠ” longitude, latitude ì»¬ëŸ¼)"),
    tfw_file: UploadFile = File(..., description="ì§€ë¦¬ì°¸ì¡°ë¥¼ ìœ„í•œ TFW íŒŒì¼"),
    file_prefix: str = Form(..., description="ìƒì„±ë  íƒ€ì¼ íŒŒì¼ëª… ì ‘ë‘ì‚¬"),
    tile_size: int = Form(default=config.DEFAULT_TILE_SIZE, description="íƒ€ì¼ í¬ê¸° (í”½ì…€)"),

    class_id: int = Form(default=0, description="YOLO í´ë˜ìŠ¤ ID"),
    train_split: float = Form(default=0.8, description="í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (0.0-1.0)"),
    class_names: str = Form(default="damaged_tree", description="í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    max_files: int = Form(default=1000, description="ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (0=ë¬´ì œí•œ)"),
    shuffle_data: bool = Form(default=True, description="ë°ì´í„° ì…”í”Œ ì—¬ë¶€"),
    auto_split: bool = Form(default=True, description="ìë™ train/val ë¶„í•  ì—¬ë¶€")
):
    """
    ğŸš€ **ì•ˆì •ì ì¸ ë”¥ëŸ¬ë‹ ë°ì´í„°ì…‹ ìƒì„± API** (ê¶Œì¥)
    
    **ì´ APIëŠ” ë‹¤ìŒ ì‘ì—…ì„ í•œë²ˆì— ìˆ˜í–‰í•©ë‹ˆë‹¤:**
    1. ğŸ–¼ï¸ GeoTIFF ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë¶„í•  (ëª¨ë“  ì˜ì—­ í¬í•¨)
    2. ï¿½ GPS ì¢Œí‘œ ê¸°ë°˜ YOLO ë¼ë²¨ ìë™ ìƒì„± (ë©€í‹°ìŠ¤ì¼€ì¼ ì ì‘ì )
    3. ğŸ” **ë¼ë²¨ë§ í’ˆì§ˆ ê²€ì¦ ìƒ˜í”Œ ìƒì„±** (NEW!)
    4. âš–ï¸ Positive/Negative ìƒ˜í”Œ ê· í˜• ë°ì´í„°ì…‹ ìƒì„±
    5. ğŸ“¦ Google Colab ìµœì í™” ë”¥ëŸ¬ë‹ìš© ZIP íŒŒì¼ ìƒì„±
    6. ğŸ“Š Train/Validation ë°ì´í„°ì…‹ ìë™ ë¶„í• 
    
    **ğŸ¯ ë©€í‹°ìŠ¤ì¼€ì¼ ë°ì´í„°ì…‹ íŠ¹ì§•:**
    - âœ… **ì ì‘ì  ë°”ìš´ë”©ë°•ìŠ¤**: GPS ë°€ë„ ê¸°ë°˜ 16~128px ê°€ë³€ í¬ê¸°
    - âœ… **ì‹¤ì œ í¬ê¸° ë°˜ì˜**: ì™¸ë”´ í”¼í•´ëª©(í° ë°•ìŠ¤) vs ë°€ì§‘ í”¼í•´ëª©(ì‘ì€ ë°•ìŠ¤)
    - âœ… **ë¼ë²¨ë§ ê²€ì¦**: ë°”ìš´ë”©ë°•ìŠ¤ ì˜¤ë²„ë ˆì´ëœ ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ í’ˆì§ˆ í™•ì¸ ê°€ëŠ¥
    - âœ… **í–¥ìƒëœ íƒì§€ìœ¨**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ í”¼í•´ëª© ëª¨ë‘ íƒì§€ ê°€ëŠ¥
    - âœ… **Negative ìƒ˜í”Œ í¬í•¨**: ì˜¤íƒì§€ ë°©ì§€ë¥¼ ìœ„í•œ ìŒì„± ìƒ˜í”Œ ìë™ ìƒì„±
    
    **ğŸ“‹ ë§¤ê°œë³€ìˆ˜:**
    5. ğŸ“‹ README ë° ì‚¬ìš©ë²• ê°€ì´ë“œ í¬í•¨
    
    **ğŸ“‹ ë§¤ê°œë³€ìˆ˜:**
    - **image_file**: ì²˜ë¦¬í•  GeoTIFF ì´ë¯¸ì§€ íŒŒì¼ (.tif/.tiff)
    - **csv_file**: GPS ì¢Œí‘œ CSV íŒŒì¼ (x,y ë˜ëŠ” longitude,latitude ì»¬ëŸ¼ í•„ìš”)
    - **tfw_file**: ì§€ë¦¬ì°¸ì¡°ë¥¼ ìœ„í•œ TFW íŒŒì¼ (.tfw)
    - **file_prefix**: ìƒì„±ë  íƒ€ì¼ íŒŒì¼ëª… ì ‘ë‘ì‚¬ (ì˜ˆ: "A20250919")
    - **tile_size**: íƒ€ì¼ í¬ê¸° (ê¸°ë³¸: 1024px, ì„¸ë°€í•œ íƒì§€ ìµœì í™”)
    - **ë©€í‹°ìŠ¤ì¼€ì¼ ë°”ìš´ë”©ë°•ìŠ¤**: GPS ë°€ë„ ê¸°ë°˜ ìë™ í¬ê¸° ì¡°ì ˆ
      - ì™¸ë”´ í”¼í•´ëª©: 128px (í° ë°”ìš´ë”©ë°•ìŠ¤)
      - ë°€ì§‘ ì§€ì—­: 16px (ì‘ì€ ë°”ìš´ë”©ë°•ìŠ¤) 
      - ì¤‘ê°„ ë°€ë„: 48~96px (ì ì‘ì  ì¡°ì ˆ)
    - **class_id**: YOLO í´ë˜ìŠ¤ ID (ê¸°ë³¸: 0)
    - **train_split**: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.8 = 80% í•™ìŠµ, 20% ê²€ì¦)
    - **class_names**: í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸: "damaged_tree")
    - **max_files**: ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (ê¸°ë³¸: 1000ê°œ, 0=ë¬´ì œí•œ)
    - **shuffle_data**: ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ê¸°ë³¸: True)
    - **auto_split**: ìë™ train/val ë¶„í•  ì—¬ë¶€ (ê¸°ë³¸: True, YOLO í˜¸í™˜)
    
    **ğŸ¯ ì¶œë ¥:**
    - ZIP íŒŒì¼ì— í¬í•¨: images/, labels/, data.yaml, README.txt
    - Google Colabì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
    - YOLOv8/YOLOv11 í˜¸í™˜ í˜•ì‹
    
    **ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ:**
    ```python
    # Google Colabì—ì„œ ë°ì´í„°ì…‹ ì‚¬ìš©
    !unzip -q "/content/drive/MyDrive/pinetree_training_dataset_*.zip" -d /content/dataset
    from ultralytics import YOLO
    model = YOLO('yolo11s.pt')
    results = model.train(data='/content/dataset/data.yaml', epochs=200)
    ```
    
    **ğŸ” ë¼ë²¨ë§ ê²€ì¦ ë°©ë²•:**
    1. `validation_samples_url`ì—ì„œ ê²€ì¦ ìƒ˜í”Œ ZIP ë‹¤ìš´ë¡œë“œ
    2. ì–‘ì„± ìƒ˜í”Œ: ë…¹ìƒ‰ ë°”ìš´ë”©ë°•ìŠ¤ê°€ ì‹¤ì œ í”¼í•´ëª©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    3. ìŒì„± ìƒ˜í”Œ: ì‹¤ì œë¡œ í”¼í•´ëª©ì´ ì—†ëŠ” ê¹¨ë—í•œ ë°°ê²½ì¸ì§€ í™•ì¸
    4. ë¬¸ì œ ë°œê²¬ ì‹œ GPS ì¢Œí‘œë‚˜ bbox_size ì¡°ì • í›„ ì¬ìƒì„±
    
    **ğŸ“‹ ì¶”ê°€ ë§¤ê°œë³€ìˆ˜:**
    - **class_names**: í´ë˜ìŠ¤ ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸: "damaged_tree")
    - **max_files**: ìµœëŒ€ íŒŒì¼ ìˆ˜ ì œí•œ (ê¸°ë³¸: 1000ê°œ, 0=ë¬´ì œí•œ)
    - **shuffle_data**: ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ê¸°ë³¸: True)
    - **auto_split**: ìë™ train/val ë¶„í•  ì—¬ë¶€ (ê¸°ë³¸: True, YOLO í˜¸í™˜)
    """
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not image_file.filename.lower().endswith(tuple(config.ALLOWED_IMAGE_EXTENSIONS)):
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {image_file.filename}"
            )
        
        if not csv_file.filename.lower().endswith('.csv'):
            raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not tfw_file.filename.lower().endswith('.tfw'):
            raise HTTPException(status_code=400, detail="TFW íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_base = DEFAULT_OUTPUT_DIR / f"complete_training_{timestamp}"
        output_base.mkdir(parents=True, exist_ok=True)
        
        # íƒ€ì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        tiles_images_dir = output_base / "tiles" / "images"
        tiles_labels_dir = output_base / "tiles" / "labels" 
        tiles_images_dir.mkdir(parents=True, exist_ok=True)
        tiles_labels_dir.mkdir(parents=True, exist_ok=True)
        
        # ìµœì¢… ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        dataset_dir = output_base / "training_dataset"
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì €ì¥
            image_path = os.path.join(temp_dir, image_file.filename)
            csv_path = os.path.join(temp_dir, csv_file.filename)
            tfw_path = os.path.join(temp_dir, tfw_file.filename)
            
            with open(image_path, "wb") as f:
                shutil.copyfileobj(image_file.file, f)
            with open(csv_path, "wb") as f:
                shutil.copyfileobj(csv_file.file, f)
            with open(tfw_path, "wb") as f:
                shutil.copyfileobj(tfw_file.file, f)
            
            # TFW íŒŒë¼ë¯¸í„° íŒŒì‹±
            tfw_params = []
            with open(tfw_path, 'r') as f:
                tfw_params = [float(line.strip()) for line in f.readlines()]
            
            if len(tfw_params) != 6:
                raise HTTPException(status_code=400, detail="TFW íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(csv_path)
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            # ì¢Œí‘œ ì»¬ëŸ¼ í™•ì¸
            x_col, y_col = None, None
            if 'x' in df.columns and 'y' in df.columns:
                x_col, y_col = 'x', 'y'
            elif 'longitude' in df.columns and 'latitude' in df.columns:
                x_col, y_col = 'longitude', 'latitude'
            else:
                raise HTTPException(
                    status_code=400, 
                    detail="CSV íŒŒì¼ì— 'x,y' ë˜ëŠ” 'longitude,latitude' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            
            # ì¢Œí‘œ ì»¬ëŸ¼ ì´ë¦„ í†µì¼
            df = df.rename(columns={x_col: 'x', y_col: 'y'})
            
            print(f"ğŸ“Š GPS ì¢Œí‘œ ë°ì´í„°: {len(df)}ê°œ í¬ì¸íŠ¸", flush=True)
            
            # Step 1: ğŸ“¦ ë©€í‹°ìŠ¤ì¼€ì¼ ì ì‘ì  íƒ€ì¼ë§ ë° ë¼ë²¨ ìƒì„±
            print(f"ğŸ”„ Step 1: ë©€í‹°ìŠ¤ì¼€ì¼ ì ì‘ì  íƒ€ì¼ë§ ë° ë¼ë²¨ ìƒì„± ì‹œì‘...", flush=True)
            print(f"ğŸ¯ ì ì‘ì  ë°”ìš´ë”©ë°•ìŠ¤: {config.ADAPTIVE_BBOX_MIN_SIZE}~{config.ADAPTIVE_BBOX_MAX_SIZE}px (ë°€ë„ ê¸°ë°˜)", flush=True)
            
            # ë©€í‹°ìŠ¤ì¼€ì¼ ì ì‘ì  ë°”ìš´ë”©ë°•ìŠ¤ ì²˜ë¦¬
            tile_info, coordinate_tracking = process_tiles_and_labels_simple(
                image_path, tfw_params, df, str(tiles_images_dir), str(tiles_labels_dir),
                tile_size, class_id, file_prefix, use_adaptive_bbox=True
            )
            
            tiles_with_labels = sum(1 for t in tile_info if t.labels_count > 0)
            total_labels = sum(t.labels_count for t in tile_info)
            
            # ë¼ë²¨ë§ í’ˆì§ˆ í†µê³„ ê³„ì‚°
            labels_per_tile = [t.labels_count for t in tile_info if t.labels_count > 0]
            avg_labels_per_tile = sum(labels_per_tile) / len(labels_per_tile) if labels_per_tile else 0
            
            preprocessing_info = {
                "total_tiles": len(tile_info),
                "tiles_with_labels": tiles_with_labels,
                "total_labels": total_labels,
                "tile_size": tile_size,
                "bbox_mode": f"Multiscale Adaptive {config.ADAPTIVE_BBOX_MIN_SIZE}~{config.ADAPTIVE_BBOX_MAX_SIZE}px",
                "file_prefix": file_prefix,
                "labeling_stats": {
                    "avg_labels_per_tile": round(avg_labels_per_tile, 2),
                    "max_labels_per_tile": max(labels_per_tile) if labels_per_tile else 0,
                    "min_labels_per_tile": min(labels_per_tile) if labels_per_tile else 0,
                    "coverage_rate": round(tiles_with_labels / len(tile_info) * 100, 1)
                }
            }
            
            print(f"âœ… Step 1 ì™„ë£Œ: {len(tile_info)}ê°œ íƒ€ì¼, {tiles_with_labels}ê°œ ë¼ë²¨ íƒ€ì¼, {total_labels}ê°œ ì´ ë¼ë²¨", flush=True)
            
            # Step 2: ğŸ” ë¼ë²¨ë§ í’ˆì§ˆ ê²€ì¦ ìƒ˜í”Œ ìƒì„±
            print("ğŸ”„ Step 2: ë¼ë²¨ë§ í’ˆì§ˆ ê²€ì¦ ìƒ˜í”Œ ìƒì„± ì‹œì‘...", flush=True)
            
            validation_sample_files = create_labeling_validation_samples(
                tiles_images_dir, tiles_labels_dir, output_base, num_samples=30
            )
            
            # ê²€ì¦ ìƒ˜í”Œ ZIP ìƒì„±
            validation_zip_path = None
            validation_info = {
                "validation_samples_count": len(validation_sample_files),
                "positive_samples": len([f for f in validation_sample_files if "positive" in f]),
                "negative_samples": len([f for f in validation_sample_files if "negative" in f])
            }
            
            if validation_sample_files:
                validation_zip_path = create_validation_samples_zip(output_base, timestamp, validation_sample_files)
                print(f"âœ… Step 2 ì™„ë£Œ: {len(validation_sample_files)}ê°œ ê²€ì¦ ìƒ˜í”Œ ìƒì„±", flush=True)
            else:
                print("âš ï¸ Step 2: ê²€ì¦ ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨", flush=True)
            
            # Step 3: ë”¥ëŸ¬ë‹ìš© ë°ì´í„°ì…‹ ìƒì„±  
            print("ğŸ”„ Step 3: ë”¥ëŸ¬ë‹ìš© ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...", flush=True)
            
            # íƒ€ì¼ ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ ë§¤ì¹­ (ëª¨ë“  ì´ë¯¸ì§€ í¬í•¨)
            image_files = list(tiles_images_dir.glob("*.tif"))
            label_files = list(tiles_labels_dir.glob("*.txt"))
            
            # ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ ë¼ë²¨ íŒŒì¼ ë§¤ì¹­ (ì—†ìœ¼ë©´ ë¹ˆ ë¼ë²¨ë¡œ ì²˜ë¦¬)
            matched_files = []
            positive_samples = 0  # ë¼ë²¨ì´ ìˆëŠ” ìƒ˜í”Œ ìˆ˜
            negative_samples = 0  # ë¼ë²¨ì´ ì—†ëŠ” ìƒ˜í”Œ ìˆ˜
            
            for img_file in image_files:
                label_file = tiles_labels_dir / f"{img_file.stem}.txt"
                
                # ë¼ë²¨ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¼ë²¨ íŒŒì¼ ìƒì„±
                if not label_file.exists():
                    with open(label_file, 'w') as f:
                        pass  # ë¹ˆ íŒŒì¼ ìƒì„±
                    negative_samples += 1
                else:
                    # ë¼ë²¨ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ë‚´ìš© í™•ì¸
                    with open(label_file, 'r') as f:
                        content = f.read().strip()
                    if content:
                        positive_samples += 1
                    else:
                        negative_samples += 1
                
                matched_files.append((img_file, label_file))
            
            print(f"ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {len(matched_files)}ê°œ")
            print(f"ğŸ“Š ì–‘ì„± ìƒ˜í”Œ (í”¼í•´ëª© ìˆìŒ): {positive_samples}ê°œ")
            print(f"ğŸ“Š ìŒì„± ìƒ˜í”Œ (í”¼í•´ëª© ì—†ìŒ): {negative_samples}ê°œ")
            print(f"ğŸ“Š í´ë˜ìŠ¤ ë¹„ìœ¨: Positive={positive_samples/(positive_samples+negative_samples)*100:.1f}%, Negative={negative_samples/(positive_samples+negative_samples)*100:.1f}%")
            
            if len(matched_files) == 0:
                raise HTTPException(
                    status_code=400, 
                    detail="íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            # ë°ì´í„° ì…”í”Œ
            if shuffle_data:
                import random
                random.shuffle(matched_files)
            
            # íŒŒì¼ ìˆ˜ ì œí•œ
            if max_files > 0:
                matched_files = matched_files[:max_files]
            
            # train/val ë¶„í• 
            split_idx = int(len(matched_files) * train_split) if auto_split else len(matched_files)
            train_files = matched_files[:split_idx]
            val_files = matched_files[split_idx:] if auto_split else []
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            for split_name in ['train', 'val'] if auto_split else ['train']:
                (dataset_dir / split_name / 'images').mkdir(parents=True, exist_ok=True)
                (dataset_dir / split_name / 'labels').mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ë³µì‚¬
            for img_file, label_file in train_files:
                shutil.copy2(img_file, dataset_dir / 'train' / 'images')
                shutil.copy2(label_file, dataset_dir / 'train' / 'labels')
            
            if auto_split and val_files:
                for img_file, label_file in val_files:
                    shutil.copy2(img_file, dataset_dir / 'val' / 'images')
                    shutil.copy2(label_file, dataset_dir / 'val' / 'labels')
            
            # data.yaml ìƒì„±
            class_list = [name.strip() for name in class_names.split(',')]
            data_yaml = {
                'path': '.',
                'train': 'train/images',
                'val': 'val/images' if auto_split else 'train/images',
                'nc': len(class_list),
                'names': class_list,
                
                # ë©”íƒ€ë°ì´í„°
                'created_date': datetime.datetime.now().isoformat(),
                'source_info': {
                    'image_file': image_file.filename,
                    'csv_file': csv_file.filename,
                    'total_coordinates': len(df),
                    'tile_size': tile_size,
                    'bbox_mode': f"Multiscale Adaptive {config.ADAPTIVE_BBOX_MIN_SIZE}~{config.ADAPTIVE_BBOX_MAX_SIZE}px (Density-based)"
                },
                'dataset_stats': {
                    'total_files': len(matched_files),
                    'train_files': len(train_files),
                    'val_files': len(val_files),
                    'total_labels': total_labels
                }
            }
            
            import yaml
            with open(dataset_dir / 'data.yaml', 'w') as f:
                yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)
            
            # Step 4: ZIP íŒŒì¼ ìƒì„±
            print("ğŸ”„ Step 4: ZIP íŒŒì¼ ìƒì„± ì‹œì‘...", flush=True)
            zip_filename = f"complete_training_dataset_{file_prefix}_{timestamp}.zip"
            zip_path = DEFAULT_OUTPUT_DIR / zip_filename
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ì „ì²´ë¥¼ ì••ì¶• (macOS ìˆ¨ê¹€ íŒŒì¼ ì œì™¸)
                for root, dirs, files in os.walk(dataset_dir):
                    # macOS ì‹œìŠ¤í…œ í´ë” ì œì™¸
                    if '__MACOSX' in root:
                        continue
                    for file in files:
                        # macOS ìˆ¨ê¹€ íŒŒì¼ ë° ì‹œìŠ¤í…œ íŒŒì¼ ì œì™¸
                        if file.startswith('.') or file.startswith('._'):
                            continue
                        file_path = os.path.join(root, file)
                        archive_name = os.path.relpath(file_path, dataset_dir)
                        zipf.write(file_path, archive_name)
            
            # ìµœì¢… ì •ë³´ êµ¬ì„±
            dataset_info = {
                "zip_filename": zip_filename,
                "total_files": len(matched_files),
                "train_files": len(train_files),
                "val_files": len(val_files),
                "total_labels": total_labels,
                "classes": class_list,
                "data_yaml_created": True,
                "file_size_mb": round(zip_path.stat().st_size / (1024 * 1024), 2)
            }
            
            download_url = f"/api/v1/preprocessing/download/{zip_filename}"
            validation_samples_url = None
            
            if validation_zip_path:
                validation_samples_url = f"/api/v1/preprocessing/download/{os.path.basename(validation_zip_path)}"
            
            print(f"âœ… í†µí•© ì²˜ë¦¬ ì™„ë£Œ!", flush=True)
            print(f"ğŸ“¦ ë°ì´í„°ì…‹ ZIP: {zip_filename} ({dataset_info['file_size_mb']}MB)", flush=True)
            if validation_samples_url:
                print(f"ğŸ” ê²€ì¦ ìƒ˜í”Œ ZIP: {os.path.basename(validation_zip_path)}", flush=True)
            
            return IntegratedTrainingResponse(
                success=True,
                message=f"í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ! ì´ {len(matched_files)}ê°œ íŒŒì¼, {total_labels}ê°œ ë¼ë²¨ (ê²€ì¦ ìƒ˜í”Œ í¬í•¨) | GPS ë§¤í•‘: {coordinate_tracking['labels_created']}ê°œ/{coordinate_tracking['csv_total']}ê°œ ({coordinate_tracking['success_rate']})",
                preprocessing_info=preprocessing_info,
                training_dataset_info=dataset_info,
                download_url=download_url,
                zip_filename=zip_filename,
                validation_samples_url=validation_samples_url,
                validation_info=validation_info,
                coordinate_validation=coordinate_tracking  # ğŸ” GPS ì¢Œí‘œ ì¶”ì  ì •ë³´ ì¶”ê°€
            )
            
    except Exception as e:
        print(f"âŒ í†µí•© ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", flush=True)
        raise HTTPException(status_code=500, detail=f"í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.get("/download/{filename}")
async def download_processed_file(filename: str):
    """ì²˜ë¦¬ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    file_path = DEFAULT_OUTPUT_DIR / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/octet-stream'
    )

# ğŸ”— ë‹¤ì¤‘ ë°ì´í„°ì…‹ í†µí•©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤
def analyze_dataset_structure(extract_dir: Path, zip_filename: str) -> Dict:
    """ZIP íŒŒì¼ì˜ ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ ë¶„ì„"""
    
    # data.yaml ì°¾ê¸°
    data_yaml_path = None
    for root, dirs, files in os.walk(extract_dir):
        if 'data.yaml' in files:
            data_yaml_path = Path(root) / 'data.yaml'
            break
    
    dataset_info = {
        "zip_filename": zip_filename,
        "has_data_yaml": data_yaml_path is not None,
        "train_path": None,
        "val_path": None,
        "images_count": {"train": 0, "val": 0},
        "labels_count": {"train": 0, "val": 0}
    }
    
    if data_yaml_path:
        try:
            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                data_yaml = yaml.safe_load(f)
                dataset_info["classes"] = data_yaml.get("names", ["damaged_tree"])
                dataset_info["nc"] = data_yaml.get("nc", 1)
        except:
            dataset_info["classes"] = ["damaged_tree"]
            dataset_info["nc"] = 1
    
    # train/val í´ë” ì°¾ê¸°
    base_dir = data_yaml_path.parent if data_yaml_path else extract_dir
    
    for split in ["train", "val"]:
        images_dir = base_dir / split / "images"
        labels_dir = base_dir / split / "labels"
        
        if images_dir.exists():
            dataset_info[f"{split}_path"] = images_dir.parent
            dataset_info["images_count"][split] = len(list(images_dir.glob("*.tif")) + list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png")))
            if labels_dir.exists():
                dataset_info["labels_count"][split] = len(list(labels_dir.glob("*.txt")))
    
    return dataset_info

def collect_dataset_files(extract_dir: Path, dataset_info: Dict, source_id: int) -> tuple:
    """ë°ì´í„°ì…‹ì—ì„œ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€"""
    
    train_files = []
    val_files = []
    
    for split, file_list in [("train", train_files), ("val", val_files)]:
        split_path = dataset_info.get(f"{split}_path")
        if not split_path:
            continue
            
        images_dir = Path(split_path) / "images"
        labels_dir = Path(split_path) / "labels"
        
        if images_dir.exists():
            for img_file in images_dir.glob("*"):
                if img_file.suffix.lower() in ['.tif', '.tiff', '.jpg', '.jpeg', '.png']:
                    label_file = labels_dir / f"{img_file.stem}.txt"
                    
                    # ë¼ë²¨ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¼ë²¨ íŒŒì¼ë¡œ ì²˜ë¦¬
                    if not label_file.exists():
                        label_file = None
                    
                    file_list.append({
                        "image_path": img_file,
                        "label_path": label_file,
                        "source_id": source_id,
                        "source_zip": dataset_info["zip_filename"],
                        "original_split": split
                    })
    
    return train_files, val_files

def create_merged_dataset(all_train_files: List, all_val_files: List, temp_path: Path, 
                         merged_dataset_name: str, train_split: float, shuffle_data: bool,
                         class_names: str, timestamp: str) -> Dict:
    """í†µí•© ë°ì´í„°ì…‹ ìƒì„±"""
    
    # ëª¨ë“  íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    all_files = all_train_files + all_val_files
    
    if shuffle_data:
        import random
        random.shuffle(all_files)
    
    # ìƒˆë¡œìš´ train/val ë¶„í• 
    split_idx = int(len(all_files) * train_split)
    new_train_files = all_files[:split_idx]
    new_val_files = all_files[split_idx:]
    
    # í†µí•© ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±
    dataset_dir = temp_path / "merged_dataset"
    for split in ["train", "val"]:
        (dataset_dir / split / "images").mkdir(parents=True, exist_ok=True)
        (dataset_dir / split / "labels").mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ ë³µì‚¬ ë° ì¤‘ë³µëª… ì²˜ë¦¬
    copied_files = {"train": 0, "val": 0}
    filename_counter = {}  # ì¤‘ë³µ íŒŒì¼ëª… ì¹´ìš´í„°
    
    for split, files in [("train", new_train_files), ("val", new_val_files)]:
        for file_info in files:
            # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„±
            orig_name = file_info["image_path"].stem
            if orig_name in filename_counter:
                filename_counter[orig_name] += 1
                unique_name = f"{orig_name}_{filename_counter[orig_name]:03d}"
            else:
                filename_counter[orig_name] = 0
                unique_name = orig_name
            
            # ì´ë¯¸ì§€ íŒŒì¼ ë³µì‚¬
            src_img = file_info["image_path"]
            dst_img = dataset_dir / split / "images" / f"{unique_name}{src_img.suffix}"
            shutil.copy2(src_img, dst_img)
            
            # ë¼ë²¨ íŒŒì¼ ë³µì‚¬ (ì—†ìœ¼ë©´ ë¹ˆ íŒŒì¼ ìƒì„±)
            dst_label = dataset_dir / split / "labels" / f"{unique_name}.txt"
            if file_info["label_path"] and file_info["label_path"].exists():
                shutil.copy2(file_info["label_path"], dst_label)
            else:
                # ë¹ˆ ë¼ë²¨ íŒŒì¼ ìƒì„±
                with open(dst_label, 'w') as f:
                    pass
            
            copied_files[split] += 1
    
    # data.yaml ìƒì„±
    class_list = [name.strip() for name in class_names.split(',')]
    data_yaml = {
        'path': '.',
        'train': 'train/images',
        'val': 'val/images',
        'nc': len(class_list),
        'names': class_list,
        
        # í†µí•© ë©”íƒ€ë°ì´í„°
        'created_date': datetime.datetime.now().isoformat(),
        'merge_info': {
            'source_datasets': len(set(f["source_zip"] for f in all_files)),
            'total_source_files': len(all_files),
            'train_split_ratio': train_split,
            'shuffled': shuffle_data
        },
        'dataset_stats': {
            'total_files': len(all_files),
            'train_files': copied_files["train"],
            'val_files': copied_files["val"]
        }
    }
    
    with open(dataset_dir / 'data.yaml', 'w', encoding='utf-8') as f:
        yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False, allow_unicode=True)
    
    # ZIP íŒŒì¼ ìƒì„±
    zip_filename = f"merged_dataset_{merged_dataset_name}_{timestamp}.zip"
    zip_path = DEFAULT_OUTPUT_DIR / zip_filename
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(dataset_dir):
            # macOS ì‹œìŠ¤í…œ íŒŒì¼ ì œì™¸
            if '__MACOSX' in root:
                continue
            for file in files:
                if file.startswith('.') or file.startswith('._'):
                    continue
                file_path = os.path.join(root, file)
                archive_name = os.path.relpath(file_path, dataset_dir)
                zipf.write(file_path, archive_name)
    
    return {
        "zip_filename": zip_filename,
        "total_files": len(all_files),
        "train_files": copied_files["train"],
        "val_files": copied_files["val"],
        "classes": class_list,
        "file_size_mb": round(zip_path.stat().st_size / (1024 * 1024), 2),
        "duplicate_names_resolved": sum(1 for count in filename_counter.values() if count > 0)
    }

@router.post("/merge-datasets", response_model=MergedDatasetResponse)
async def merge_multiple_datasets(
    dataset_zips: List[UploadFile] = File(..., description="í†µí•©í•  ë°ì´í„°ì…‹ ZIP íŒŒì¼ë“¤ (ì—¬ëŸ¬ ê°œ)"),
    merged_dataset_name: str = Form(default="merged_dataset", description="í†µí•© ë°ì´í„°ì…‹ ì´ë¦„"),
    train_split: float = Form(default=0.8, description="í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  ë¹„ìœ¨ (0.0-1.0)"),
    shuffle_data: bool = Form(default=True, description="ë°ì´í„° ì…”í”Œ ì—¬ë¶€"),
    class_names: str = Form(default="damaged_tree", description="í´ë˜ìŠ¤ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)")
):
    """
    ğŸ”— ì—¬ëŸ¬ ë°ì´í„°ì…‹ ZIP íŒŒì¼ì„ í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ ëŒ€ìš©ëŸ‰ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
    
    - ê° ZIPì˜ train/val ë°ì´í„°ë¥¼ ëª¨ë‘ í†µí•©
    - íŒŒì¼ëª… ì¤‘ë³µ ìë™ í•´ê²° (ë²ˆí˜¸ ì¶”ê°€)
    - ìƒˆë¡œìš´ train/val ë¶„í•  ì ìš©
    - í†µí•© data.yaml ìƒì„±
    """
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        # ì„ì‹œ ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            print(f"ğŸ”— ë‹¤ì¤‘ ë°ì´í„°ì…‹ í†µí•© ì‹œì‘: {len(dataset_zips)}ê°œ ZIP íŒŒì¼", flush=True)
            
            # Step 1: ê° ZIP íŒŒì¼ ì••ì¶• í•´ì œ ë° ë¶„ì„
            source_datasets = []
            all_train_files = []  # (image_path, label_path, source_info)
            all_val_files = []
            
            for i, zip_file in enumerate(dataset_zips):
                print(f"ğŸ”„ ZIP {i+1}/{len(dataset_zips)} ì²˜ë¦¬ ì¤‘: {zip_file.filename}", flush=True)
                
                # ZIP íŒŒì¼ ì €ì¥
                zip_path = temp_path / f"dataset_{i}_{zip_file.filename}"
                with open(zip_path, "wb") as f:
                    shutil.copyfileobj(zip_file.file, f)
                
                # ZIP ì••ì¶• í•´ì œ
                extract_dir = temp_path / f"extracted_{i}"
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                
                # ë°ì´í„°ì…‹ êµ¬ì¡° ë¶„ì„
                dataset_info = analyze_dataset_structure(extract_dir, zip_file.filename)
                source_datasets.append(dataset_info)
                
                # íŒŒì¼ ìˆ˜ì§‘
                train_files, val_files = collect_dataset_files(extract_dir, dataset_info, i)
                all_train_files.extend(train_files)
                all_val_files.extend(val_files)
            
            print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ: í•™ìŠµ {len(all_train_files)}ê°œ, ê²€ì¦ {len(all_val_files)}ê°œ íŒŒì¼", flush=True)
            
            # Step 2: í†µí•© ë°ì´í„°ì…‹ ìƒì„±
            merged_info = create_merged_dataset(
                all_train_files, all_val_files, temp_path, merged_dataset_name, 
                train_split, shuffle_data, class_names, timestamp
            )
            
            return MergedDatasetResponse(
                success=True,
                message=f"ë°ì´í„°ì…‹ í†µí•© ì™„ë£Œ! {len(dataset_zips)}ê°œ ZIP â†’ í†µí•© ë°ì´í„°ì…‹ ìƒì„±",
                merged_dataset_info=merged_info,
                download_url=f"/api/v1/preprocessing/download/{merged_info['zip_filename']}",
                zip_filename=merged_info['zip_filename'],
                source_datasets=source_datasets
            )
            
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ í†µí•© ì˜¤ë¥˜: {str(e)}", flush=True)
        raise HTTPException(status_code=500, detail=f"ë°ì´í„°ì…‹ í†µí•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
