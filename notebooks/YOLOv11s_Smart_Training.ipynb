{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800110f8",
   "metadata": {},
   "source": [
    "# ğŸš€ YOLOv11s ì†Œë‚˜ë¬´ í”¼í•´ëª© íƒì§€ - ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì‹œìŠ¤í…œ\n",
    "\n",
    "## ğŸ¯ **3ë‹¨ê³„ ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì›Œí¬í”Œë¡œ**\n",
    "\n",
    "### **âš¡ ë¹ ë¥¸ ì‹œì‘**\n",
    "1. **ğŸ–¥ï¸ GPU ì„¤ì •**: `ëŸ°íƒ€ì„` â†’ `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½` â†’ `T4 GPU` ì„ íƒ\n",
    "2. **ğŸš€ ë‹¨ê³„ë³„ ì‹¤í–‰**: 1ë‹¨ê³„ë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹¤í–‰  \n",
    "3. **â˜• ëŒ€ê¸°**: ì•½ 1-2ì‹œê°„ í›„ ì™„ì„±!\n",
    "\n",
    "### **ğŸ”„ 3ë‹¨ê³„ ì‹¤í–‰ í”Œë¡œìš°**\n",
    "- **ğŸ”§ 1ë‹¨ê³„**: í™˜ê²½ ì„¤ì • ë° í›ˆë ¨ ë°ì´í„° ìë™ íƒì§€\n",
    "- **ğŸ“ 2ë‹¨ê³„**: YOLOv11s ë©€í‹°ìŠ¤ì¼€ì¼ í•™ìŠµ + Google Drive ë°±ì—…\n",
    "- **ğŸ¯ 3ë‹¨ê³„**: í•™ìŠµëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° íƒì§€ ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "### **ğŸ¯ ë„¤ì´ë° ê·œì¹™**\n",
    "- **í”„ë¡œì íŠ¸**: `pinetree_multiscale` (ë©€í‹°ìŠ¤ì¼€ì¼ ìµœì í™”)\n",
    "- **ì‹¤í–‰ëª…**: `multiscale_training_YYYYMMDD_HHMM`\n",
    "- **ì˜ˆì‹œ**: `multiscale_training_20250925_1430`\n",
    "\n",
    "### **ğŸ“ˆ ëª©í‘œ ì„±ëŠ¥ (mAP 0.65+ ë‹¬ì„±)**\n",
    "- **mAP50**: 65%+ (ë©€í‹°ìŠ¤ì¼€ì¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)\n",
    "- **Precision**: 70%+ (ì˜¤íƒì§€ ìµœì†Œí™”)  \n",
    "- **Recall**: 60%+ (ì‹¤ì œ í”¼í•´ëª© íƒì§€ìœ¨)\n",
    "- **ì¶”ë¡  ì†ë„**: Tesla T4ì—ì„œ ì‹¤ì‹œê°„ ì²˜ë¦¬\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e444eb0",
   "metadata": {
    "id": "4e444eb0"
   },
   "source": [
    "## ğŸ”§ **1ë‹¨ê³„: í™˜ê²½ ì„¤ì • ë° í›ˆë ¨ ë°ì´í„° ìë™ íƒì§€**\n",
    "\n",
    "> YOLOv11 ì„¤ì¹˜, GPU ìµœì í™”, í›ˆë ¨ ë°ì´í„°ì…‹ ìë™ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69829093",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27047,
     "status": "ok",
     "timestamp": 1758779435876,
     "user": {
      "displayName": "ë¦¬ì§€ëŒ",
      "userId": "13174544160225006253"
     },
     "user_tz": -540
    },
    "id": "69829093",
    "outputId": "0b946b79-efd6-45f9-bcf6-f9078e4b697e"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ 1ë‹¨ê³„: í™˜ê²½ ì„¤ì • ë° ê¸°ì¡´ ëª¨ë¸ íƒì§€\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0])\n",
    "        print(f\"âœ… {package} ì´ë¯¸ ì„¤ì¹˜ë¨\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "print(\"ğŸš€ YOLOv11s ì†Œë‚˜ë¬´ í”¼í•´ëª© íƒì§€ - í™˜ê²½ ì„¤ì •\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ï¿½ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (êµ¬ê¸€ ì½”ë© í™˜ê²½)\n",
    "print(\"ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸:\")\n",
    "required_packages = [\n",
    "    \"ultralytics\",\n",
    "    \"torch\", \n",
    "    \"torchvision\",\n",
    "    \"opencv-python\",\n",
    "    \"pillow\",\n",
    "    \"matplotlib\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"pyyaml\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ ì™„ë£Œ\\n\")\n",
    "\n",
    "# ï¿½ğŸ—‚ï¸ Google Drive ë§ˆìš´íŠ¸ ë° ZIP ë°ì´í„° ìë™ ì••ì¶• í•´ì œ (Colab í™˜ê²½ì—ì„œë§Œ)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ğŸ“ Google Colab í™˜ê²½ ê°ì§€\")\n",
    "    \n",
    "    # Google Drive ë§ˆìš´íŠ¸\n",
    "    print(\"ğŸ”— Google Drive ë§ˆìš´íŠ¸ ì¤‘...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")\n",
    "    \n",
    "    # ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "    drive_path = '/content/drive/MyDrive/pinetree_scan'\n",
    "    if not os.path.exists(drive_path):\n",
    "        os.makedirs(drive_path, exist_ok=True)\n",
    "        print(f\"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±: {drive_path}\")\n",
    "    \n",
    "    # ğŸ” ZIP íŒŒì¼ ìë™ íƒì§€ ë° ì••ì¶• í•´ì œ í•¨ìˆ˜\n",
    "    def find_and_extract_training_zip():\n",
    "        \"\"\"Google Driveì—ì„œ training_data ZIP íŒŒì¼ì„ ì°¾ì•„ ì••ì¶• í•´ì œ\"\"\"\n",
    "        import zipfile\n",
    "        import shutil\n",
    "        \n",
    "        print(\"\\nğŸ” Google Driveì—ì„œ ZIP ë°ì´í„°ì…‹ íƒì§€ ì¤‘...\")\n",
    "        \n",
    "        # ZIP íŒŒì¼ ê²€ìƒ‰ ê²½ë¡œ (ìš°ì„ ìˆœìœ„ë³„)\n",
    "        zip_search_paths = [\n",
    "            # 1ìˆœìœ„: complete_training_dataset_ íŒ¨í„´ (ìµœìš°ì„  - ìµœì‹  í•™ìŠµ ë°ì´í„°ì…‹)\n",
    "            '/content/drive/MyDrive/pinetree_scan/complete_training_dataset_*.zip',\n",
    "            '/content/drive/MyDrive/complete_training_dataset_*.zip',\n",
    "            \n",
    "            # 2ìˆœìœ„: pinetree_scan/training_data í´ë” ë‚´ì˜ ëª¨ë“  ZIP íŒŒì¼\n",
    "            '/content/drive/MyDrive/pinetree_scan/training_data/*.zip',\n",
    "            \n",
    "            # 3ìˆœìœ„: pinetree_scan í´ë” ë‚´\n",
    "            '/content/drive/MyDrive/pinetree_scan/training_data.zip',\n",
    "            '/content/drive/MyDrive/pinetree_scan/dataset.zip', \n",
    "            '/content/drive/MyDrive/pinetree_scan/data.zip',\n",
    "            \n",
    "            # 4ìˆœìœ„: ì§ì ‘ ì—…ë¡œë“œ\n",
    "            '/content/drive/MyDrive/training_data.zip',\n",
    "            '/content/drive/MyDrive/dataset.zip',\n",
    "            '/content/drive/MyDrive/data.zip',\n",
    "            '/content/drive/MyDrive/yolo_data.zip',\n",
    "            '/content/drive/MyDrive/pine_data.zip',\n",
    "            \n",
    "            # 5ìˆœìœ„: ê¸°íƒ€ ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰\n",
    "            '/content/drive/MyDrive/*training*.zip',\n",
    "            '/content/drive/MyDrive/*dataset*.zip',\n",
    "            '/content/drive/MyDrive/*data*.zip',\n",
    "            '/content/drive/MyDrive/pinetree_scan/*training*.zip',\n",
    "            '/content/drive/MyDrive/pinetree_scan/*dataset*.zip'\n",
    "        ]\n",
    "        \n",
    "        found_zips = []\n",
    "        \n",
    "        for search_path in zip_search_paths:\n",
    "            if '*' in search_path:\n",
    "                # ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰\n",
    "                import glob\n",
    "                matches = glob.glob(search_path)\n",
    "                for match in matches:\n",
    "                    if os.path.exists(match) and match.endswith('.zip'):\n",
    "                        found_zips.append(match)\n",
    "            else:\n",
    "                # ì§ì ‘ ê²½ë¡œ í™•ì¸\n",
    "                if os.path.exists(search_path):\n",
    "                    found_zips.append(search_path)\n",
    "        \n",
    "        if found_zips:\n",
    "            # ì¤‘ë³µ ì œê±°\n",
    "            found_zips = list(set(found_zips))\n",
    "            \n",
    "            print(f\"ğŸ“¦ ë°œê²¬ëœ ZIP íŒŒì¼ë“¤:\")\n",
    "            for i, zip_file in enumerate(found_zips, 1):\n",
    "                file_size = os.path.getsize(zip_file) / 1024 / 1024  # MB\n",
    "                print(f\"  {i}. {os.path.basename(zip_file)} ({file_size:.1f}MB)\")\n",
    "                print(f\"     ê²½ë¡œ: {zip_file}\")\n",
    "            \n",
    "            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œí•˜ì—¬ ìµœì‹  íŒŒì¼ ì„ íƒ\n",
    "            def extract_date_from_filename(filename):\n",
    "                \"\"\"íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ íŒ¨í„´(YYYYMMDD_HHMMSS) ì¶”ì¶œ\"\"\"\n",
    "                import re\n",
    "                # complete_training_dataset_ì§€ì—­ëª…_20250925_150027.zip íŒ¨í„´\n",
    "                match = re.search(r'(\\d{8}_\\d{6})', filename)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "                # ë‹¤ë¥¸ ë‚ ì§œ íŒ¨í„´ë“¤\n",
    "                match = re.search(r'(\\d{8})', filename)\n",
    "                if match:\n",
    "                    return match.group(1) + \"_000000\"\n",
    "                return \"19700101_000000\"  # ê¸°ë³¸ê°’ (ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œ)\n",
    "            \n",
    "            # complete_training_dataset_ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "            complete_training_zips = [f for f in found_zips if 'complete_training_dataset_' in os.path.basename(f)]\n",
    "            \n",
    "            if complete_training_zips:\n",
    "                # complete_training_dataset_ íŒŒì¼ ì¤‘ ê°€ì¥ ìµœì‹  ë‚ ì§œ ì„ íƒ\n",
    "                zip_path = max(complete_training_zips, key=lambda x: extract_date_from_filename(os.path.basename(x)))\n",
    "                print(f\"\\nğŸ¯ ìµœì‹  complete_training_dataset íŒŒì¼ ìë™ ì„ íƒ!\")\n",
    "                \n",
    "                # ë‚ ì§œ ë° ì§€ì—­ ì •ë³´ ì¶”ì¶œ\n",
    "                filename = os.path.basename(zip_path)\n",
    "                date_match = extract_date_from_filename(filename)\n",
    "                if date_match != \"19700101_000000\":\n",
    "                    date_str = f\"{date_match[:4]}-{date_match[4:6]}-{date_match[6:8]} {date_match[9:11]}:{date_match[11:13]}:{date_match[13:15]}\"\n",
    "                    print(f\"  ğŸ“… ë°ì´í„°ì…‹ ë‚ ì§œ: {date_str}\")\n",
    "                \n",
    "                # ì§€ì—­ëª… ì¶”ì¶œ\n",
    "                import re\n",
    "                region_match = re.search(r'complete_training_dataset_([^_]+)_\\d{8}_\\d{6}\\.zip', filename)\n",
    "                if region_match:\n",
    "                    region_name = region_match.group(1)\n",
    "                    print(f\"  ğŸŒ ëŒ€ìƒ ì§€ì—­: {region_name}\")\n",
    "                    \n",
    "            else:\n",
    "                # ê¸°ë³¸ ë¡œì§: ê°€ì¥ í° íŒŒì¼ì„ ì„ íƒ (í›ˆë ¨ ë°ì´í„°ê°€ í´ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)\n",
    "                zip_path = max(found_zips, key=os.path.getsize)\n",
    "                print(f\"\\nâœ… í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ì„ íƒëœ íŒŒì¼:\")\n",
    "                \n",
    "            print(f\"ğŸ“ ìµœì¢… ì„ íƒ: {os.path.basename(zip_path)}\")\n",
    "            print(f\"ğŸ“ ì „ì²´ ê²½ë¡œ: {zip_path}\")\n",
    "            \n",
    "            # íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "            zip_size = os.path.getsize(zip_path) / 1024 / 1024  # MB\n",
    "            print(f\"ğŸ“ í¬ê¸°: {zip_size:.1f}MB\")\n",
    "            \n",
    "            # ì••ì¶• í•´ì œ ëŒ€ìƒ ê²½ë¡œ\n",
    "            extract_path = '/content/drive/MyDrive/pinetree_scan/training_data'\n",
    "            temp_extract_path = '/content/training_data_temp'\n",
    "            \n",
    "            try:\n",
    "                print(f\"\\nğŸ”„ ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì¤‘...\")\n",
    "                \n",
    "                # ì„ì‹œ ê²½ë¡œì— ì••ì¶• í•´ì œ\n",
    "                if os.path.exists(temp_extract_path):\n",
    "                    shutil.rmtree(temp_extract_path)\n",
    "                \n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(temp_extract_path)\n",
    "                \n",
    "                print(f\"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ: {temp_extract_path}\")\n",
    "                \n",
    "                # ì••ì¶• í•´ì œëœ ë‚´ìš© í™•ì¸ ë° ì •ë¦¬\n",
    "                extracted_items = os.listdir(temp_extract_path)\n",
    "                print(f\"ğŸ“‹ ì••ì¶• í•´ì œ ë‚´ìš©: {extracted_items}\")\n",
    "                \n",
    "                # training_data í´ë” êµ¬ì¡° ì •ë¦¬\n",
    "                final_path = extract_path\n",
    "                \n",
    "                # ê¸°ì¡´ training_dataê°€ ìˆìœ¼ë©´ ì‚­ì œ\n",
    "                if os.path.exists(final_path):\n",
    "                    shutil.rmtree(final_path)\n",
    "                    print(f\"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {final_path}\")\n",
    "                \n",
    "                # ì••ì¶• í•´ì œëœ ë‚´ìš©ì´ training_data í´ë”ì¸ì§€ í™•ì¸\n",
    "                if 'training_data' in extracted_items:\n",
    "                    # training_data í´ë”ê°€ í¬í•¨ëœ ê²½ìš°\n",
    "                    source_path = os.path.join(temp_extract_path, 'training_data')\n",
    "                    shutil.move(source_path, final_path)\n",
    "                elif len(extracted_items) == 1 and os.path.isdir(os.path.join(temp_extract_path, extracted_items[0])):\n",
    "                    # ë‹¨ì¼ í´ë”ì¸ ê²½ìš°\n",
    "                    source_path = os.path.join(temp_extract_path, extracted_items[0])\n",
    "                    shutil.move(source_path, final_path)\n",
    "                else:\n",
    "                    # íŒŒì¼ë“¤ì´ ì§ì ‘ ì••ì¶•ëœ ê²½ìš°\n",
    "                    shutil.move(temp_extract_path, final_path)\n",
    "                \n",
    "                # ì„ì‹œ í´ë” ì •ë¦¬\n",
    "                if os.path.exists(temp_extract_path):\n",
    "                    shutil.rmtree(temp_extract_path)\n",
    "                \n",
    "                # ì••ì¶• í•´ì œ ê²°ê³¼ í™•ì¸\n",
    "                if os.path.exists(final_path):\n",
    "                    contents = os.listdir(final_path)\n",
    "                    print(f\"ğŸ“‚ ìµœì¢… ê²½ë¡œ: {final_path}\")\n",
    "                    print(f\"ğŸ“‹ ë‚´ìš©: {contents}\")\n",
    "                    \n",
    "                    # ê¸°ë³¸ êµ¬ì¡° í™•ì¸\n",
    "                    images_dir = os.path.join(final_path, 'images')\n",
    "                    labels_dir = os.path.join(final_path, 'labels')\n",
    "                    \n",
    "                    if os.path.exists(images_dir):\n",
    "                        image_count = len([f for f in os.listdir(images_dir) \n",
    "                                         if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff'))])\n",
    "                        print(f\"ğŸ“¸ ì´ë¯¸ì§€: {image_count}ê°œ\")\n",
    "                    \n",
    "                    if os.path.exists(labels_dir):\n",
    "                        label_count = len([f for f in os.listdir(labels_dir) \n",
    "                                         if f.endswith('.txt')])\n",
    "                        print(f\"ğŸ·ï¸ ë¼ë²¨: {label_count}ê°œ\")\n",
    "                \n",
    "                return final_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ZIP ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}\")\n",
    "                return None\n",
    "        \n",
    "        else:\n",
    "            print(\"âŒ ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            print(\"ğŸ’¡ ë‹¤ìŒ ê²½ë¡œì— ZIP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:\")\n",
    "            print(\"  ğŸ“¦ /content/drive/MyDrive/pinetree_scan/training_data/your_data.zip\")\n",
    "            print(\"  ğŸ“¦ /content/drive/MyDrive/pinetree_scan/training_data.zip\")\n",
    "            print(\"  ğŸ“¦ /content/drive/MyDrive/training_data.zip\")\n",
    "            print(\"  ğŸ“¦ /content/drive/MyDrive/dataset.zip\")\n",
    "            return None\n",
    "    \n",
    "    # ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì‹¤í–‰\n",
    "    extracted_data_path = find_and_extract_training_zip()\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„° ê²½ë¡œ í™•ì¸ (ì••ì¶• í•´ì œ ê²°ê³¼ í¬í•¨)\n",
    "    data_paths = [\n",
    "        extracted_data_path,  # ì••ì¶• í•´ì œëœ ê²½ë¡œ ìš°ì„ \n",
    "        '/content/drive/MyDrive/pinetree_scan/training_data',\n",
    "        '/content/training_data',\n",
    "        '/content/drive/MyDrive/training_data'\n",
    "    ]\n",
    "    \n",
    "    data_path = None\n",
    "    for path in data_paths:\n",
    "        if path and os.path.exists(path):\n",
    "            # data.yamlì´ ìˆê±°ë‚˜ images í´ë”ê°€ ìˆìœ¼ë©´ ìœ íš¨í•œ ê²½ë¡œë¡œ ê°„ì£¼\n",
    "            if (os.path.exists(f\"{path}/data.yaml\") or \n",
    "                os.path.exists(f\"{path}/images\")):\n",
    "                data_path = path\n",
    "                print(f\"âœ… í•™ìŠµ ë°ì´í„° ë°œê²¬: {data_path}\")\n",
    "                break\n",
    "    \n",
    "    if not data_path:\n",
    "        print(\"âš ï¸ í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ğŸ“‹ í•„ìš”í•œ íŒŒì¼ êµ¬ì¡°:\")\n",
    "        print(\"  ğŸ“¦ ZIP íŒŒì¼:\")\n",
    "        print(\"    ğŸ“‚ training_data.zip\")\n",
    "        print(\"      ğŸ“‚ images/ (ì´ë¯¸ì§€ íŒŒì¼ë“¤)\")\n",
    "        print(\"      ğŸ“‚ labels/ (ë¼ë²¨ íŒŒì¼ë“¤)\")\n",
    "        print(\"      ğŸ“„ data.yaml (ì„ íƒì‚¬í•­)\")\n",
    "        print(\"\")\n",
    "        print(\"  ğŸ“ ë˜ëŠ” í´ë” êµ¬ì¡°:\")\n",
    "        print(\"    ğŸ“‚ training_data/\")\n",
    "        print(\"      ğŸ“„ data.yaml\")\n",
    "        print(\"      ğŸ“‚ images/\")\n",
    "        print(\"      ğŸ“‚ labels/\")\n",
    "        data_path = '/content/training_data'  # ê¸°ë³¸ê°’\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ğŸ’» ë¡œì»¬ í™˜ê²½ ê°ì§€\")\n",
    "    data_path = './data'  # ë¡œì»¬ ê¸°ë³¸ ê²½ë¡œ\n",
    "\n",
    "# ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "packages = [\n",
    "    \"ultralytics\",\n",
    "    \"torch\", \n",
    "    \"torchvision\",\n",
    "    \"opencv-python\",\n",
    "    \"pillow\",\n",
    "    \"matplotlib\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"roboflow\"  # ë°ì´í„°ì…‹ ê´€ë¦¬ìš© ì¶”ê°€\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜:\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"\\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´:\")\n",
    "print(f\"  Python: {sys.version.split()[0]}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA ì‚¬ìš©ê°€ëŠ¥: {'âœ…' if torch.cuda.is_available() else 'âŒ'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"  GPU: {gpu_name}\")\n",
    "    print(f\"  GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB\")\n",
    "    \n",
    "    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"  ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"  âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŸ°íƒ€ì„ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”!\")\n",
    "\n",
    "# ğŸ† ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ìë™ íƒì§€ (ê°•í™”ëœ ë²„ì „)\n",
    "print(f\"\\nğŸ” ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ íƒì§€:\")\n",
    "\n",
    "def find_best_model():\n",
    "    \"\"\"ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ëŠ” í–¥ìƒëœ í•¨ìˆ˜\"\"\"\n",
    "    # ìš°ì„ ìˆœìœ„ë³„ ëª¨ë¸ ê²€ìƒ‰ ê²½ë¡œ\n",
    "    search_priorities = [\n",
    "        # 1ìˆœìœ„: ìµœì‹  í•™ìŠµ ê²°ê³¼ (í”¼í•´ëª© íƒì§€ ì „ìš©)\n",
    "        {\n",
    "            'priority': 1,\n",
    "            'description': 'ìµœì‹  í”¼í•´ëª© íƒì§€ ëª¨ë¸',\n",
    "            'patterns': [\n",
    "                'pinetree_yolov11s/damage_detection_*/weights/best.pt',\n",
    "                '/content/pinetree_yolov11s/damage_detection_*/weights/best.pt',\n",
    "                '/content/drive/MyDrive/pinetree_scan/results/damage_detection_*/weights/best.pt',\n",
    "                '/content/drive/MyDrive/best.pt'\n",
    "            ]\n",
    "        },\n",
    "        # 2ìˆœìœ„: ì¼ë°˜ í•™ìŠµ ê²°ê³¼\n",
    "        {\n",
    "            'priority': 2,\n",
    "            'description': 'ì¼ë°˜ í•™ìŠµ ëª¨ë¸',\n",
    "            'patterns': [\n",
    "                'runs/detect/train*/weights/best.pt',\n",
    "                '/content/runs/detect/train*/weights/best.pt',\n",
    "                '/content/drive/MyDrive/pinetree_scan/runs/detect/train*/weights/best.pt',\n",
    "            ]\n",
    "        },\n",
    "        # 3ìˆœìœ„: ìˆ˜ë™ ì €ì¥ ëª¨ë¸\n",
    "        {\n",
    "            'priority': 3,\n",
    "            'description': 'ìˆ˜ë™ ì €ì¥ ëª¨ë¸',\n",
    "            'patterns': [\n",
    "                '/content/drive/MyDrive/pinetree_scan/models/best.pt',\n",
    "                '/content/drive/MyDrive/pinetree_scan/weights/best.pt',\n",
    "                'models/best.pt',\n",
    "                'weights/best.pt',\n",
    "                '/content/best.pt'\n",
    "            ]\n",
    "        },\n",
    "        # 4ìˆœìœ„: ê¸°ë³¸ ëª¨ë¸\n",
    "        {\n",
    "            'priority': 4,\n",
    "            'description': 'ê¸°ë³¸ YOLO ëª¨ë¸',\n",
    "            'patterns': [\n",
    "                'yolo11s.pt',\n",
    "                '/content/yolo11s.pt'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    found_models = []\n",
    "    \n",
    "    for priority_group in search_priorities:\n",
    "        for pattern in priority_group['patterns']:\n",
    "            if '*' in pattern:\n",
    "                matches = glob.glob(pattern)\n",
    "                for match in matches:\n",
    "                    if os.path.exists(match):\n",
    "                        found_models.append({\n",
    "                            'path': match,\n",
    "                            'priority': priority_group['priority'],\n",
    "                            'description': priority_group['description'],\n",
    "                            'size': os.path.getsize(match) / 1024 / 1024,  # MB\n",
    "                            'modified': os.path.getctime(match)\n",
    "                        })\n",
    "            else:\n",
    "                if os.path.exists(pattern):\n",
    "                    found_models.append({\n",
    "                        'path': pattern,\n",
    "                        'priority': priority_group['priority'],\n",
    "                        'description': priority_group['description'],\n",
    "                        'size': os.path.getsize(pattern) / 1024 / 1024,  # MB\n",
    "                        'modified': os.path.getctime(pattern)\n",
    "                    })\n",
    "    \n",
    "    if found_models:\n",
    "        # ìš°ì„ ìˆœìœ„ì™€ ìˆ˜ì •ì‹œê°„ìœ¼ë¡œ ì •ë ¬\n",
    "        found_models.sort(key=lambda x: (x['priority'], -x['modified']))\n",
    "        return found_models\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ëª¨ë¸ ê²€ìƒ‰ ì‹¤í–‰\n",
    "found_models = find_best_model()\n",
    "\n",
    "if found_models:\n",
    "    print(f\"ğŸ¯ ë°œê²¬ëœ ëª¨ë¸: {len(found_models)}ê°œ\")\n",
    "    \n",
    "    # ìƒìœ„ 3ê°œ ëª¨ë¸ í‘œì‹œ\n",
    "    for i, model_info in enumerate(found_models[:3]):\n",
    "        emoji = \"ğŸ†\" if i == 0 else \"ğŸ¥ˆ\" if i == 1 else \"ğŸ¥‰\"\n",
    "        print(f\"  {emoji} {model_info['description']}\")\n",
    "        print(f\"     ê²½ë¡œ: {model_info['path']}\")\n",
    "        print(f\"     í¬ê¸°: {model_info['size']:.1f}MB\")\n",
    "        print(f\"     ìˆ˜ì •: {datetime.fromtimestamp(model_info['modified']).strftime('%Y-%m-%d %H:%M')}\")\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ëª¨ë¸ ì„ íƒ\n",
    "        if i == 0:\n",
    "            selected_model = model_info['path']\n",
    "    \n",
    "    # ì„ íƒëœ ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦\n",
    "    print(f\"\\nğŸ“¥ ì„ íƒëœ ëª¨ë¸ ë¡œë“œ: {os.path.basename(selected_model)}\")\n",
    "    \n",
    "    try:\n",
    "        temp_model = YOLO(selected_model)\n",
    "        \n",
    "        # ëª¨ë¸ ì •ë³´ í™•ì¸\n",
    "        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "        print(f\"  ğŸ“Š ëª¨ë¸ ì •ë³´:\")\n",
    "        \n",
    "        if hasattr(temp_model, 'model') and hasattr(temp_model.model, 'names'):\n",
    "            class_names = list(temp_model.model.names.values())\n",
    "            print(f\"    ğŸ·ï¸ í´ë˜ìŠ¤ ìˆ˜: {len(class_names)}\")\n",
    "            print(f\"    ğŸ·ï¸ í´ë˜ìŠ¤ëª…: {class_names}\")\n",
    "            \n",
    "            # ì†Œë‚˜ë¬´/í”¼í•´ëª© ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸\n",
    "            pine_related = any(keyword in ' '.join(class_names).lower() \n",
    "                             for keyword in ['pine', 'damage', 'tree', 'í”¼í•´', 'ì†Œë‚˜ë¬´', 'damaged'])\n",
    "            \n",
    "            if pine_related:\n",
    "                print(f\"    ğŸŒ² ì†Œë‚˜ë¬´ í”¼í•´ëª© íƒì§€ìš© ëª¨ë¸ í™•ì¸!\")\n",
    "            else:\n",
    "                print(f\"    âš ï¸ ì¼ë°˜ ê°ì²´ íƒì§€ ëª¨ë¸ (ì „ì´í•™ìŠµ ì˜ˆì •)\")\n",
    "        \n",
    "        # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ë³´\n",
    "        try:\n",
    "            model_yaml = temp_model.model.yaml\n",
    "            if 'backbone' in str(model_yaml):\n",
    "                print(f\"    ğŸ—ï¸ ì•„í‚¤í…ì²˜: YOLOv11s\")\n",
    "        except:\n",
    "            print(f\"    ğŸ—ï¸ ì•„í‚¤í…ì²˜: YOLO ê³„ì—´\")\n",
    "            \n",
    "        existing_model = selected_model\n",
    "        del temp_model  # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        print(f\"ğŸ“¥ ê¸°ë³¸ YOLOv11s ëª¨ë¸ë¡œ ëŒ€ì²´\")\n",
    "        existing_model = 'yolo11s.pt'\n",
    "\n",
    "else:\n",
    "    print(\"âŒ ê¸°ì¡´ í•™ìŠµ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ“¥ ê¸°ë³¸ YOLOv11s ëª¨ë¸ ì‚¬ìš©\")\n",
    "    existing_model = 'yolo11s.pt'\n",
    "\n",
    "# ğŸ¯ ëª¨ë¸ ì‚¬ìš© ê³„íš ì•ˆë‚´\n",
    "print(f\"\\nğŸ¯ ëª¨ë¸ ì‚¬ìš© ê³„íš:\")\n",
    "if 'yolo11s.pt' in existing_model:\n",
    "    print(f\"  ğŸ“ ê¸°ë³¸ YOLOv11s â†’ ì†Œë‚˜ë¬´ í”¼í•´ëª© ì „ì´í•™ìŠµ\")\n",
    "    print(f\"  ğŸ¯ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 1-2ì‹œê°„ (ì²˜ìŒë¶€í„°)\")\n",
    "    print(f\"  ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥: 85-95% mAP50\")\n",
    "elif 'damage_detection' in existing_model:\n",
    "    print(f\"  ğŸ“ ê¸°ì¡´ í”¼í•´ëª© ëª¨ë¸ â†’ ì¶”ê°€ í•™ìŠµ/ë¯¸ì„¸ì¡°ì •\")\n",
    "    print(f\"  ğŸ¯ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 30ë¶„-1ì‹œê°„ (ë¯¸ì„¸ì¡°ì •)\")\n",
    "    print(f\"  ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥: 90-98% mAP50\")\n",
    "else:\n",
    "    print(f\"  ğŸ“ ê¸°ì¡´ ëª¨ë¸ â†’ ì†Œë‚˜ë¬´ í”¼í•´ëª© ì „ì´í•™ìŠµ\")\n",
    "    print(f\"  ğŸ¯ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 1ì‹œê°„ (ì „ì´í•™ìŠµ)\")\n",
    "    print(f\"  ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥: 87-95% mAP50\")\n",
    "\n",
    "# ğŸ—‚ï¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "if IN_COLAB:\n",
    "    output_base = '/content/drive/MyDrive/pinetree_scan/results'\n",
    "else:\n",
    "    output_base = './results'\n",
    "\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "print(f\"\\nğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {output_base}\")\n",
    "\n",
    "# ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸\n",
    "data_yaml_path = f\"{data_path}/data.yaml\"\n",
    "if os.path.exists(data_yaml_path):\n",
    "    print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:\")\n",
    "    try:\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(f\"  ğŸ“„ data.yaml ë‚´ìš© í™•ì¸ë¨\")\n",
    "            if 'names:' in content:\n",
    "                print(f\"  ğŸ·ï¸ í´ë˜ìŠ¤ ì •ë³´ í¬í•¨ë¨\")\n",
    "    except:\n",
    "        print(f\"  âš ï¸ data.yaml ì½ê¸° ì‹¤íŒ¨\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ë°ì´í„°ì…‹ íŒŒì¼ ì—†ìŒ: {data_yaml_path}\")\n",
    "    print(\"ğŸ“‹ data.yaml íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤!\")\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "globals()['existing_model'] = existing_model\n",
    "globals()['data_path'] = data_path\n",
    "globals()['output_base'] = output_base\n",
    "globals()['IN_COLAB'] = IN_COLAB\n",
    "globals()['found_models'] = found_models\n",
    "\n",
    "print(f\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"  ğŸ† ì‚¬ìš©í•  ëª¨ë¸: {os.path.basename(existing_model)}\")\n",
    "print(f\"  ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {existing_model}\")\n",
    "print(f\"  ğŸ“Š ë°ì´í„° ê²½ë¡œ: {data_path}\")\n",
    "print(f\"  ğŸ“ ê²°ê³¼ ê²½ë¡œ: {output_base}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… 1ë‹¨ê³„ ì™„ë£Œ! í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ íƒì§€ ì™„ë£Œ\")\n",
    "print(f\"ğŸ“¦ ì‚¬ìš©í•  ëª¨ë¸: {existing_model}\")  \n",
    "print(f\"ï¿½ ë°ì´í„° ê²½ë¡œ: {data_path}\")\n",
    "print(f\"ï¿½ í™˜ê²½: {'Google Colab' if IN_COLAB else 'ë¡œì»¬'}\")\n",
    "print(\"ğŸ”„ ë‹¤ìŒ ë‹¨ê³„: 2ë‹¨ê³„ ë©€í‹°ìŠ¤ì¼€ì¼ í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3021b",
   "metadata": {
    "id": "e7b3021b"
   },
   "source": [
    "## ğŸš€ **2ë‹¨ê³„: YOLOv11s ë©€í‹°ìŠ¤ì¼€ì¼ í•™ìŠµ + Google Drive ìë™ ë°±ì—…**\n",
    "\n",
    "> **ë©€í‹°ìŠ¤ì¼€ì¼ ìµœì í™”ë¡œ mAP 0.65+ ëª©í‘œ ë‹¬ì„±**  \n",
    "> Tesla T4 GPU ìµœì í™” ì„¤ì •ìœ¼ë¡œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ í”¼í•´ëª©ì„ íš¨ê³¼ì ìœ¼ë¡œ íƒì§€í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.  \n",
    "> **ìë™ ë°±ì—…**: í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë“  ê²°ê³¼ë¥¼ Google Driveì— ìë™ ì €ì¥  \n",
    "> **ë„¤ì´ë°**: `damage_detection_YYYYMMDD_HHMM` í˜•ì‹ìœ¼ë¡œ ìë™ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24c8d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 208343,
     "status": "error",
     "timestamp": 1758779886897,
     "user": {
      "displayName": "ë¦¬ì§€ëŒ",
      "userId": "13174544160225006253"
     },
     "user_tz": -540
    },
    "id": "bf24c8d1",
    "outputId": "e9ae1d15-9e00-47cb-b57b-04e03672fa9a"
   },
   "outputs": [],
   "source": [
    "# ğŸš€ 2ë‹¨ê³„: YOLOv11s Multi-Scale í•™ìŠµ ì¤€ë¹„ + ì‹¤í–‰ (T4 GPU ìµœì í™”)\n",
    "print(\"ğŸŒ² YOLOv11s Multi-Scale ì†Œë‚˜ë¬´ í”¼í•´ëª© í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"ğŸ¯ T4 GPU ìµœì í™” + Multi-Scale Detection + ì˜¤íƒì§€ ê°œì„  + ë™ì  ë°”ìš´ë”©ë°•ìŠ¤\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ğŸ“š í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¬ì„í¬íŠ¸ (ì…€ ë…ë¦½ ì‹¤í–‰ ì§€ì›)\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "\n",
    "# ğŸ§¹ ë©”ëª¨ë¦¬ ì‚¬ì „ ì •ë¦¬ (ë¬´í•œë¡œë”© ë°©ì§€)\n",
    "import gc\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "# ğŸ” 1ë‹¨ê³„ ì—°ê²° í™•ì¸ (ì•ˆì „í•œ ê¸°ë³¸ê°’)\n",
    "existing_model = globals().get('existing_model', 'yolo11s.pt')\n",
    "data_path = globals().get('data_path', '/content/drive/MyDrive/pinetree_scan/training_data')\n",
    "\n",
    "print(f\"ğŸ“¦ ì‚¬ìš©í•  ëª¨ë¸: {existing_model}\")\n",
    "print(f\"ğŸ“‚ ë°ì´í„° ê¸°ë³¸ ê²½ë¡œ: {data_path}\")\n",
    "\n",
    "# ğŸ“Š 1ë‹¨ê³„ì—ì„œ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ í™•ì¸ ë° ì ˆëŒ€ ê²½ë¡œ ìˆ˜ì •\n",
    "print(\"\\nğŸ” 1ë‹¨ê³„ ë°ì´í„°ì…‹ í™•ì¸ ë° ì ˆëŒ€ ê²½ë¡œ ìˆ˜ì • ì¤‘...\")\n",
    "\n",
    "# ğŸ¯ 1ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ê²½ë¡œ\n",
    "original_yaml_path = '/content/drive/MyDrive/pinetree_scan/training_data/data.yaml'\n",
    "data_base_dir = '/content/drive/MyDrive/pinetree_scan/training_data'\n",
    "\n",
    "print(f\"ğŸ“‚ ì›ë³¸ data.yaml: {original_yaml_path}\")\n",
    "print(f\"ğŸ“‚ ë°ì´í„° ê¸°ë³¸ ê²½ë¡œ: {data_base_dir}\")\n",
    "\n",
    "if not os.path.exists(original_yaml_path):\n",
    "    print(\"âŒ 1ë‹¨ê³„ data.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"ğŸ“ í™•ì¸í•œ ê²½ë¡œ: {original_yaml_path}\")\n",
    "    print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "    print(\"1. 1ë‹¨ê³„ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”\")\n",
    "    print(\"2. Google Drive ë§ˆìš´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”\")\n",
    "    raise FileNotFoundError(\"1ë‹¨ê³„ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "# ğŸ“– ì›ë³¸ data.yaml ì½ê¸°\n",
    "try:\n",
    "    with open(original_yaml_path, 'r', encoding='utf-8') as f:\n",
    "        original_data = yaml.safe_load(f)\n",
    "        \n",
    "    print(f\"ğŸ“Š ì›ë³¸ data.yaml ë‚´ìš©:\")\n",
    "    print(f\"  ğŸ·ï¸ í´ë˜ìŠ¤: {original_data.get('names', ['ì •ë³´ ì—†ìŒ'])}\")\n",
    "    print(f\"  ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {original_data.get('nc', 'ì •ë³´ ì—†ìŒ')}\")\n",
    "    print(f\"  ğŸ“ ê²½ë¡œ: {original_data.get('path', 'ì •ë³´ ì—†ìŒ')}\")\n",
    "    print(f\"  ğŸ¯ í›ˆë ¨: {original_data.get('train', 'ì •ë³´ ì—†ìŒ')}\")\n",
    "    print(f\"  âœ… ê²€ì¦: {original_data.get('val', 'ì •ë³´ ì—†ìŒ')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì›ë³¸ data.yaml ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "    raise e\n",
    "\n",
    "# ğŸ”§ ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •ëœ data.yaml ìƒì„±\n",
    "print(f\"\\nğŸ”§ ì ˆëŒ€ ê²½ë¡œ data.yaml ìƒì„± ì¤‘...\")\n",
    "\n",
    "# ì‹¤ì œ í´ë” ì¡´ì¬ í™•ì¸\n",
    "train_images_path = os.path.join(data_base_dir, 'train', 'images')\n",
    "val_images_path = os.path.join(data_base_dir, 'val', 'images')\n",
    "train_labels_path = os.path.join(data_base_dir, 'train', 'labels')\n",
    "val_labels_path = os.path.join(data_base_dir, 'val', 'labels')\n",
    "\n",
    "print(f\"ğŸ“‚ í™•ì¸í•  ê²½ë¡œë“¤:\")\n",
    "print(f\"  ğŸ“ˆ í›ˆë ¨ ì´ë¯¸ì§€: {train_images_path}\")\n",
    "print(f\"  ğŸ“Š ê²€ì¦ ì´ë¯¸ì§€: {val_images_path}\")\n",
    "print(f\"  ğŸ·ï¸ í›ˆë ¨ ë¼ë²¨: {train_labels_path}\")\n",
    "print(f\"  ğŸ·ï¸ ê²€ì¦ ë¼ë²¨: {val_labels_path}\")\n",
    "\n",
    "# í•„ìˆ˜ í´ë” í™•ì¸\n",
    "missing_folders = []\n",
    "if not os.path.exists(train_images_path):\n",
    "    missing_folders.append(\"train/images\")\n",
    "if not os.path.exists(val_images_path):\n",
    "    missing_folders.append(\"val/images\")\n",
    "if not os.path.exists(train_labels_path):\n",
    "    missing_folders.append(\"train/labels\")\n",
    "if not os.path.exists(val_labels_path):\n",
    "    missing_folders.append(\"val/labels\")\n",
    "\n",
    "if missing_folders:\n",
    "    print(f\"âŒ í•„ìˆ˜ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_folders)}\")\n",
    "    print(\"\\nğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "    print(\"1. 1ë‹¨ê³„ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "    print(\"2. ZIP íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ ì••ì¶•í•´ì œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\")\n",
    "    raise FileNotFoundError(f\"í•„ìˆ˜ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_folders)}\")\n",
    "\n",
    "print(\"âœ… ëª¨ë“  í•„ìˆ˜ í´ë” í™•ì¸ ì™„ë£Œ!\")\n",
    "\n",
    "# ğŸ“Š ì‹¤ì œ íŒŒì¼ ìˆ˜ í™•ì¸\n",
    "train_images = [f for f in os.listdir(train_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff'))]\n",
    "val_images = [f for f in os.listdir(val_images_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff'))]\n",
    "train_labels = [f for f in os.listdir(train_labels_path) if f.endswith('.txt')]\n",
    "val_labels = [f for f in os.listdir(val_labels_path) if f.endswith('.txt')]\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì‹¤ì œ íŒŒì¼ ìˆ˜:\")\n",
    "print(f\"  ğŸ–¼ï¸ í›ˆë ¨ ì´ë¯¸ì§€: {len(train_images)}ê°œ\")\n",
    "print(f\"  ğŸ–¼ï¸ ê²€ì¦ ì´ë¯¸ì§€: {len(val_images)}ê°œ\")\n",
    "print(f\"  ğŸ·ï¸ í›ˆë ¨ ë¼ë²¨: {len(train_labels)}ê°œ\")\n",
    "print(f\"  ğŸ·ï¸ ê²€ì¦ ë¼ë²¨: {len(val_labels)}ê°œ\")\n",
    "\n",
    "# ğŸ†• ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •ëœ data.yaml ìƒì„±\n",
    "fixed_data = {\n",
    "    'path': data_base_dir,  # ì ˆëŒ€ ê²½ë¡œ ë£¨íŠ¸\n",
    "    'train': 'train/images',  # ìƒëŒ€ ê²½ë¡œ\n",
    "    'val': 'val/images',     # ìƒëŒ€ ê²½ë¡œ\n",
    "    'nc': original_data.get('nc', 1),\n",
    "    'names': original_data.get('names', ['damaged_tree'])\n",
    "}\n",
    "\n",
    "# ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥\n",
    "fixed_yaml_path = '/tmp/fixed_training_data.yaml'\n",
    "with open(fixed_yaml_path, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(fixed_data, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"ğŸ”§ ìˆ˜ì •ëœ data.yaml ìƒì„±: {fixed_yaml_path}\")\n",
    "print(f\"ğŸ“Š ìˆ˜ì •ëœ ë‚´ìš©:\")\n",
    "print(f\"  ğŸ“ ë£¨íŠ¸ ê²½ë¡œ: {fixed_data['path']}\")\n",
    "print(f\"  ğŸ¯ í›ˆë ¨: {fixed_data['train']}\")\n",
    "print(f\"  âœ… ê²€ì¦: {fixed_data['val']}\")\n",
    "print(f\"  ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {fixed_data['nc']}\")\n",
    "print(f\"  ğŸ·ï¸ í´ë˜ìŠ¤ëª…: {fixed_data['names']}\")\n",
    "\n",
    "# ìˆ˜ì •ëœ yaml ì‚¬ìš©\n",
    "data_yaml_path = fixed_yaml_path\n",
    "\n",
    "# ğŸ¤– 1ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©\n",
    "print(f\"\\nğŸ¤– 1ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "print(f\"ğŸ“‚ 1ë‹¨ê³„ ëª¨ë¸: {existing_model}\")\n",
    "\n",
    "# 1ë‹¨ê³„ì—ì„œ ì„¤ì •í•œ ëª¨ë¸ ì‚¬ìš©\n",
    "model_path = existing_model\n",
    "\n",
    "try:\n",
    "    # ğŸ”’ ê¸°ì¡´ ëª¨ë¸ë§Œ ì‚¬ìš© (ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ë°©ì§€)\n",
    "    model = YOLO(model_path, task='detect')\n",
    "    print(f\"âœ… ê¸°ì¡´ YOLOv11s ëª¨ë¸ ë¡œë“œ ì„±ê³µ! (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)\")\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ í‘œì‹œ\n",
    "    model_info = model.info(detailed=False)\n",
    "    print(f\"ğŸš« ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ë°©ì§€: AMP ì²´í¬ ì‹œ YOLOv11n ë‹¤ìš´ë¡œë“œ ì°¨ë‹¨ë¨\")\n",
    "    \n",
    "    # ëª¨ë¸ íƒ€ì… í™•ì¸\n",
    "    if 'best.pt' in model_path:\n",
    "        print(f\"ğŸ“Š ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ì •ë³´:\")\n",
    "        print(f\"  ğŸ¯ ëª¨ë¸: ê¸°ì¡´ í•™ìŠµëœ YOLO ëª¨ë¸ (ì „ì´í•™ìŠµ)\")\n",
    "        print(f\"  ğŸ’¡ ì¥ì : ì´ë¯¸ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¡œ ë¹ ë¥¸ ìˆ˜ë ´\")\n",
    "        print(f\"  ğŸ”„ ì „ëµ: Fine-tuningìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\")\n",
    "    else:\n",
    "        print(f\"ğŸ“Š ê¸°ë³¸ YOLOv11s ëª¨ë¸ ì •ë³´:\")\n",
    "        print(f\"  ğŸ”¢ íŒŒë¼ë¯¸í„°: ~9.4M (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\")\n",
    "        print(f\"  ğŸ¯ mAP ëª©í‘œ: 65%+ (ì•ˆì •ì  ì„±ëŠ¥)\")\n",
    "        print(f\"  ğŸ’¾ ëª¨ë¸ í¬ê¸°: ~18MB\")\n",
    "        \n",
    "    print(f\"  ğŸ“ ëª¨ë¸ ê²½ë¡œ: {model_path}\")\n",
    "    print(f\"  ğŸ’¡ 1ë‹¨ê³„ ì—°ë™: ê¸°ì¡´ ì„¤ì • í™œìš©\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(f\"ğŸ’¡ YOLOv11s ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´...\")\n",
    "    model_path = 'yolo11s.pt'\n",
    "    model = YOLO(model_path)  # ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´\n",
    "    print(f\"âœ… YOLOv11s ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ğŸ”¥ T4 GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nğŸ–¥ï¸ ì»´í“¨íŒ… ì¥ì¹˜: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    # T4 GPU ë©”ëª¨ë¦¬ ì •ë³´\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(f\"  ğŸš€ GPU: {gpu_name}\")\n",
    "    print(f\"  ğŸ’¾ ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ í¬ê¸° (YOLOv11s + T4 ìµœì í™”)\n",
    "    if 'T4' in gpu_name:\n",
    "        batch_size = 4   # T4 ë©”ëª¨ë¦¬ ì•ˆì •ì„± ìš°ì„  (8â†’4)\n",
    "    else:\n",
    "        batch_size = 6   # ë‹¤ë¥¸ GPUì—ì„œë„ ë³´ìˆ˜ì \n",
    "        \n",
    "    print(f\"ğŸ“¦ ë©”ëª¨ë¦¬ ì•ˆì • ë°°ì¹˜ í¬ê¸°: {batch_size} (ê¸°ì¡´ 8â†’4ë¡œ ê°ì†Œ)\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ìµœì í™” ì„¤ì •\n",
    "    torch.backends.cudnn.benchmark = True   # ì„±ëŠ¥ í–¥ìƒ\n",
    "    if hasattr(torch.backends.cudnn, 'allow_tf32'):\n",
    "        torch.backends.cudnn.allow_tf32 = True  # T4 Tensor Core í™œìš©\n",
    "    \n",
    "    if hasattr(torch, 'set_float32_matmul_precision'):\n",
    "        torch.set_float32_matmul_precision('medium')  # ì†ë„ vs ì •ë°€ë„ ê· í˜•\n",
    "        \n",
    "    # Multi-Scale ìµœì í™” \n",
    "    torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ ìš°ì„ \n",
    "    \n",
    "else:\n",
    "    batch_size = 2  # CPU ëª¨ë“œ\n",
    "    print(f\"ğŸ“¦ CPU ëª¨ë“œ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "\n",
    "# ================================\n",
    "# ğŸ¯ mAP 65% ë‹¬ì„± ìµœì í™” íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\n",
    "# ================================\n",
    "# ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì¦ê°• ì„¤ì •ì´ ì•„ë˜ model.train() í•¨ìˆ˜ì— ì§ì ‘ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "# ì¬í˜„ìœ¨ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ì„¤ì •:\n",
    "# - í•™ìŠµë¥ : 0.008 (ì ê·¹ì  í•™ìŠµ)\n",
    "# - ë°•ìŠ¤ ì†ì‹¤: 10.0 (ìœ„ì¹˜ ì •í™•ë„)\n",
    "# - Mosaic: 1.0 + Copy-Paste: 0.5 (ì‘ì€ ê°ì²´ ê°•í™”)\n",
    "# - Patience: 80 (ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„)\n",
    "print(f\"\\nğŸ¯ ì¬í˜„ìœ¨ ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ëª©í‘œ: mAP 42.3% â†’ 65%+ (22.7% í–¥ìƒ)\")\n",
    "\n",
    "print(f\"\\nğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  YOLOv11s í•™ìŠµ ì„¤ì •:\")\n",
    "print(f\"  ğŸ¤– ëª¨ë¸: YOLOv11s (9.4M íŒŒë¼ë¯¸í„°) - ë©”ëª¨ë¦¬ íš¨ìœ¨ì \")\n",
    "print(f\"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size} (ë©”ëª¨ë¦¬ ì•ˆì •ì„± ìš°ì„ )\")\n",
    "print(f\"  ğŸ”„ ì—í­: 200 (Early Stopping: 50 patience)\")\n",
    "print(f\"  ğŸ’¾ ì¥ì¹˜: {device}\")\n",
    "print(f\"  ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: 640px\")\n",
    "print(f\"  ğŸ“‚ ìˆ˜ì •ëœ ë°ì´í„°ì…‹: {data_yaml_path}\")\n",
    "print(f\"  ğŸ“Š í›ˆë ¨ ì´ë¯¸ì§€: {len(train_images)}ê°œ\")\n",
    "print(f\"  ğŸ“Š ê²€ì¦ ì´ë¯¸ì§€: {len(val_images)}ê°œ\")\n",
    "print(f\"\")\n",
    "print(f\"\udd27 ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ:\")\n",
    "print(f\"  ğŸ“‰ ë°°ì¹˜ í¬ê¸° ìµœì í™”: 8 â†’ {batch_size} (T4 GPU ì•ˆì •ì„±)\")\n",
    "print(f\"  ğŸ“ˆ í•™ìŠµë¥  ìµœì í™”: 0.005 â†’ 0.008 (ì ê·¹ì  í•™ìŠµ)\")\n",
    "print(f\"  ğŸ¯ ì†ì‹¤ ê°€ì¤‘ì¹˜ ê°•í™”: box=10.0, cls=1.0, dfl=2.0\")\n",
    "print(f\"  ğŸ¨ ë°ì´í„° ì¦ê°• ê·¹ëŒ€í™”:\")\n",
    "print(f\"    - Mosaic: 1.0 (ìµœëŒ€ ë‹¤ì–‘ì„±)\")\n",
    "print(f\"    - Copy-Paste: 0.5 (ì‘ì€ ê°ì²´ ë³µì œ)\")\n",
    "print(f\"    - íšŒì „ ê°ë„: 15Â° (ë‹¤ì–‘í•œ ê°ë„)\")\n",
    "print(f\"    - í¬ê¸° ë³€í™”: 0.8 (ê·¹í•œ í¬ê¸° ë³€í™”)\")\n",
    "print(f\"  â° ì¶©ë¶„í•œ í•™ìŠµ: 80 ì—í­ patience (ì¬í˜„ìœ¨ ì§‘ì¤‘)\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ¯ mAP 65% ë‹¬ì„±ì„ ìœ„í•œ ì¬í˜„ìœ¨ ì§‘ì¤‘ ìµœì í™”:\")\n",
    "print(f\"  â€¢ ğŸ¯ mAP@0.5: 42.3% â†’ 65%+ (22.7% ëŒ€í­ í–¥ìƒ ëª©í‘œ)\")\n",
    "print(f\"  â€¢ ğŸ”„ ì¬í˜„ìœ¨: 45.3% â†’ 65%+ (ë†“ì¹œ í”¼í•´ëª© 20% ê°ì†Œ)\")  \n",
    "print(f\"  â€¢ ğŸ¯ ì •ë°€ë„: 56.4% â†’ 70%+ (ì˜¤íƒì§€ ì¶”ê°€ ê°ì†Œ)\")\n",
    "print(f\"  â€¢ âš¡ í•µì‹¬ ì „ëµ: ì‘ì€ ê°ì²´ íƒì§€ ê°•í™”\")\n",
    "print(f\"  â€¢ ğŸ¨ ì¦ê°• ê°•í™”: ë‹¤ì–‘í•œ ë³€í˜•ìœ¼ë¡œ ê²¬ê³ í•œ í•™ìŠµ\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ” ì¬í˜„ìœ¨ í–¥ìƒ í•µì‹¬ ìµœì í™”:\")\n",
    "print(f\"  â€¢ ğŸ¨ Mosaic 1.0 + Copy-Paste 0.5 (ì‘ì€ ê°ì²´ ë³µì œ)\")\n",
    "print(f\"  â€¢ ğŸ“ˆ í•™ìŠµë¥  0.008 (ì ê·¹ì  í•™ìŠµ)\")\n",
    "print(f\"  â€¢ ğŸ¯ ë°•ìŠ¤ ì†ì‹¤ 10.0 (ìœ„ì¹˜ ì •í™•ë„)\")\n",
    "print(f\"  â€¢ â° Patience 80 (ì¶©ë¶„í•œ í•™ìŠµ)\")\n",
    "print(f\"  â€¢ ğŸ”„ Close Mosaic 15 (ë” ì˜¤ë˜ ì¦ê°•)\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ’¡ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 1.5-2.5ì‹œê°„ (ê³ ì„±ëŠ¥)\")\n",
    "print(f\"ğŸš€ ëª©í‘œ mAP 65% ë‹¬ì„±ìœ¼ë¡œ ì‹¤ìš©ì  ì„±ëŠ¥ í™•ë³´!\")\n",
    "\n",
    "# ğŸ¯ mAP 65% ë‹¬ì„± ì¬í˜„ìœ¨ ìµœì í™” í•™ìŠµ ì‹¤í–‰\n",
    "print(f\"\\nğŸš€ ì¬í˜„ìœ¨ ì§‘ì¤‘ YOLOv11s ê³ ì„±ëŠ¥ í•™ìŠµ ì‹œì‘!\")\n",
    "print(f\"ğŸ¯ ëª©í‘œ: mAP 42.3% â†’ 65%+ (22.7% ëŒ€í­ í–¥ìƒ)\")\n",
    "print(f\"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: 640x640 (ê³ ì •)\")\n",
    "print(f\"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\")\n",
    "print(f\"ğŸ” ì¬í˜„ìœ¨ í–¥ìƒ í•µì‹¬ ì „ëµ:\")\n",
    "print(f\"  1ï¸âƒ£ ê°•í™”ëœ ë°ì´í„° ì¦ê°• (Mosaic 1.0, Copy-Paste 0.5)\")\n",
    "print(f\"  2ï¸âƒ£ ë†’ì€ í•™ìŠµë¥  (0.008) + ê¸´ ì›Œë°ì—… (15 ì—í­)\")\n",
    "print(f\"  3ï¸âƒ£ ë°•ìŠ¤ ì†ì‹¤ ê°•í™” (10.0) + í´ë˜ìŠ¤ ì†ì‹¤ (1.0)\")\n",
    "print(f\"  4ï¸âƒ£ ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ (Patience 80)\")\n",
    "print(f\"  5ï¸âƒ£ ë“œë¡­ì•„ì›ƒ 0.1ë¡œ ì¼ë°˜í™” í–¥ìƒ\")\n",
    "print(f\"\")\n",
    "\n",
    "try:\n",
    "    # ğŸ¯ mAP 65% ë‹¬ì„±ì„ ìœ„í•œ ê³ ì„±ëŠ¥ í•™ìŠµ ì‹¤í–‰\n",
    "    results = model.train(\n",
    "        # ================================\n",
    "        # ğŸ“‚ ê¸°ë³¸ ì„¤ì •\n",
    "        # ================================\n",
    "        data=data_yaml_path,              # ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "        epochs=200,                       # í•™ìŠµ ì—í­ ìˆ˜\n",
    "        batch=batch_size,                 # ë°°ì¹˜ í¬ê¸° (T4 GPU ìµœì í™”)\n",
    "        imgsz=640,                        # ì´ë¯¸ì§€ í¬ê¸° (640x640 ê³ ì •)\n",
    "        device=device,                    # GPU/CPU ì„¤ì •\n",
    "        \n",
    "        # ================================\n",
    "        # ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§ ë° ì €ì¥\n",
    "        # ================================\n",
    "        patience=80,                      # Early Stopping (ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„)\n",
    "        save=True,                        # ëª¨ë¸ ì €ì¥ í™œì„±í™”\n",
    "        save_period=10,                   # 10 ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸\n",
    "        project='runs/detect',            # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "        name=f'pinetree_optimized_{datetime.now().strftime(\"%Y%m%d_%H%M\")}',\n",
    "        exist_ok=True,                    # ê¸°ì¡´ í´ë” ë®ì–´ì“°ê¸°\n",
    "        \n",
    "        # ================================\n",
    "        # ğŸ“ˆ ì¬í˜„ìœ¨ í–¥ìƒ í•™ìŠµë¥  ì„¤ì •\n",
    "        # ================================\n",
    "        lr0=0.008,                        # ì´ˆê¸° í•™ìŠµë¥  (ì ê·¹ì  í•™ìŠµ)\n",
    "        lrf=0.0001,                       # ìµœì¢… í•™ìŠµë¥  (ì„¸ë°€í•œ ì¡°ì •)\n",
    "        momentum=0.95,                    # ëª¨ë©˜í…€ (ì•ˆì •ì  ìˆ˜ë ´)\n",
    "        weight_decay=0.0003,              # ê°€ì¤‘ì¹˜ ê°ì‡  (ê³¼ì í•© ë°©ì§€)\n",
    "        \n",
    "        # ================================\n",
    "        # ğŸ”¥ ì›Œë°ì—… ìµœì í™”\n",
    "        # ================================\n",
    "        warmup_epochs=15.0,               # ì›Œë°ì—… ì—í­ (ì•ˆì •ì  ì‹œì‘)\n",
    "        warmup_momentum=0.9,              # ì›Œë°ì—… ëª¨ë©˜í…€\n",
    "        warmup_bias_lr=0.01,              # ì›Œë°ì—… ë°”ì´ì–´ìŠ¤ í•™ìŠµë¥ \n",
    "        \n",
    "        # ================================\n",
    "        # ğŸ¯ ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜ (ì¬í˜„ìœ¨ ìµœì í™”)\n",
    "        # ================================\n",
    "        box=10.0,                         # ë°•ìŠ¤ ì†ì‹¤ (ìœ„ì¹˜ ì •í™•ë„ ê°•í™”)\n",
    "        cls=1.0,                          # í´ë˜ìŠ¤ ì†ì‹¤ (ë¶„ë¥˜ ì •í™•ë„)\n",
    "        dfl=2.0,                          # DFL ì†ì‹¤ (ì„¸ë°€í•œ ê²½ê³„ í•™ìŠµ)\n",
    "        \n",
    "        # ================================\n",
    "        # ğŸŒˆ ìƒ‰ìƒ ì¦ê°• (ë‹¤ì–‘í•œ ì¡°ê±´ ëŒ€ì‘)\n",
    "        # ================================\n",
    "        hsv_h=0.025,                      # ìƒ‰ì¡° ë³€í™” (ê³„ì ˆë³„ ë³€í™”)\n",
    "        hsv_s=0.8,                        # ì±„ë„ ë³€í™” (ë°ê¸° ë³€í™”)\n",
    "        hsv_v=0.5,                        # ëª…ë„ ë³€í™” (ì¡°ëª… ë³€í™”)\n",
    "        \n",
    "        # ================================\n",
    "        # ğŸ¨ ì¬í˜„ìœ¨ í–¥ìƒ ë°ì´í„° ì¦ê°•\n",
    "        # ================================\n",
    "        mosaic=1.0,                       # Mosaic ìµœëŒ€í™” (ë‹¤ì–‘í•œ ì¡°í•©)\n",
    "        mixup=0.3,                        # MixUp (ê²½ê³„ í•™ìŠµ ê°•í™”)\n",
    "        copy_paste=0.5,                   # Copy-Paste (ì‘ì€ ê°ì²´ ë³µì œ)\n",
    "        \n",
    "        # ================================\n",
    "        # ğŸ”„ ê¸°í•˜í•™ì  ë³€í™˜ (ë‹¤ì–‘í•œ ìƒí™© ëŒ€ì‘)\n",
    "        # ================================\n",
    "        degrees=15.0,                     # íšŒì „ ê°ë„ (ë‹¤ì–‘í•œ ê°ë„)\n",
    "        translate=0.2,                    # ì´ë™ ë²”ìœ„ (ìœ„ì¹˜ ë³€í™”)\n",
    "        scale=0.8,                        # í¬ê¸° ë³€í™” (ê·¹í•œ í¬ê¸°)\n",
    "        shear=3.0,                        # ì „ë‹¨ ë³€í˜• (ë³€í˜•ëœ í˜•íƒœ)\n",
    "        perspective=0.0005,               # ì›ê·¼ ë³€í™˜ (ê±°ë¦¬ê°)\n",
    "        flipud=0.0,                       # ìƒí•˜ ë°˜ì „ ë¹„í™œì„±í™”\n",
    "        fliplr=0.7,                       # ì¢Œìš° ë°˜ì „ (ëŒ€ì¹­ì„±)\n",
    "        erasing=0.2,                      # ëœë¤ ì§€ìš°ê¸° (ì •ë³´ ë³´ì¡´)\n",
    "        \n",
    "        # ================================\n",
    "        # ğŸ§  ì •ê·œí™” ë° ìµœì í™”\n",
    "        # ================================\n",
    "        dropout=0.1,                      # ë“œë¡­ì•„ì›ƒ (ì¼ë°˜í™” í–¥ìƒ)\n",
    "        nbs=64,                           # ì •ê·œí™” ë°°ì¹˜ í¬ê¸°\n",
    "        overlap_mask=True,                # ê²¹ì¹¨ ë§ˆìŠ¤í¬ (ì¤‘ë³µ ë°©ì§€)\n",
    "        mask_ratio=4,                     # ë§ˆìŠ¤í¬ ë¹„ìœ¨ (ì„¸ë°€í•œ íƒì§€)\n",
    "        \n",
    "        # ================================\n",
    "        # âš™ï¸ ì‹œìŠ¤í…œ ìµœì í™”\n",
    "        # ================================\n",
    "        workers=2,                        # ë°ì´í„° ë¡œë” ì›Œì»¤ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        verbose=True,                     # ìƒì„¸ ë¡œê·¸\n",
    "        seed=42,                          # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼\n",
    "        deterministic=False,              # ì„±ëŠ¥ ìš°ì„ \n",
    "        single_cls=True,                  # ë‹¨ì¼ í´ë˜ìŠ¤ (ì†Œë‚˜ë¬´ë§Œ)\n",
    "        rect=False,                       # ë‹¤ì–‘í•œ ë¹„ìœ¨ í•™ìŠµ\n",
    "        cos_lr=True,                      # ì½”ì‚¬ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "        close_mosaic=15,                  # Mosaic ë¹„í™œì„±í™” ì‹œì  (ë” ì˜¤ë˜ ì¦ê°•)\n",
    "        resume=False,                     # ìƒˆë¡œ ì‹œì‘\n",
    "        amp=False,                        # AMP ë¹„í™œì„±í™” (YOLOv11n ë‹¤ìš´ë¡œë“œ ë°©ì§€)\n",
    "        fraction=1.0,                     # ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©\n",
    "        profile=False,                    # í”„ë¡œíŒŒì¼ë§ ë¹„í™œì„±í™”\n",
    "        cache=False,                      # ìºì‹œ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… í¬ê¸°ë³„ ì†Œë‚˜ë¬´ íƒì§€ YOLOv11s í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸŒ² ë‹¤ì–‘í•œ í¬ê¸° ì†Œë‚˜ë¬´ íƒì§€ ëª¨ë¸ í•™ìŠµ ì„±ê³µ!\")\n",
    "    print(f\"â° ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # ğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½\n",
    "    if results:\n",
    "        print(f\"\\nğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "        \n",
    "        # ìµœì¢… ë©”íŠ¸ë¦­ ì¶”ì¶œ\n",
    "        final_metrics = results.results_dict if hasattr(results, 'results_dict') else {}\n",
    "        \n",
    "        if final_metrics:\n",
    "            map50 = final_metrics.get('metrics/mAP50(B)', 'N/A')\n",
    "            map50_95 = final_metrics.get('metrics/mAP50-95(B)', 'N/A') \n",
    "            precision = final_metrics.get('metrics/precision(B)', 'N/A')\n",
    "            recall = final_metrics.get('metrics/recall(B)', 'N/A')\n",
    "            \n",
    "            print(f\"  ğŸ“ˆ mAP@0.5: {map50}\")\n",
    "            print(f\"  ğŸ“ˆ mAP@0.5:0.95: {map50_95}\")\n",
    "            print(f\"  ğŸ¯ ì •ë°€ë„: {precision}\")\n",
    "            print(f\"  ğŸ”„ ì¬í˜„ìœ¨: {recall}\")\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ í™•ì¸\n",
    "        save_dir = results.save_dir if hasattr(results, 'save_dir') else 'runs/detect/train'\n",
    "        best_model = os.path.join(save_dir, 'weights', 'best.pt')\n",
    "        last_model = os.path.join(save_dir, 'weights', 'last.pt')\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜:\")\n",
    "        print(f\"  ğŸ¥‡ ìµœê³  ëª¨ë¸: {best_model}\")\n",
    "        print(f\"  ğŸ“ ìµœì¢… ëª¨ë¸: {last_model}\")\n",
    "        \n",
    "        # ğŸš€ Google Drive ì „ì²´ ë”¥ëŸ¬ë‹ ê²°ê³¼ ë°±ì—…\n",
    "        try:\n",
    "            print(f\"\\nâ˜ï¸ Google Drive ì „ì²´ ê²°ê³¼ ë°±ì—… ì‹œì‘...\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # ğŸ—‚ï¸ ë°±ì—… ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n",
    "            main_backup_dir = '/content/drive/MyDrive/pinetree_scan/training_results'\n",
    "            session_backup_dir = os.path.join(main_backup_dir, f'training_session_{timestamp}')\n",
    "            models_backup_dir = os.path.join(session_backup_dir, 'models')\n",
    "            results_backup_dir = os.path.join(session_backup_dir, 'results')\n",
    "            logs_backup_dir = os.path.join(session_backup_dir, 'logs')\n",
    "            \n",
    "            os.makedirs(models_backup_dir, exist_ok=True)\n",
    "            os.makedirs(results_backup_dir, exist_ok=True) \n",
    "            os.makedirs(logs_backup_dir, exist_ok=True)\n",
    "            \n",
    "            print(f\"ğŸ“ ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±: {session_backup_dir}\")\n",
    "            \n",
    "            # ğŸ¤– ëª¨ë¸ íŒŒì¼ ë°±ì—…\n",
    "            model_backup_count = 0\n",
    "            if os.path.exists(best_model):\n",
    "                backup_best = os.path.join(models_backup_dir, f'best_model_{timestamp}.pt')\n",
    "                shutil.copy2(best_model, backup_best)\n",
    "                print(f\"  ğŸ¥‡ ìµœê³  ëª¨ë¸ ë°±ì—…: {os.path.basename(backup_best)}\")\n",
    "                model_backup_count += 1\n",
    "            \n",
    "            if os.path.exists(last_model):\n",
    "                backup_last = os.path.join(models_backup_dir, f'last_model_{timestamp}.pt')\n",
    "                shutil.copy2(last_model, backup_last)\n",
    "                print(f\"  ğŸ“ ìµœì¢… ëª¨ë¸ ë°±ì—…: {os.path.basename(backup_last)}\")\n",
    "                model_backup_count += 1\n",
    "            \n",
    "            # ğŸ“Š í•™ìŠµ ê²°ê³¼ íŒŒì¼ë“¤ ë°±ì—…\n",
    "            results_backup_count = 0\n",
    "            if os.path.exists(save_dir):\n",
    "                # ê²°ê³¼ ì´ë¯¸ì§€ë“¤ ë°±ì—…\n",
    "                for img_file in ['confusion_matrix.png', 'results.png', 'labels.jpg', 'val_batch0_pred.jpg']:\n",
    "                    img_path = os.path.join(save_dir, img_file)\n",
    "                    if os.path.exists(img_path):\n",
    "                        backup_img = os.path.join(results_backup_dir, f'{timestamp}_{img_file}')\n",
    "                        shutil.copy2(img_path, backup_img)\n",
    "                        results_backup_count += 1\n",
    "                \n",
    "                # CSV ê²°ê³¼ íŒŒì¼ ë°±ì—…\n",
    "                for csv_file in ['results.csv']:\n",
    "                    csv_path = os.path.join(save_dir, csv_file)\n",
    "                    if os.path.exists(csv_path):\n",
    "                        backup_csv = os.path.join(logs_backup_dir, f'{timestamp}_{csv_file}')\n",
    "                        shutil.copy2(csv_path, backup_csv)\n",
    "                        results_backup_count += 1\n",
    "                \n",
    "                print(f\"  ğŸ“Š ê²°ê³¼ íŒŒì¼ ë°±ì—…: {results_backup_count}ê°œ íŒŒì¼\")\n",
    "            \n",
    "            # ğŸ“ˆ í•™ìŠµ ìš”ì•½ ì •ë³´ ì €ì¥\n",
    "            summary_info = {\n",
    "                'training_timestamp': timestamp,\n",
    "                'model_type': 'YOLOv11s',\n",
    "                'training_images': len(train_images) if 'train_images' in globals() else 'N/A',\n",
    "                'validation_images': len(val_images) if 'val_images' in globals() else 'N/A',\n",
    "                'batch_size': batch_size if 'batch_size' in globals() else 'N/A',\n",
    "                'epochs': 200,\n",
    "                'device': device if 'device' in globals() else 'N/A',\n",
    "                'final_metrics': final_metrics if 'final_metrics' in locals() else {}\n",
    "            }\n",
    "            \n",
    "            summary_path = os.path.join(session_backup_dir, f'training_summary_{timestamp}.json')\n",
    "            with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(summary_info, f, indent=2, ensure_ascii=False, default=str)\n",
    "            \n",
    "            print(f\"  ğŸ“‹ í•™ìŠµ ìš”ì•½ ì €ì¥: training_summary_{timestamp}.json\")\n",
    "            \n",
    "            # ğŸ¯ ë°±ì—… ì™„ë£Œ ìš”ì•½\n",
    "            print(f\"\\nâœ… Google Drive ì „ì²´ ë”¥ëŸ¬ë‹ ê²°ê³¼ ë°±ì—… ì™„ë£Œ!\")\n",
    "            print(f\"ğŸ“ ë°±ì—… ìœ„ì¹˜: {session_backup_dir}\")\n",
    "            print(f\"ğŸ¤– ëª¨ë¸ íŒŒì¼: {model_backup_count}ê°œ\")\n",
    "            print(f\"ğŸ“Š ê²°ê³¼ íŒŒì¼: {results_backup_count}ê°œ\")\n",
    "            print(f\"ğŸ“‹ ìš”ì•½ ì •ë³´: 1ê°œ\")\n",
    "            print(f\"ğŸ’¾ ì´ ë°±ì—… í¬ê¸°: ì•½ {sum(os.path.getsize(os.path.join(root, f)) for root, dirs, files in os.walk(session_backup_dir) for f in files) / 1024 / 1024:.1f}MB\")\n",
    "            \n",
    "        except Exception as backup_error:\n",
    "            print(f\"âŒ Google Drive ë°±ì—… ì‹¤íŒ¨: {backup_error}\")\n",
    "            print(f\"ğŸ’¡ ìˆ˜ë™ ë°±ì—… í•„ìš”:\")\n",
    "            print(f\"  ğŸ¥‡ ìµœê³  ëª¨ë¸: {best_model}\")\n",
    "            print(f\"  ğŸ“ ìµœì¢… ëª¨ë¸: {last_model}\")\n",
    "            print(f\"  ğŸ“Š ê²°ê³¼ í´ë”: {save_dir}\")\n",
    "        \n",
    "        # ğŸ”— ì „ì—­ ë³€ìˆ˜ë¡œ ê²°ê³¼ ì €ì¥ (3ë‹¨ê³„ ì—°ë™)\n",
    "        globals()['multiscale_results'] = results\n",
    "        globals()['best_model_path'] = best_model\n",
    "        globals()['last_model_path'] = last_model\n",
    "        globals()['training_save_dir'] = save_dir\n",
    "        globals()['model_trained'] = True\n",
    "        globals()['trained_model_path'] = best_model\n",
    "        globals()['training_timestamp'] = timestamp if 'timestamp' in locals() else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ í¬ê¸°ë³„ ì†Œë‚˜ë¬´ íƒì§€ YOLOv11s í•™ìŠµ ëŒ€ì„±ê³µ!\")\n",
    "        print(f\"ğŸŒ² ì‘ì€ ì†Œë‚˜ë¬´ë¶€í„° í° ì†Œë‚˜ë¬´ê¹Œì§€ ì™„ë²½ íƒì§€!\")\n",
    "        print(f\"â˜ï¸ Google Drive ì „ì²´ ê²°ê³¼ ë°±ì—… ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ¯ 3ë‹¨ê³„ ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}\")\n",
    "    print(f\"ğŸ’¡ í•´ê²° ë°©ì•ˆ:\")\n",
    "    print(f\"  1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ì¤„ì—¬ë³´ì„¸ìš” (í˜„ì¬: {batch_size})\")\n",
    "    print(f\"  2. ë°ì´í„° ê²½ë¡œ ì˜¤ë¥˜: 1ë‹¨ê³„ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\")  \n",
    "    print(f\"  3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”\")\n",
    "    print(f\"  4. ê¶Œí•œ ë¬¸ì œ: Google Drive ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”\")\n",
    "    \n",
    "    # ë””ë²„ê¹… ì •ë³´\n",
    "    print(f\"\\nğŸ” ë””ë²„ê¹… ì •ë³´:\")\n",
    "    print(f\"  ğŸ“‚ ìˆ˜ì •ëœ data.yaml: {data_yaml_path}\")\n",
    "    print(f\"  ğŸ“‚ ì›ë³¸ data.yaml: {original_yaml_path}\")\n",
    "    print(f\"  ğŸ“‚ ë°ì´í„° ê¸°ë³¸ ê²½ë¡œ: {data_base_dir}\")\n",
    "    print(f\"  ğŸ¤– ëª¨ë¸ ê²½ë¡œ: {model_path}\")\n",
    "    print(f\"  ğŸ’¾ ì¥ì¹˜: {device}\")\n",
    "    print(f\"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "    \n",
    "    raise e\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸŒ² 2ë‹¨ê³„: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  YOLOv11s í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"ğŸ¯ ì•ˆì •ì  ì„±ëŠ¥ í–¥ìƒ + GPU ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©ë¨\")\n",
    "print(\"ğŸ“Š 3ë‹¨ê³„ ì„±ëŠ¥ ë¶„ì„ ì…€ë¡œ ì´ë™í•˜ì„¸ìš”!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0b724",
   "metadata": {},
   "source": [
    "## ğŸ” **3ë‹¨ê³„: í•™ìŠµëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ì‹œê°í™”**\n",
    "\n",
    "> **í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ë¡œ ì‹¤ì œ ì´ë¯¸ì§€ì—ì„œ í”¼í•´ëª© íƒì§€ í…ŒìŠ¤íŠ¸**  \n",
    "> í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ íƒì§€ ì„±ëŠ¥ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.  \n",
    "> **ê²°ê³¼**: ë°”ìš´ë”©ë°•ìŠ¤ ë§ˆí‚¹ëœ íƒì§€ ê²°ê³¼ ì´ë¯¸ì§€ ë° ì„±ëŠ¥ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” 3ë‹¨ê³„: í•™ìŠµëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° íƒì§€ ê²°ê³¼ ì‹œê°í™”\n",
    "print(\"ğŸ” 3ë‹¨ê³„: í•™ìŠµëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ì‹œê°í™”\")\n",
    "print(\"ğŸ¯ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹¤ì œ í”¼í•´ëª© íƒì§€ ë° ì‹œê°ì  í™•ì¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "# ğŸ” 2ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ ìƒíƒœ ë° ëª¨ë¸ ê²½ë¡œ í™•ì¸\n",
    "model_trained = globals().get('model_trained', False)\n",
    "trained_model_path = globals().get('trained_model_path', None)\n",
    "data_yaml_path = globals().get('data_yaml_path', None)\n",
    "save_dir = globals().get('save_dir', None)\n",
    "\n",
    "print(\"ğŸ“‹ 2ë‹¨ê³„ í•™ìŠµ ìƒíƒœ í™•ì¸:\")\n",
    "print(f\"  âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {'Yes' if model_trained else 'No'}\")\n",
    "print(f\"  ğŸ¤– ëª¨ë¸ ê²½ë¡œ: {trained_model_path or 'ì—†ìŒ'}\")\n",
    "print(f\"  ğŸ“Š ë°ì´í„° ê²½ë¡œ: {data_yaml_path or 'ì—†ìŒ'}\")\n",
    "print(f\"  ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {save_dir or 'ì—†ìŒ'}\")\n",
    "\n",
    "# ğŸ¯ 2ë‹¨ê³„ì—ì„œ ì™„ë£Œëœ ëª¨ë¸ ì§ì ‘ ì‚¬ìš©\n",
    "def get_trained_model_from_step2():\n",
    "    \"\"\"2ë‹¨ê³„ì—ì„œ í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ê²½ë¡œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    print(\"\\nğŸ¯ 2ë‹¨ê³„ ì™„ë£Œ ëª¨ë¸ ê²½ë¡œ í™•ì¸ ì¤‘...\")\n",
    "    \n",
    "    # 1ìˆœìœ„: 2ë‹¨ê³„ì—ì„œ ì €ì¥ëœ ì „ì—­ë³€ìˆ˜ ëª¨ë¸ ê²½ë¡œ\n",
    "    if trained_model_path and os.path.exists(trained_model_path):\n",
    "        print(f\"âœ… 2ë‹¨ê³„ ì™„ë£Œ ëª¨ë¸ ë°œê²¬: {os.path.basename(trained_model_path)}\")\n",
    "        print(f\"  ğŸ“ ëª¨ë¸ ê²½ë¡œ: {trained_model_path}\")\n",
    "        \n",
    "        model_size = os.path.getsize(trained_model_path) / 1024 / 1024  # MB\n",
    "        print(f\"  ğŸ“ ëª¨ë¸ í¬ê¸°: {model_size:.1f}MB\")\n",
    "        print(f\"  ğŸ•’ ìˆ˜ì • ì‹œê°„: {datetime.fromtimestamp(os.path.getctime(trained_model_path)).strftime('%Y-%m-%d %H:%M')}\")\n",
    "        \n",
    "        return trained_model_path\n",
    "    \n",
    "    # 2ìˆœìœ„: 2ë‹¨ê³„ ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ best.pt ì°¾ê¸°\n",
    "    if save_dir:\n",
    "        best_pt_path = os.path.join(save_dir, 'weights', 'best.pt')\n",
    "        if os.path.exists(best_pt_path):\n",
    "            print(f\"âœ… 2ë‹¨ê³„ ê²°ê³¼ í´ë”ì—ì„œ ëª¨ë¸ ë°œê²¬: best.pt\")\n",
    "            print(f\"  ğŸ“ ëª¨ë¸ ê²½ë¡œ: {best_pt_path}\")\n",
    "            \n",
    "            model_size = os.path.getsize(best_pt_path) / 1024 / 1024  # MB\n",
    "            print(f\"  ğŸ“ ëª¨ë¸ í¬ê¸°: {model_size:.1f}MB\")\n",
    "            print(f\"  ğŸ•’ ìˆ˜ì • ì‹œê°„: {datetime.fromtimestamp(os.path.getctime(best_pt_path)).strftime('%Y-%m-%d %H:%M')}\")\n",
    "            \n",
    "            return best_pt_path\n",
    "    \n",
    "    print(\"âŒ 2ë‹¨ê³„ ì™„ë£Œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "    print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "    print(\"  1. 2ë‹¨ê³„ í•™ìŠµì„ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”\")\n",
    "    print(\"  2. í•™ìŠµì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´ ë…¸íŠ¸ë¶ì„ ì¬ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”\")\n",
    "    print(\"  3. ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ğŸ” ë°±ì—… ëª¨ë¸ íƒì§€ (2ë‹¨ê³„ ì‹¤íŒ¨ ì‹œ)\n",
    "def find_backup_trained_model():\n",
    "    \"\"\"2ë‹¨ê³„ ëª¨ë¸ì´ ì—†ì„ ë•Œ ë°±ì—… íƒì§€\"\"\"\n",
    "    print(\"\\nğŸ” ë°±ì—… ëª¨ë¸ íƒì§€ ì¤‘...\")\n",
    "    \n",
    "    # ê°€ëŠ¥í•œ ë°±ì—… ê²½ë¡œë“¤\n",
    "    backup_search_paths = [\n",
    "        '/content/runs/detect/pinetree_multiscale_*/weights/best.pt',\n",
    "        '/content/runs/detect/train*/weights/best.pt', \n",
    "        '/content/drive/MyDrive/pinetree_scan/models/multiscale_yolo11s/best_multiscale_*.pt',\n",
    "        '/content/drive/MyDrive/pinetree_scan/training_results/*/weights/best.pt'\n",
    "    ]\n",
    "    \n",
    "    found_models = []\n",
    "    \n",
    "    for path_pattern in backup_search_paths:\n",
    "        matches = glob.glob(path_pattern)\n",
    "        found_models.extend(matches)\n",
    "    \n",
    "    if found_models:\n",
    "        # ê°€ì¥ ìµœì‹  ëª¨ë¸ ì„ íƒ\n",
    "        latest_model = max(found_models, key=os.path.getctime)\n",
    "        model_size = os.path.getsize(latest_model) / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"âœ… ë°±ì—… ëª¨ë¸ ë°œê²¬: {os.path.basename(latest_model)}\")\n",
    "        print(f\"  ğŸ“ ì „ì²´ ê²½ë¡œ: {latest_model}\")\n",
    "        print(f\"  ğŸ“ ëª¨ë¸ í¬ê¸°: {model_size:.1f}MB\")\n",
    "        print(f\"  ğŸ•’ ìˆ˜ì • ì‹œê°„: {datetime.fromtimestamp(os.path.getctime(latest_model)).strftime('%Y-%m-%d %H:%M')}\")\n",
    "        \n",
    "        return latest_model\n",
    "    else:\n",
    "        print(\"âŒ ë°±ì—… ëª¨ë¸ë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return None\n",
    "\n",
    "# ğŸ“‚ 2ë‹¨ê³„ ë°ì´í„°ì—ì„œ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°\n",
    "def get_test_images_from_step2():\n",
    "    \"\"\"2ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    print(\"\\nğŸ“‚ 2ë‹¨ê³„ ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì„ íƒ ì¤‘...\")\n",
    "    \n",
    "    # ğŸ” ì‹¤ì œ í›ˆë ¨ì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ ê²½ë¡œ íƒì§€ (ìš°ì„ ìˆœìœ„ ìˆœ)\n",
    "    search_paths = []\n",
    "    \n",
    "    # 1ìˆœìœ„: data.yamlì—ì„œ ì‹¤ì œ ê²½ë¡œ ì½ê¸°\n",
    "    if data_yaml_path and os.path.exists(data_yaml_path):\n",
    "        print(f\"âœ… 2ë‹¨ê³„ ë°ì´í„°ì…‹ YAML: {data_yaml_path}\")\n",
    "        \n",
    "        # data.yaml íŒŒì¼ ì½ì–´ì„œ ì‹¤ì œ ê²½ë¡œ í™•ì¸\n",
    "        try:\n",
    "            with open(data_yaml_path, 'r', encoding='utf-8') as f:\n",
    "                yaml_content = f.read()\n",
    "                print(f\"ğŸ“„ data.yaml ë‚´ìš© í™•ì¸:\")\n",
    "                for line in yaml_content.split('\\n')[:10]:  # ì²« 10ì¤„ë§Œ ì¶œë ¥\n",
    "                    if line.strip():\n",
    "                        print(f\"    {line}\")\n",
    "                        \n",
    "                # path ë˜ëŠ” train/val ê²½ë¡œ ì¶”ì¶œ ì‹œë„\n",
    "                for line in yaml_content.split('\\n'):\n",
    "                    if 'path:' in line and not line.strip().startswith('#'):\n",
    "                        yaml_path = line.split('path:')[1].strip().strip('\"\\'')\n",
    "                        if yaml_path:\n",
    "                            search_paths.append(f\"{yaml_path}/val/images\")\n",
    "                            search_paths.append(f\"{yaml_path}/train/images\")\n",
    "                            print(f\"  ğŸ¯ YAMLì—ì„œ ê²½ë¡œ ë°œê²¬: {yaml_path}\")\n",
    "                    elif ('train:' in line or 'val:' in line) and not line.strip().startswith('#'):\n",
    "                        # ì ˆëŒ€ ê²½ë¡œ ì¶”ì¶œ\n",
    "                        path_part = line.split(':')[1].strip().strip('\"\\'')\n",
    "                        if path_part and os.path.isabs(path_part):\n",
    "                            search_paths.append(path_part)\n",
    "                            print(f\"  ğŸ¯ YAMLì—ì„œ ì ˆëŒ€ ê²½ë¡œ ë°œê²¬: {path_part}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ data.yaml ì½ê¸° ì‹¤íŒ¨: {str(e)[:50]}\")\n",
    "    \n",
    "    # 2ìˆœìœ„: data_yaml_path ê¸°ì¤€ ê²½ë¡œë“¤\n",
    "    if data_yaml_path:\n",
    "        dataset_dir = os.path.dirname(data_yaml_path)\n",
    "        search_paths.extend([\n",
    "            f\"{dataset_dir}/val/images\",\n",
    "            f\"{dataset_dir}/train/images\",\n",
    "            f\"{dataset_dir}/test/images\"\n",
    "        ])\n",
    "    \n",
    "    # 3ìˆœìœ„: ì¼ë°˜ì ì¸ ì™„ë£Œ í›ˆë ¨ ë°ì´í„°ì…‹ ê²½ë¡œë“¤\n",
    "    search_paths.extend([\n",
    "        \"/content/complete_training_*/training_dataset/val/images\",\n",
    "        \"/content/complete_training_*/training_dataset/train/images\", \n",
    "        \"/content/complete_training_*/training_dataset/test/images\",\n",
    "        \"/content/drive/MyDrive/pinetree_scan/training_data/val/images\",\n",
    "        \"/content/drive/MyDrive/pinetree_scan/training_data/train/images\"\n",
    "    ])\n",
    "    \n",
    "    print(f\"ğŸ” íƒì§€í•  ê²½ë¡œë“¤: {len(search_paths)}ê°œ\")\n",
    "    \n",
    "    # ê²½ë¡œë³„ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰ (ìƒì„¸ ë””ë²„ê¹… í¬í•¨)\n",
    "    all_images = []\n",
    "    found_paths = []\n",
    "    \n",
    "    for i, search_path in enumerate(search_paths, 1):\n",
    "        print(f\"  ğŸ” [{i}/{len(search_paths)}] íƒìƒ‰ ì¤‘: {search_path}\")\n",
    "        \n",
    "        if '*' in search_path:\n",
    "            # ì™€ì¼ë“œì¹´ë“œ ê²½ë¡œ ì²˜ë¦¬\n",
    "            for ext in ['jpg', 'png', 'jpeg', 'tif', 'tiff']:\n",
    "                pattern = f\"{search_path}/*.{ext}\"\n",
    "                matches = glob.glob(pattern)\n",
    "                if matches:\n",
    "                    all_images.extend(matches)\n",
    "                    found_paths.append(pattern)\n",
    "                    print(f\"    âœ… {ext.upper()}: {len(matches)}ê°œ ë°œê²¬\")\n",
    "                    # ì²˜ìŒ ëª‡ ê°œ íŒŒì¼ëª… ìƒ˜í”Œ ì¶œë ¥\n",
    "                    for j, match in enumerate(matches[:3], 1):\n",
    "                        print(f\"      {j}. {os.path.basename(match)}\")\n",
    "                    if len(matches) > 3:\n",
    "                        print(f\"      ... ì™¸ {len(matches)-3}ê°œ\")\n",
    "                else:\n",
    "                    print(f\"    âŒ {ext.upper()}: ì—†ìŒ\")\n",
    "        else:\n",
    "            # ì§ì ‘ ê²½ë¡œ ì²˜ë¦¬\n",
    "            print(f\"    ğŸ“ ê²½ë¡œ ì¡´ì¬ í™•ì¸: {os.path.exists(search_path)}\")\n",
    "            if os.path.exists(search_path):\n",
    "                # í•´ë‹¹ í´ë”ì˜ ëª¨ë“  ë‚´ìš© í™•ì¸\n",
    "                try:\n",
    "                    contents = os.listdir(search_path)\n",
    "                    print(f\"    ğŸ“‹ í´ë” ë‚´ìš©: {len(contents)}ê°œ í•­ëª©\")\n",
    "                    if contents:\n",
    "                        image_files = [f for f in contents if f.lower().endswith(('.jpg', '.png', '.jpeg', '.tif', '.tiff'))]\n",
    "                        if image_files:\n",
    "                            print(f\"    ğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼: {len(image_files)}ê°œ\")\n",
    "                            # ì‹¤ì œ full pathë¡œ ì¶”ê°€\n",
    "                            for img_file in image_files:\n",
    "                                full_path = os.path.join(search_path, img_file)\n",
    "                                all_images.append(full_path)\n",
    "                            found_paths.append(search_path)\n",
    "                            # ì²˜ìŒ ëª‡ ê°œ íŒŒì¼ëª… ì¶œë ¥\n",
    "                            for j, img_file in enumerate(image_files[:3], 1):\n",
    "                                print(f\"      {j}. {img_file}\")\n",
    "                            if len(image_files) > 3:\n",
    "                                print(f\"      ... ì™¸ {len(image_files)-3}ê°œ\")\n",
    "                        else:\n",
    "                            print(f\"    âŒ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ\")\n",
    "                            # ë‹¤ë¥¸ íŒŒì¼ë“¤ì´ ë­ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "                            other_files = contents[:5]\n",
    "                            if other_files:\n",
    "                                print(f\"    ğŸ“„ ë‹¤ë¥¸ íŒŒì¼ë“¤: {', '.join(other_files)}\")\n",
    "                    else:\n",
    "                        print(f\"    ğŸ“‚ ë¹ˆ í´ë”\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    âŒ í´ë” ì½ê¸° ì˜¤ë¥˜: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"    âŒ ê²½ë¡œ ì—†ìŒ\")\n",
    "        \n",
    "        print()  # ë¹ˆ ì¤„ë¡œ êµ¬ë¶„\n",
    "    \n",
    "    if all_images:\n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        unique_images = list(set(all_images))\n",
    "        print(f\"\\nâœ… ì´ ë°œê²¬ëœ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(unique_images)}ê°œ\")\n",
    "        print(f\"  ğŸ“‚ ê²€ìƒ‰ëœ ê²½ë¡œ: {len(found_paths)}ê°œ\")\n",
    "        \n",
    "        # ê²€ì¦ìš© ì´ë¯¸ì§€ ìš°ì„  ì„ íƒ (val > test > train ìˆœ)\n",
    "        val_images = [img for img in unique_images if '/val/' in img]\n",
    "        test_images = [img for img in unique_images if '/test/' in img]\n",
    "        train_images = [img for img in unique_images if '/train/' in img]\n",
    "        \n",
    "        selected_images = []\n",
    "        \n",
    "        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì„ íƒ\n",
    "        if val_images:\n",
    "            count = min(6, len(val_images))\n",
    "            selected_images.extend(random.sample(val_images, count))\n",
    "            print(f\"  ğŸ¯ ê²€ì¦ìš©(val) ì´ë¯¸ì§€ì—ì„œ {count}ê°œ ì„ íƒ\")\n",
    "        \n",
    "        remaining_slots = 8 - len(selected_images)\n",
    "        if remaining_slots > 0 and test_images:\n",
    "            count = min(remaining_slots, len(test_images))\n",
    "            selected_images.extend(random.sample(test_images, count))\n",
    "            print(f\"  ğŸ¯ í…ŒìŠ¤íŠ¸(test) ì´ë¯¸ì§€ì—ì„œ {count}ê°œ ì¶”ê°€ ì„ íƒ\")\n",
    "        \n",
    "        remaining_slots = 8 - len(selected_images)\n",
    "        if remaining_slots > 0 and train_images:\n",
    "            count = min(remaining_slots, len(train_images))\n",
    "            selected_images.extend(random.sample(train_images, count))\n",
    "            print(f\"  ğŸ¯ í›ˆë ¨ìš©(train) ì´ë¯¸ì§€ì—ì„œ {count}ê°œ ì¶”ê°€ ì„ íƒ\")\n",
    "        \n",
    "        if selected_images:\n",
    "            print(f\"\\nğŸ¯ ìµœì¢… ì„ íƒëœ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(selected_images)}ê°œ\")\n",
    "            for i, img_path in enumerate(selected_images, 1):\n",
    "                print(f\"    {i}. {os.path.basename(img_path)} ({'val' if '/val/' in img_path else 'test' if '/test/' in img_path else 'train'})\")\n",
    "            \n",
    "            return selected_images\n",
    "    \n",
    "    print(\"âš ï¸ 2ë‹¨ê³„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë°±ì—… ê²½ë¡œì—ì„œ íƒì§€í•©ë‹ˆë‹¤...\")\n",
    "    return find_backup_test_images()\n",
    "\n",
    "# ğŸ“‚ ë°±ì—… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íƒì§€\n",
    "def find_backup_test_images():\n",
    "    \"\"\"2ë‹¨ê³„ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ë°±ì—… ì´ë¯¸ì§€ íƒì§€\"\"\"\n",
    "    print(\"\\nğŸ“‚ ë°±ì—… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íƒì§€ ì¤‘...\")\n",
    "    \n",
    "    # ê°€ëŠ¥í•œ ë°±ì—… ì´ë¯¸ì§€ ê²½ë¡œë“¤ (ëª¨ë“  ì´ë¯¸ì§€ í˜•ì‹ í¬í•¨)\n",
    "    backup_base_paths = [\n",
    "        '/content/drive/MyDrive/pinetree_scan/training_data/val/images',\n",
    "        '/content/drive/MyDrive/pinetree_scan/training_data/test/images', \n",
    "        '/content/drive/MyDrive/pinetree_scan/training_data/train/images',\n",
    "        '/content/complete_training_*/training_dataset/val/images',\n",
    "        '/content/complete_training_*/training_dataset/train/images',\n",
    "    ]\n",
    "    \n",
    "    all_images = []\n",
    "    \n",
    "    for base_path in backup_base_paths:\n",
    "        for ext in ['jpg', 'png', 'jpeg', 'tif', 'tiff']:\n",
    "            pattern = f\"{base_path}/*.{ext}\"\n",
    "            matches = glob.glob(pattern)\n",
    "            if matches:\n",
    "                all_images.extend(matches)\n",
    "                print(f\"  âœ… {pattern}: {len(matches)}ê°œ ë°œê²¬\")\n",
    "    \n",
    "    if all_images:\n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        unique_images = list(set(all_images))\n",
    "        print(f\"âœ… ë°±ì—… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë°œê²¬: {len(unique_images)}ê°œ\")\n",
    "        \n",
    "        # ëœë¤í•˜ê²Œ 6-8ê°œ ì„ íƒ\n",
    "        test_count = min(8, len(unique_images))\n",
    "        selected_images = random.sample(unique_images, test_count)\n",
    "        \n",
    "        print(f\"  ğŸ“‹ ì„ íƒëœ ì´ë¯¸ì§€: {test_count}ê°œ\")\n",
    "        for i, img_path in enumerate(selected_images, 1):\n",
    "            print(f\"    {i}. {os.path.basename(img_path)}\")\n",
    "        \n",
    "        return selected_images\n",
    "    else:\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return []\n",
    "\n",
    "# ğŸ¨ íƒì§€ ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜ (Google Colab ìµœì í™”)\n",
    "def visualize_detection_results(model, image_paths, save_dir):\n",
    "    \"\"\"íƒì§€ ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥ (Colabì—ì„œ ë°”ë¡œ í™•ì¸ ê°€ëŠ¥)\"\"\"\n",
    "    print(f\"\\nğŸ¨ íƒì§€ ê²°ê³¼ ì‹œê°í™” ì¤‘...\")\n",
    "    \n",
    "    detection_results = []\n",
    "    visualization_paths = []\n",
    "    \n",
    "    # ì‹œê°í™” ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    viz_dir = os.path.join(save_dir, 'detection_visualizations')\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # matplotlib í•œê¸€ í°íŠ¸ ì„¤ì • (Colabìš©)\n",
    "    plt.rcParams['font.family'] = ['DejaVu Sans']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths, 1):\n",
    "        try:\n",
    "            print(f\"  ğŸ“¸ {i}/{len(image_paths)}: {os.path.basename(image_path)} ì²˜ë¦¬ ì¤‘...\")\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ì¶”ë¡ \n",
    "            results = model(image_path, conf=0.25, iou=0.6)\n",
    "            \n",
    "            # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"    âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            height, width = image_rgb.shape[:2]\n",
    "            \n",
    "            # íƒì§€ ê²°ê³¼ ì¶”ì¶œ\n",
    "            detections = []\n",
    "            if len(results) > 0 and results[0].boxes is not None:\n",
    "                boxes = results[0].boxes\n",
    "                for box in boxes:\n",
    "                    # ë°”ìš´ë”©ë°•ìŠ¤ ì¢Œí‘œ (xyxy í˜•ì‹)\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                    confidence = float(box.conf[0])\n",
    "                    class_id = int(box.cls[0])\n",
    "                    \n",
    "                    detections.append({\n",
    "                        'bbox': [x1, y1, x2, y2],\n",
    "                        'confidence': confidence,\n",
    "                        'class_id': class_id\n",
    "                    })\n",
    "            \n",
    "            # ğŸ¨ ì‹œê°í™” ìƒì„± (Colab ìµœì í™”)\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "            \n",
    "            # ì›ë³¸ ì´ë¯¸ì§€ (ì™¼ìª½)\n",
    "            ax1.imshow(image_rgb)\n",
    "            ax1.set_title(f'Original: {os.path.basename(image_path)}', fontsize=12, fontweight='bold')\n",
    "            ax1.set_xticks([])\n",
    "            ax1.set_yticks([])\n",
    "            \n",
    "            # íƒì§€ ê²°ê³¼ ì´ë¯¸ì§€ (ì˜¤ë¥¸ìª½)\n",
    "            ax2.imshow(image_rgb)\n",
    "            ax2.set_title(f'Detection Result: {len(detections)} damaged trees found', \n",
    "                         fontsize=12, fontweight='bold', color='red' if detections else 'green')\n",
    "            \n",
    "            # íƒì§€ëœ ê°ì²´ì— ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
    "            colors = ['red', 'blue', 'lime', 'orange', 'purple', 'yellow', 'pink', 'cyan']\n",
    "            \n",
    "            for j, detection in enumerate(detections):\n",
    "                x1, y1, x2, y2 = detection['bbox']\n",
    "                confidence = detection['confidence']\n",
    "                \n",
    "                # ë°”ìš´ë”©ë°•ìŠ¤ ìƒ‰ìƒ\n",
    "                color = colors[j % len(colors)]\n",
    "                \n",
    "                # ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=4, edgecolor=color, facecolor='none', alpha=0.8)\n",
    "                ax2.add_patch(rect)\n",
    "                \n",
    "                # ì‹ ë¢°ë„ í…ìŠ¤íŠ¸ (ë” í¬ê³  ëª…í™•í•˜ê²Œ)\n",
    "                label = f'Damaged Tree\\\\nConfidence: {confidence:.3f}'\n",
    "                ax2.text(x1, y1-20, label, fontsize=9, fontweight='bold',\n",
    "                       color='white', \n",
    "                       bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=color, alpha=0.9, edgecolor='white'))\n",
    "            \n",
    "            ax2.set_xticks([])\n",
    "            ax2.set_yticks([])\n",
    "            \n",
    "            # ì „ì²´ ê²°ê³¼ ì •ë³´\n",
    "            fig.suptitle(f'ğŸ¯ Pine Tree Damage Detection Test #{i}\\\\n' + \n",
    "                        f'Image: {width}x{height}px | Detections: {len(detections)} | ' +\n",
    "                        f'Avg Confidence: {np.mean([d[\"confidence\"] for d in detections]):.3f}' if detections else f'No detections',\n",
    "                        fontsize=14, fontweight='bold', y=0.95)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.85)\n",
    "            \n",
    "            # ğŸ” Colabì—ì„œ ë°”ë¡œ ì´ë¯¸ì§€ í‘œì‹œ\n",
    "            plt.show()\n",
    "            \n",
    "            # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥\n",
    "            save_path = os.path.join(viz_dir, f'detection_result_{i:02d}_{os.path.basename(image_path)}.png')\n",
    "            fig.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "            plt.close(fig)  # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            \n",
    "            visualization_paths.append(save_path)\n",
    "            \n",
    "            # ê²°ê³¼ ì •ë³´ ì €ì¥\n",
    "            detection_results.append({\n",
    "                'image_path': image_path,\n",
    "                'image_name': os.path.basename(image_path),\n",
    "                'detections': len(detections),\n",
    "                'avg_confidence': np.mean([d['confidence'] for d in detections]) if detections else 0,\n",
    "                'detection_details': detections,\n",
    "                'visualization_path': save_path\n",
    "            })\n",
    "            \n",
    "            print(f\"    âœ… íƒì§€ ì™„ë£Œ: {len(detections)}ê°œ í”¼í•´ëª© ë°œê²¬ (ì‹ ë¢°ë„: {np.mean([d['confidence'] for d in detections]):.3f})\" if detections else \"    âšª íƒì§€ëœ í”¼í•´ëª© ì—†ìŒ\")\n",
    "            \n",
    "            # ê° ì´ë¯¸ì§€ë³„ êµ¬ë¶„ì„ \n",
    "            print(\"    \" + \"-\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return detection_results, visualization_paths\n",
    "\n",
    "# ğŸ“Š íƒì§€ ì„±ëŠ¥ ë¶„ì„ ë° ìš”ì•½\n",
    "def analyze_detection_performance(detection_results):\n",
    "    \"\"\"íƒì§€ ì„±ëŠ¥ ë¶„ì„ ë° í†µê³„ ìš”ì•½\"\"\"\n",
    "    print(f\"\\nğŸ“Š íƒì§€ ì„±ëŠ¥ ë¶„ì„:\")\n",
    "    \n",
    "    if not detection_results:\n",
    "        print(\"âŒ ë¶„ì„í•  íƒì§€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return\n",
    "    \n",
    "    # í†µê³„ ê³„ì‚°\n",
    "    total_images = len(detection_results)\n",
    "    total_detections = sum(r['detections'] for r in detection_results)\n",
    "    images_with_detections = sum(1 for r in detection_results if r['detections'] > 0)\n",
    "    \n",
    "    confidences = []\n",
    "    for result in detection_results:\n",
    "        for detection in result['detection_details']:\n",
    "            confidences.append(detection['confidence'])\n",
    "    \n",
    "    print(f\"  ğŸ“¸ ì´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {total_images}ê°œ\")\n",
    "    print(f\"  ğŸ¯ ì´ íƒì§€ëœ í”¼í•´ëª©: {total_detections}ê°œ\")\n",
    "    print(f\"  âœ… í”¼í•´ëª© ë°œê²¬ëœ ì´ë¯¸ì§€: {images_with_detections}ê°œ ({images_with_detections/total_images*100:.1f}%)\")\n",
    "    print(f\"  ğŸ“Š ì´ë¯¸ì§€ë‹¹ í‰ê·  íƒì§€: {total_detections/total_images:.2f}ê°œ\")\n",
    "    \n",
    "    if confidences:\n",
    "        avg_conf = np.mean(confidences)\n",
    "        min_conf = np.min(confidences)\n",
    "        max_conf = np.max(confidences)\n",
    "        \n",
    "        print(f\"  ğŸ¯ ì‹ ë¢°ë„ í†µê³„:\")\n",
    "        print(f\"    í‰ê· : {avg_conf:.3f}\")\n",
    "        print(f\"    ìµœì†Œ: {min_conf:.3f}\")\n",
    "        print(f\"    ìµœëŒ€: {max_conf:.3f}\")\n",
    "        \n",
    "        # ì‹ ë¢°ë„ ë¶„í¬\n",
    "        high_conf = sum(1 for c in confidences if c >= 0.7)\n",
    "        medium_conf = sum(1 for c in confidences if 0.5 <= c < 0.7)\n",
    "        low_conf = sum(1 for c in confidences if c < 0.5)\n",
    "        \n",
    "        print(f\"  ğŸ“ˆ ì‹ ë¢°ë„ ë¶„í¬:\")\n",
    "        print(f\"    ë†’ìŒ (â‰¥0.7): {high_conf}ê°œ ({high_conf/len(confidences)*100:.1f}%)\")\n",
    "        print(f\"    ì¤‘ê°„ (0.5-0.7): {medium_conf}ê°œ ({medium_conf/len(confidences)*100:.1f}%)\")\n",
    "        print(f\"    ë‚®ìŒ (<0.5): {low_conf}ê°œ ({low_conf/len(confidences)*100:.1f}%)\")\n",
    "    \n",
    "    # ì„±ëŠ¥ í‰ê°€\n",
    "    if images_with_detections/total_images >= 0.8:\n",
    "        performance_grade = \"ğŸ† ìš°ìˆ˜\"\n",
    "        performance_desc = \"ëŒ€ë¶€ë¶„ ì´ë¯¸ì§€ì—ì„œ í”¼í•´ëª©ì„ ì„±ê³µì ìœ¼ë¡œ íƒì§€\"\n",
    "    elif images_with_detections/total_images >= 0.6:\n",
    "        performance_grade = \"âœ… ì–‘í˜¸\"  \n",
    "        performance_desc = \"ë§ì€ ì´ë¯¸ì§€ì—ì„œ í”¼í•´ëª© íƒì§€ ì„±ê³µ\"\n",
    "    elif images_with_detections/total_images >= 0.4:\n",
    "        performance_grade = \"âš ï¸ ë³´í†µ\"\n",
    "        performance_desc = \"ì¼ë¶€ ì´ë¯¸ì§€ì—ì„œ íƒì§€ ì„±ê³µ, ê°œì„  ì—¬ì§€ ìˆìŒ\"\n",
    "    else:\n",
    "        performance_grade = \"âŒ ë¯¸í¡\"\n",
    "        performance_desc = \"íƒì§€ ì„±ëŠ¥ì´ ë¶€ì¡±, ì¶”ê°€ í•™ìŠµ í•„ìš”\"\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ì„±ëŠ¥ í‰ê°€: {performance_grade}\")\n",
    "    print(f\"  ğŸ“ í‰ê°€: {performance_desc}\")\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'total_detections': total_detections, \n",
    "        'detection_rate': images_with_detections/total_images,\n",
    "        'avg_confidence': np.mean(confidences) if confidences else 0,\n",
    "        'performance_grade': performance_grade\n",
    "    }\n",
    "\n",
    "# ğŸš€ ë©”ì¸ ì‹¤í–‰ ë¡œì§\n",
    "print(\"ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘!\")\n",
    "\n",
    "try:\n",
    "    # 1ï¸âƒ£ 2ë‹¨ê³„ ì™„ë£Œ ëª¨ë¸ ì§ì ‘ ì‚¬ìš©\n",
    "    best_model_path = get_trained_model_from_step2()\n",
    "    \n",
    "    # 2ë‹¨ê³„ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë°±ì—… íƒì§€\n",
    "    if not best_model_path:\n",
    "        print(\"\\nâš ï¸ 2ë‹¨ê³„ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë°±ì—… ëª¨ë¸ì„ íƒì§€í•©ë‹ˆë‹¤...\")\n",
    "        best_model_path = find_backup_trained_model()\n",
    "    \n",
    "    if not best_model_path:\n",
    "        print(\"âŒ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤\")\n",
    "        print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "        print(\"  1. 2ë‹¨ê³„ í•™ìŠµì„ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”\")\n",
    "        print(\"  2. í•™ìŠµ ì™„ë£Œ í›„ ë…¸íŠ¸ë¶ì„ ì¬ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”\") \n",
    "        print(\"  3. ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”:\")\n",
    "        print(\"     best_model_path = '/content/runs/detect/your_model/weights/best.pt'\")\n",
    "        print(\"     ê·¸ í›„ ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "    else:\n",
    "        # 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ\n",
    "        print(f\"\\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        test_model = YOLO(best_model_path)\n",
    "        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "        \n",
    "        # 3ï¸âƒ£ 2ë‹¨ê³„ ë°ì´í„°ì—ì„œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°\n",
    "        test_images = get_test_images_from_step2()\n",
    "        \n",
    "        if not test_images:\n",
    "            print(\"âŒ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤\")\n",
    "            print(\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "            print(\"  1. Google Driveì—ì„œ í›ˆë ¨ ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ë§ˆìš´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸\")\n",
    "            print(\"  2. /content/drive/MyDrive/pinetree_scan/training_data/ ê²½ë¡œ í™•ì¸\")\n",
    "            print(\"  3. TIF/TIFF íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸\")\n",
    "        else:\n",
    "            # 4ï¸âƒ£ íƒì§€ ë° ì‹œê°í™” ì‹¤í–‰\n",
    "            results_dir = save_dir or '/content/detection_test_results'\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            \n",
    "            detection_results, viz_paths = visualize_detection_results(test_model, test_images, results_dir)\n",
    "            \n",
    "            # 5ï¸âƒ£ ì„±ëŠ¥ ë¶„ì„\n",
    "            performance_stats = analyze_detection_performance(detection_results)\n",
    "            \n",
    "            # 6ï¸âƒ£ ê²°ê³¼ ë°±ì—… (Google Drive)\n",
    "            if IN_COLAB and viz_paths:\n",
    "                print(f\"\\nğŸ’¾ Google Drive ê²°ê³¼ ë°±ì—… ì¤‘...\")\n",
    "                \n",
    "                backup_dir = '/content/drive/MyDrive/pinetree_scan/detection_test_results'\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                final_backup_dir = f\"{backup_dir}/detection_test_{timestamp}\"\n",
    "                \n",
    "                os.makedirs(final_backup_dir, exist_ok=True)\n",
    "                \n",
    "                # ì‹œê°í™” ì´ë¯¸ì§€ ë°±ì—…\n",
    "                for viz_path in viz_paths:\n",
    "                    if os.path.exists(viz_path):\n",
    "                        backup_path = os.path.join(final_backup_dir, os.path.basename(viz_path))\n",
    "                        shutil.copy2(viz_path, backup_path)\n",
    "                \n",
    "                print(f\"âœ… Google Drive ë°±ì—… ì™„ë£Œ!\")\n",
    "                print(f\"  ğŸ“ ë°±ì—… ìœ„ì¹˜: MyDrive/pinetree_scan/detection_test_results/detection_test_{timestamp}\")\n",
    "                print(f\"  ğŸ“Š ë°±ì—…ëœ íŒŒì¼: {len(viz_paths)}ê°œ\")\n",
    "            \n",
    "            # 7ï¸âƒ£ ìµœì¢… ìš”ì•½\n",
    "            print(f\"\\nğŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "            print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:\")\n",
    "            print(f\"  ğŸ¤– ì‚¬ìš© ëª¨ë¸: {os.path.basename(best_model_path)}\")\n",
    "            print(f\"  ğŸ“¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(test_images)}ê°œ\")\n",
    "            print(f\"  ğŸ¯ ì´ íƒì§€: {sum(r['detections'] for r in detection_results)}ê°œ\")\n",
    "            print(f\"  ğŸ“ˆ ì„±ëŠ¥ ë“±ê¸‰: {performance_stats.get('performance_grade', 'N/A')}\")\n",
    "            \n",
    "            # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸\n",
    "            globals()['detection_test_completed'] = True\n",
    "            globals()['detection_results'] = detection_results\n",
    "            globals()['test_performance'] = performance_stats\n",
    "            \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        if 'test_model' in locals():\n",
    "            del test_model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}\")\n",
    "    print(f\"ğŸ’¡ í•´ê²° ë°©ë²•:\")\n",
    "    print(f\"  1. 2ë‹¨ê³„ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸\")\n",
    "    print(f\"  2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ëŸ°íƒ€ì„ ì¬ì‹œì‘\")\n",
    "    print(f\"  3. ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # ìµœì¢… ì •ë¦¬\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… 3ë‹¨ê³„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ¯ ìˆ˜í–‰ëœ ì‘ì—…:\")\n",
    "print(\"  âœ“ í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹¤ì œ í”¼í•´ëª© íƒì§€\")\n",
    "print(\"  âœ“ íƒì§€ ê²°ê³¼ ì‹œê°í™” (ë°”ìš´ë”©ë°•ìŠ¤ ë§ˆí‚¹)\")\n",
    "print(\"  âœ“ íƒì§€ ì„±ëŠ¥ í†µê³„ ë¶„ì„\")\n",
    "print(\"  âœ“ Google Drive ê²°ê³¼ ë°±ì—…\")\n",
    "print(\"ğŸš€ ì´ì œ ëª¨ë¸ì˜ ì‹¤ì œ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
