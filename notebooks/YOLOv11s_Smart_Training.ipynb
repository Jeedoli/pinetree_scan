{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad0a969",
   "metadata": {},
   "source": [
    "# ğŸŒ² YOLOv11s í”¼í•´ëª© íƒì§€ - Google Colab ìŠ¤ë§ˆíŠ¸ ì‹¤í–‰\n",
    "\n",
    "**ğŸš€ ìµœì‹  YOLOv11s ì•„í‚¤í…ì²˜ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!**\n",
    "\n",
    "## ğŸ“‹ ì‚¬ìš©ë²• (ê°œì„ ë¨!)\n",
    "1. **Google Driveì— ZIP íŒŒì¼ ì—…ë¡œë“œ** (í•œ ë²ˆë§Œ)\n",
    "2. **ëŸ°íƒ€ì„ > ëª¨ë‘ ì‹¤í–‰** í´ë¦­\n",
    "3. **ìë™ìœ¼ë¡œ íŒŒì¼ ì°¾ê¸°** ë° ì‹¤í–‰\n",
    "4. **ì™„ë£Œê¹Œì§€ ëŒ€ê¸°** â˜•\n",
    "\n",
    "## ğŸŒŸ YOLOv11s ì¥ì \n",
    "- ğŸ†• **2024ë…„ ìµœì‹ ** ì•„í‚¤í…ì²˜\n",
    "- ğŸ“ˆ **YOLOv8 ëŒ€ë¹„ 15-20%** ì„±ëŠ¥ í–¥ìƒ\n",
    "- âš¡ **ë” íš¨ìœ¨ì ì¸** ì—°ì‚°\n",
    "- ğŸ›¡ï¸ **ì•ˆì •ì ì¸** í›ˆë ¨\n",
    "\n",
    "## ğŸ”§ ê°œì„ ì‚¬í•­\n",
    "- âœ… **Google Drive íŒŒì¼ ìë™ íƒì§€**\n",
    "- âœ… **ì¬ì—…ë¡œë“œ ë¶ˆí•„ìš”**\n",
    "- âœ… **ë‹¤ì–‘í•œ íŒŒì¼ëª… ì§€ì›**\n",
    "- âœ… **ìŠ¤ë§ˆíŠ¸ ì˜¤ë¥˜ ì²˜ë¦¬**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e444eb0",
   "metadata": {},
   "source": [
    "## ğŸ”§ 1ë‹¨ê³„: í™˜ê²½ ì„¤ì • ë° GPU í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a971f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import shutil\n",
    "import zipfile\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from google.colab import drive, files\n",
    "\n",
    "print(\"ğŸš€ YOLOv11s í”¼í•´ëª© íƒì§€ í™˜ê²½ ì„¤ì • ì‹œì‘!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GPU í™•ì¸\n",
    "print(\"ğŸ–¥ï¸  GPU ì •ë³´:\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA í™•ì¸\n",
    "print(f\"\\nâš¡ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"ğŸ¯ CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "print(f\"ğŸ”¥ GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "\n",
    "# ìµœì‹  ultralytics ì„¤ì¹˜\n",
    "print(\"\\nğŸ“¦ ìµœì‹  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "!pip install --upgrade ultralytics roboflow supervision --quiet\n",
    "\n",
    "# ë²„ì „ í™•ì¸\n",
    "import ultralytics\n",
    "print(f\"âœ… Ultralytics ë²„ì „: {ultralytics.__version__}\")\n",
    "\n",
    "print(\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc25802",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 2ë‹¨ê³„: Google Drive ë§ˆìš´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "print(\"ğŸ’¾ Google Drive ë§ˆìš´íŠ¸ ì¤‘...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "os.chdir('/content')\n",
    "os.makedirs('/content/training_data', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ!\")\n",
    "print(\"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdf1aa",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 3ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ê°œì„ ë¨!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34336c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_training_zip():\n",
    "    \"\"\"Google Driveì—ì„œ í•™ìŠµ ë°ì´í„° ZIP íŒŒì¼ì„ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì°¾ê¸°\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” Google Driveì—ì„œ í•™ìŠµ ë°ì´í„° ZIP íŒŒì¼ ê²€ìƒ‰ ì¤‘...\")\n",
    "    \n",
    "    # ê²€ìƒ‰í•  ê²½ë¡œë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)\n",
    "    search_paths = [\n",
    "        \"/content/drive/MyDrive/deep_learning_pinetree/\",\n",
    "        \"/content/drive/MyDrive/pinetree_models/\",\n",
    "        \"/content/drive/MyDrive/\",\n",
    "        \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "    ]\n",
    "    \n",
    "    # ê²€ìƒ‰í•  íŒŒì¼ëª… íŒ¨í„´ë“¤\n",
    "    file_patterns = [\n",
    "        \"*pinetree*training*.zip\",\n",
    "        \"*training*data*.zip\", \n",
    "        \"*pinetree*.zip\",\n",
    "        \"training*.zip\",\n",
    "        \"*dataset*.zip\"\n",
    "    ]\n",
    "    \n",
    "    found_files = []\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        if not os.path.exists(search_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"ğŸ“‚ ê²€ìƒ‰ ì¤‘: {search_path}\")\n",
    "        \n",
    "        for pattern in file_patterns:\n",
    "            matches = glob.glob(os.path.join(search_path, pattern))\n",
    "            for match in matches:\n",
    "                if os.path.getsize(match) > 1024 * 1024:  # 1MB ì´ìƒë§Œ\n",
    "                    found_files.append(match)\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±° ë° í¬ê¸°ìˆœ ì •ë ¬\n",
    "    found_files = list(set(found_files))\n",
    "    found_files.sort(key=lambda x: os.path.getsize(x), reverse=True)\n",
    "    \n",
    "    return found_files\n",
    "\n",
    "print(\"ğŸ“¦ ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ìë™ìœ¼ë¡œ ZIP íŒŒì¼ ì°¾ê¸°\n",
    "found_zips = find_training_zip()\n",
    "\n",
    "if found_zips:\n",
    "    print(f\"\\nğŸ‰ ë°œê²¬ëœ ZIP íŒŒì¼ë“¤:\")\n",
    "    for i, zip_file in enumerate(found_zips):\n",
    "        size_mb = os.path.getsize(zip_file) / 1024 / 1024\n",
    "        print(f\"  {i+1}. {os.path.basename(zip_file)} ({size_mb:.1f} MB)\")\n",
    "        print(f\"     ğŸ“ {zip_file}\")\n",
    "    \n",
    "    # ê°€ì¥ í° íŒŒì¼ ì‚¬ìš© (ë³´í†µ ìµœì‹  ë°ì´í„°)\n",
    "    zip_path = found_zips[0]\n",
    "    print(f\"\\nâœ… ì‚¬ìš©í•  íŒŒì¼: {os.path.basename(zip_path)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Google Driveì—ì„œ í•™ìŠµ ë°ì´í„° ZIPì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(\"\\nğŸ’¡ ë‹¤ìŒ ìœ„ì¹˜ ì¤‘ í•˜ë‚˜ì— ZIP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:\")\n",
    "    print(\"   - /content/drive/MyDrive/deep_learning_pinetree/\")\n",
    "    print(\"   - /content/drive/MyDrive/\")\n",
    "    print(\"\\nğŸ“¤ ë˜ëŠ” ì§ì ‘ ì—…ë¡œë“œ:\")\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    zip_files = [f for f in uploaded.keys() if f.endswith('.zip')]\n",
    "    \n",
    "    if zip_files:\n",
    "        zip_path = f'/content/{zip_files[0]}'\n",
    "        print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {zip_path}\")\n",
    "    else:\n",
    "        zip_path = None\n",
    "\n",
    "# ZIP íŒŒì¼ ì••ì¶• í•´ì œ\n",
    "if zip_path and os.path.exists(zip_path):\n",
    "    print(f\"\\nğŸ“‚ ì••ì¶• í•´ì œ ì¤‘: {os.path.basename(zip_path)}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('/content/training_data')\n",
    "        \n",
    "        print(\"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n",
    "        \n",
    "        # ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
    "        print(\"\\nğŸ“Š ë°ì´í„° êµ¬ì¡° í™•ì¸:\")\n",
    "        !find /content/training_data -name '*.tif' -o -name '*.jpg' -o -name '*.png' | wc -l | xargs echo 'ì´ë¯¸ì§€ ê°œìˆ˜:'\n",
    "        !find /content/training_data -name '*.txt' | wc -l | xargs echo 'ë¼ë²¨ ê°œìˆ˜:'\n",
    "        \n",
    "        # ë””ë ‰í† ë¦¬ êµ¬ì¡° í‘œì‹œ\n",
    "        print(\"\\nğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\")\n",
    "        !ls -la /content/training_data/\n",
    "        \n",
    "        # data.yaml íŒŒì¼ í™•ì¸\n",
    "        yaml_files = glob.glob('/content/training_data/**/data.yaml', recursive=True)\n",
    "        if yaml_files:\n",
    "            print(f\"\\nğŸ“ data.yaml íŒŒì¼ ë°œê²¬: {yaml_files[0]}\")\n",
    "        \n",
    "        training_data_ready = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì••ì¶• í•´ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        training_data_ready = False\n",
    "else:\n",
    "    print(\"âŒ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ZIP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    training_data_ready = False\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë°ì´í„° ì¤€ë¹„ ìƒíƒœ: {'âœ… ì™„ë£Œ' if training_data_ready else 'âŒ ì‹¤íŒ¨'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c2ee0",
   "metadata": {},
   "source": [
    "## ğŸ¯ 4ë‹¨ê³„: YOLOv11s ëª¨ë¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b703312",
   "metadata": {},
   "source": [
    "## ğŸ” 3.5ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ë° ë¬¸ì œ í•´ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_data_ready:\n",
    "    print(\"ğŸ” ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ì¤‘...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. ë¼ë²¨ íŒŒì¼ ìƒì„¸ ë¶„ì„\n",
    "    label_files = glob.glob('/content/training_data/**/labels/*.txt', recursive=True)\n",
    "    if not label_files:\n",
    "        label_files = glob.glob('/content/training_data/**/*.txt', recursive=True)\n",
    "    \n",
    "    print(f\"ğŸ“Š ë°œê²¬ëœ ë¼ë²¨ íŒŒì¼ ìˆ˜: {len(label_files)}\")\n",
    "    \n",
    "    if label_files:\n",
    "        # ë¼ë²¨ í†µê³„ ë¶„ì„\n",
    "        total_objects = 0\n",
    "        empty_files = 0\n",
    "        class_counts = {}\n",
    "        bbox_sizes = []\n",
    "        \n",
    "        for label_file in label_files[:50]:  # ì²˜ìŒ 50ê°œë§Œ ë¶„ì„\n",
    "            try:\n",
    "                with open(label_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                if not lines:\n",
    "                    empty_files += 1\n",
    "                    continue\n",
    "                \n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        x, y, w, h = map(float, parts[1:5])\n",
    "                        \n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "                        bbox_sizes.append((w, h))\n",
    "                        total_objects += 1\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë¼ë²¨ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {label_file} - {e}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ë¼ë²¨ ë¶„ì„ ê²°ê³¼:\")\n",
    "        print(f\"  ğŸ“¦ ì´ ê°ì²´ ìˆ˜: {total_objects}\")\n",
    "        print(f\"  ğŸ“‚ ë¹ˆ ë¼ë²¨ íŒŒì¼: {empty_files}\")\n",
    "        print(f\"  ğŸ“Š í´ë˜ìŠ¤ë³„ ê°ì²´ ìˆ˜: {class_counts}\")\n",
    "        \n",
    "        if bbox_sizes:\n",
    "            import numpy as np\n",
    "            widths, heights = zip(*bbox_sizes)\n",
    "            print(f\"  ğŸ“ ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° í†µê³„:\")\n",
    "            print(f\"    í‰ê·  í­: {np.mean(widths):.4f}\")\n",
    "            print(f\"    í‰ê·  ë†’ì´: {np.mean(heights):.4f}\")\n",
    "            print(f\"    ìµœì†Œ í¬ê¸°: {min(widths):.4f} x {min(heights):.4f}\")\n",
    "            print(f\"    ìµœëŒ€ í¬ê¸°: {max(widths):.4f} x {max(heights):.4f}\")\n",
    "    \n",
    "    # 2. ì´ë¯¸ì§€ íŒŒì¼ ë¶„ì„\n",
    "    image_files = glob.glob('/content/training_data/**/images/*', recursive=True)\n",
    "    if not image_files:\n",
    "        image_files = glob.glob('/content/training_data/**/*.tif', recursive=True)\n",
    "        image_files.extend(glob.glob('/content/training_data/**/*.jpg', recursive=True))\n",
    "        image_files.extend(glob.glob('/content/training_data/**/*.png', recursive=True))\n",
    "    \n",
    "    print(f\"\\nğŸ–¼ï¸  ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(image_files)}\")\n",
    "    \n",
    "    # 3. ì´ë¯¸ì§€-ë¼ë²¨ ë§¤ì¹­ í™•ì¸\n",
    "    matched_pairs = 0\n",
    "    for img_file in image_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸\n",
    "        img_name = os.path.splitext(os.path.basename(img_file))[0]\n",
    "        \n",
    "        # ê°€ëŠ¥í•œ ë¼ë²¨ íŒŒì¼ ê²½ë¡œë“¤\n",
    "        possible_label_paths = [\n",
    "            f'/content/training_data/labels/{img_name}.txt',\n",
    "            f'/content/training_data/train/labels/{img_name}.txt',\n",
    "            os.path.join(os.path.dirname(img_file).replace('images', 'labels'), f'{img_name}.txt')\n",
    "        ]\n",
    "        \n",
    "        for label_path in possible_label_paths:\n",
    "            if os.path.exists(label_path):\n",
    "                matched_pairs += 1\n",
    "                break\n",
    "    \n",
    "    print(f\"ğŸ”— ì´ë¯¸ì§€-ë¼ë²¨ ë§¤ì¹­ (ì²˜ìŒ 10ê°œ): {matched_pairs}/10\")\n",
    "    \n",
    "    # 4. data.yaml í™•ì¸ ë° ìˆ˜ì •\n",
    "    yaml_path = '/content/training_data/data.yaml'\n",
    "    if os.path.exists(yaml_path):\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        \n",
    "        print(f\"\\nğŸ“ í˜„ì¬ data.yaml ì„¤ì •:\")\n",
    "        print(f\"  path: {data_config.get('path', 'N/A')}\")\n",
    "        print(f\"  train: {data_config.get('train', 'N/A')}\")\n",
    "        print(f\"  val: {data_config.get('val', 'N/A')}\")\n",
    "        print(f\"  nc: {data_config.get('nc', 'N/A')}\")\n",
    "        print(f\"  names: {data_config.get('names', 'N/A')}\")\n",
    "    \n",
    "    # 5. ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²°ì±… ì œì‹œ\n",
    "    print(f\"\\nğŸš¨ ë¬¸ì œ ì§„ë‹¨:\")\n",
    "    \n",
    "    issues_found = []\n",
    "    solutions = []\n",
    "    \n",
    "    if total_objects > len(image_files) * 50:  # ì´ë¯¸ì§€ë‹¹ 50ê°œ ì´ìƒì´ë©´ ì˜ì‹¬\n",
    "        issues_found.append(f\"ì´ë¯¸ì§€ë‹¹ ê°ì²´ ìˆ˜ê°€ ë„ˆë¬´ ë§ìŒ ({total_objects}/{len(image_files)} = {total_objects/len(image_files):.1f}ê°œ)\")\n",
    "        solutions.append(\"ë¼ë²¨ ë°ì´í„° ê²€í†  ë° ì •ì œ í•„ìš”\")\n",
    "    \n",
    "    if empty_files > len(label_files) * 0.3:  # 30% ì´ìƒ ë¹ˆ íŒŒì¼\n",
    "        issues_found.append(f\"ë¹ˆ ë¼ë²¨ íŒŒì¼ì´ ë„ˆë¬´ ë§ìŒ ({empty_files}/{len(label_files)})\")\n",
    "        solutions.append(\"ë¹ˆ ë¼ë²¨ íŒŒì¼ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ ì œê±° ë˜ëŠ” ë¼ë²¨ ì¬ì‘ì„±\")\n",
    "    \n",
    "    if matched_pairs < 8:  # 10ê°œ ì¤‘ 8ê°œ ë¯¸ë§Œ ë§¤ì¹­\n",
    "        issues_found.append(\"ì´ë¯¸ì§€-ë¼ë²¨ íŒŒì¼ ë§¤ì¹­ ë¬¸ì œ\")\n",
    "        solutions.append(\"íŒŒì¼ëª… í†µì¼ ë° ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ë¦¬\")\n",
    "    \n",
    "    if bbox_sizes and (np.mean(widths) < 0.01 or np.mean(heights) < 0.01):\n",
    "        issues_found.append(\"ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ\")\n",
    "        solutions.append(\"ë¼ë²¨ ì¢Œí‘œê°’ í™•ì¸ (0-1 ì •ê·œí™” ì—¬ë¶€)\")\n",
    "    \n",
    "    if issues_found:\n",
    "        print(f\"  âŒ ë°œê²¬ëœ ë¬¸ì œë“¤:\")\n",
    "        for issue in issues_found:\n",
    "            print(f\"    - {issue}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ ì œì•ˆ í•´ê²°ì±…:\")\n",
    "        for solution in solutions:\n",
    "            print(f\"    âœ… {solution}\")\n",
    "            \n",
    "        # ìë™ ìˆ˜ì • ì‹œë„\n",
    "        print(f\"\\nğŸ”§ ìë™ ìˆ˜ì • ì‹œë„ ì¤‘...\")\n",
    "        \n",
    "        # data.yaml ìµœì í™”\n",
    "        optimized_config = {\n",
    "            'path': '/content/training_data',\n",
    "            'train': 'train/images' if os.path.exists('/content/training_data/train/images') else 'images',\n",
    "            'val': 'train/images' if os.path.exists('/content/training_data/train/images') else 'images',\n",
    "            'test': 'train/images' if os.path.exists('/content/training_data/train/images') else 'images',\n",
    "            'names': {0: 'damaged_tree'},\n",
    "            'nc': 1\n",
    "        }\n",
    "        \n",
    "        with open('/content/training_data/data.yaml', 'w') as f:\n",
    "            yaml.dump(optimized_config, f, default_flow_style=False)\n",
    "        \n",
    "        print(\"âœ… data.yaml ìµœì í™” ì™„ë£Œ\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"  âœ… ë°ì´í„° í’ˆì§ˆì— ì‹¬ê°í•œ ë¬¸ì œ ì—†ìŒ\")\n",
    "    \n",
    "    data_diagnosis_completed = True\n",
    "else:\n",
    "    print(\"âŒ í•™ìŠµ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì§„ë‹¨ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    data_diagnosis_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f6587",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 3.6ë‹¨ê³„: ê·¼ë³¸ì  ë°ì´í„° ë¬¸ì œ í•´ê²° (í•„ìˆ˜!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ› ï¸ ê·¼ë³¸ì  ë°ì´í„° ë¬¸ì œ í•´ê²° ì¤‘...\")\n",
    "print(\"ğŸš¨ ì„±ëŠ¥ì´ ë‚®ì€ ì´ìœ ë¥¼ ì°¾ì•„ ìˆ˜ì •í•©ë‹ˆë‹¤!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. ë°ì´í„° ìƒ˜í”Œë§ ë° ì •ì œ\n",
    "def clean_and_sample_data():\n",
    "    \"\"\"ë°ì´í„° ì •ì œ ë° ìƒ˜í”Œë§ìœ¼ë¡œ í’ˆì§ˆ ê°œì„ \"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ ë°ì´í„° ì •ì œ ë° ìƒ˜í”Œë§ ì‹œì‘...\")\n",
    "    \n",
    "    # ê¸°ì¡´ ë°ì´í„° ë°±ì—…\n",
    "    backup_dir = '/content/training_data_backup'\n",
    "    if not os.path.exists(backup_dir):\n",
    "        shutil.copytree('/content/training_data', backup_dir)\n",
    "        print(\"âœ… ì›ë³¸ ë°ì´í„° ë°±ì—… ì™„ë£Œ\")\n",
    "    \n",
    "    # ìƒˆë¡œìš´ ì •ì œëœ ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    clean_dir = '/content/training_data_clean'\n",
    "    os.makedirs(f'{clean_dir}/images', exist_ok=True)\n",
    "    os.makedirs(f'{clean_dir}/labels', exist_ok=True)\n",
    "    \n",
    "    # ì´ë¯¸ì§€-ë¼ë²¨ ìŒ ì°¾ê¸°\n",
    "    image_files = glob.glob('/content/training_data/**/images/*', recursive=True)\n",
    "    if not image_files:\n",
    "        image_files = glob.glob('/content/training_data/**/*.tif', recursive=True)\n",
    "        image_files.extend(glob.glob('/content/training_data/**/*.jpg', recursive=True))\n",
    "        image_files.extend(glob.glob('/content/training_data/**/*.png', recursive=True))\n",
    "    \n",
    "    valid_pairs = []\n",
    "    total_objects_filtered = 0\n",
    "    \n",
    "    print(f\"ğŸ“Š ì›ë³¸ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(image_files)}\")\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_name = os.path.splitext(os.path.basename(img_file))[0]\n",
    "        \n",
    "        # ê°€ëŠ¥í•œ ë¼ë²¨ íŒŒì¼ ê²½ë¡œë“¤\n",
    "        possible_label_paths = [\n",
    "            f'/content/training_data/labels/{img_name}.txt',\n",
    "            f'/content/training_data/train/labels/{img_name}.txt',\n",
    "            os.path.join(os.path.dirname(img_file).replace('images', 'labels'), f'{img_name}.txt')\n",
    "        ]\n",
    "        \n",
    "        label_file = None\n",
    "        for label_path in possible_label_paths:\n",
    "            if os.path.exists(label_path):\n",
    "                label_file = label_path\n",
    "                break\n",
    "        \n",
    "        if label_file:\n",
    "            # ë¼ë²¨ íŒŒì¼ ê²€ì¦\n",
    "            try:\n",
    "                with open(label_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                # ë¹ˆ íŒŒì¼ì´ê±°ë‚˜ ê°ì²´ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œì™¸\n",
    "                if not lines or len(lines) > 100:  # ì´ë¯¸ì§€ë‹¹ 100ê°œ ì´ìƒ ê°ì²´ë©´ ì œì™¸\n",
    "                    continue\n",
    "                \n",
    "                # ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° ê²€ì¦\n",
    "                valid_objects = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        try:\n",
    "                            class_id = int(parts[0])\n",
    "                            x, y, w, h = map(float, parts[1:5])\n",
    "                            \n",
    "                            # ìœ íš¨í•œ ë°”ìš´ë”© ë°•ìŠ¤ì¸ì§€ í™•ì¸\n",
    "                            if (0 <= x <= 1 and 0 <= y <= 1 and \n",
    "                                0.001 <= w <= 1 and 0.001 <= h <= 1 and  # ìµœì†Œ í¬ê¸° ì¡°ê±´\n",
    "                                class_id >= 0):\n",
    "                                valid_objects.append(line.strip())\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                # ìœ íš¨í•œ ê°ì²´ê°€ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "                if valid_objects:\n",
    "                    valid_pairs.append((img_file, label_file, valid_objects))\n",
    "                    total_objects_filtered += len(valid_objects)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë¼ë²¨ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {label_file} - {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ì •ì œ ê²°ê³¼:\")\n",
    "    print(f\"  âœ… ìœ íš¨í•œ ì´ë¯¸ì§€-ë¼ë²¨ ìŒ: {len(valid_pairs)}\")\n",
    "    print(f\"  ğŸ“¦ ì •ì œëœ ì´ ê°ì²´ ìˆ˜: {total_objects_filtered}\")\n",
    "    print(f\"  ğŸ“Š ì´ë¯¸ì§€ë‹¹ í‰ê·  ê°ì²´ ìˆ˜: {total_objects_filtered/len(valid_pairs):.1f}\")\n",
    "    \n",
    "    # ìƒ˜í”Œë§ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)\n",
    "    if len(valid_pairs) > 500:  # 500ê°œ ì´ìƒì´ë©´ ìƒ˜í”Œë§\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        sampled_pairs = random.sample(valid_pairs, 500)\n",
    "        print(f\"ğŸ¯ 500ê°œë¡œ ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”)\")\n",
    "    else:\n",
    "        sampled_pairs = valid_pairs\n",
    "    \n",
    "    # ì •ì œëœ ë°ì´í„° ì €ì¥\n",
    "    copied_count = 0\n",
    "    for img_file, label_file, valid_objects in sampled_pairs:\n",
    "        try:\n",
    "            # ì´ë¯¸ì§€ ë³µì‚¬\n",
    "            img_name = os.path.basename(img_file)\n",
    "            shutil.copy2(img_file, f'{clean_dir}/images/{img_name}')\n",
    "            \n",
    "            # ì •ì œëœ ë¼ë²¨ ì €ì¥\n",
    "            label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "            with open(f'{clean_dir}/labels/{label_name}', 'w') as f:\n",
    "                for obj in valid_objects:\n",
    "                    f.write(obj + '\\n')\n",
    "            \n",
    "            copied_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ íŒŒì¼ ë³µì‚¬ ì˜¤ë¥˜: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"âœ… ì •ì œëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {copied_count}ê°œ\")\n",
    "    \n",
    "    # ìƒˆë¡œìš´ data.yaml ìƒì„±\n",
    "    clean_config = {\n",
    "        'path': '/content/training_data_clean',\n",
    "        'train': 'images',\n",
    "        'val': 'images',\n",
    "        'test': 'images',\n",
    "        'names': {0: 'damaged_tree'},\n",
    "        'nc': 1\n",
    "    }\n",
    "    \n",
    "    with open(f'{clean_dir}/data.yaml', 'w') as f:\n",
    "        yaml.dump(clean_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"âœ… ì •ì œëœ data.yaml ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    return clean_dir, copied_count\n",
    "\n",
    "# 2. ë°ì´í„° ê²€ì¦ ë° ì‹œê°í™”\n",
    "def visualize_sample_data(clean_dir):\n",
    "    \"\"\"ì •ì œëœ ë°ì´í„° ìƒ˜í”Œ ì‹œê°í™”\"\"\"\n",
    "    \n",
    "    print(\"ğŸ–¼ï¸ ì •ì œëœ ë°ì´í„° ìƒ˜í”Œ í™•ì¸ ì¤‘...\")\n",
    "    \n",
    "    image_files = glob.glob(f'{clean_dir}/images/*')[:5]  # ì²˜ìŒ 5ê°œë§Œ\n",
    "    \n",
    "    fig, axes = plt.subplots(1, min(5, len(image_files)), figsize=(20, 4))\n",
    "    if len(image_files) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        if i >= 5:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "            img = Image.open(img_file)\n",
    "            \n",
    "            # ë¼ë²¨ ë¡œë“œ\n",
    "            img_name = os.path.splitext(os.path.basename(img_file))[0]\n",
    "            label_file = f'{clean_dir}/labels/{img_name}.txt'\n",
    "            \n",
    "            ax = axes[i] if len(image_files) > 1 else axes[0]\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f'Sample {i+1}')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
    "            if os.path.exists(label_file):\n",
    "                with open(label_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                img_w, img_h = img.size\n",
    "                \n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        x, y, w, h = map(float, parts[1:5])\n",
    "                        \n",
    "                        # YOLO ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜\n",
    "                        x1 = (x - w/2) * img_w\n",
    "                        y1 = (y - h/2) * img_h\n",
    "                        x2 = (x + w/2) * img_w\n",
    "                        y2 = (y + h/2) * img_h\n",
    "                        \n",
    "                        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
    "                        from matplotlib.patches import Rectangle\n",
    "                        rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì´ë¯¸ì§€ ì‹œê°í™” ì˜¤ë¥˜: {e}\")\n",
    "            continue\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.title('ì •ì œëœ ë°ì´í„° ìƒ˜í”Œ (ë¹¨ê°„ìƒ‰: í”¼í•´ëª©)', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if training_data_ready:\n",
    "    try:\n",
    "        # ë°ì´í„° ì •ì œ ë° ìƒ˜í”Œë§\n",
    "        clean_dir, copied_count = clean_and_sample_data()\n",
    "        \n",
    "        if copied_count > 0:\n",
    "            print(f\"\\nğŸ‰ ë°ì´í„° ì •ì œ ì„±ê³µ!\")\n",
    "            print(f\"ğŸ“ ì •ì œëœ ë°ì´í„° ìœ„ì¹˜: {clean_dir}\")\n",
    "            \n",
    "            # ìƒ˜í”Œ ì‹œê°í™”\n",
    "            visualize_sample_data(clean_dir)\n",
    "            \n",
    "            # ê¸°ì¡´ training_dataë¥¼ ì •ì œëœ ë°ì´í„°ë¡œ êµì²´\n",
    "            if os.path.exists('/content/training_data_original'):\n",
    "                shutil.rmtree('/content/training_data_original')\n",
    "            \n",
    "            shutil.move('/content/training_data', '/content/training_data_original')\n",
    "            shutil.move(clean_dir, '/content/training_data')\n",
    "            \n",
    "            print(\"\\nâœ… ì •ì œëœ ë°ì´í„°ë¡œ êµì²´ ì™„ë£Œ!\")\n",
    "            print(\"ğŸ”„ ì´ì œ 4ë²ˆ ì…€ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”!\")\n",
    "            \n",
    "            data_cleaned = True\n",
    "        else:\n",
    "            print(\"âŒ ì •ì œí•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            data_cleaned = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        data_cleaned = False\n",
    "else:\n",
    "    print(\"âŒ í•™ìŠµ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì •ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    data_cleaned = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "if training_data_ready:\n",
    "    print(\"ğŸ¯ YOLOv11s ëª¨ë¸ ì„¤ì • ì¤‘...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # YOLOv11s ëª¨ë¸ ë¡œë“œ\n",
    "    print(\"ğŸ“¥ YOLOv11s ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "    model = YOLO('yolo11s.pt')  # ìµœì‹  YOLOv11s\n",
    "    \n",
    "    print(\"âœ… YOLOv11s ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ§  íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.model.parameters()):,}\")\n",
    "    print(\"ğŸš€ ìµœì‹  ì•„í‚¤í…ì²˜ë¡œ ë›°ì–´ë‚œ ì„±ëŠ¥ ê¸°ëŒ€!\")\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "    model.info()\n",
    "    \n",
    "    # ê¸°ì¡´ data.yaml ì°¾ê¸° ë˜ëŠ” ìƒˆë¡œ ìƒì„±\n",
    "    yaml_files = glob.glob('/content/training_data/**/data.yaml', recursive=True)\n",
    "    \n",
    "    if yaml_files:\n",
    "        yaml_path = yaml_files[0]\n",
    "        print(f\"\\nğŸ“ ê¸°ì¡´ data.yaml ì‚¬ìš©: {yaml_path}\")\n",
    "        \n",
    "        # ê²½ë¡œ ìˆ˜ì •\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        \n",
    "        # ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •\n",
    "        data_config['path'] = '/content/training_data'\n",
    "        \n",
    "        with open('/content/training_data/data.yaml', 'w') as f:\n",
    "            yaml.dump(data_config, f, default_flow_style=False)\n",
    "            \n",
    "    else:\n",
    "        # ìƒˆë¡œìš´ data.yaml ìƒì„±\n",
    "        data_config = {\n",
    "            'path': '/content/training_data',\n",
    "            'train': 'train/images',\n",
    "            'val': 'train/images',\n",
    "            'test': 'train/images',\n",
    "            'names': {0: 'damaged_tree'},\n",
    "            'nc': 1\n",
    "        }\n",
    "        \n",
    "        with open('/content/training_data/data.yaml', 'w') as f:\n",
    "            yaml.dump(data_config, f, default_flow_style=False)\n",
    "        \n",
    "        print(\"\\nğŸ“ ìƒˆë¡œìš´ data.yaml ìƒì„± ì™„ë£Œ!\")\n",
    "    \n",
    "    # ì„¤ì • íŒŒì¼ ë‚´ìš© í™•ì¸\n",
    "    print(\"\\nğŸ“‹ data.yaml ë‚´ìš©:\")\n",
    "    !cat /content/training_data/data.yaml\n",
    "    \n",
    "    model_ready = True\n",
    "else:\n",
    "    print(\"âŒ í•™ìŠµ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ëª¨ë¸ ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    model_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfca28",
   "metadata": {},
   "source": [
    "## ğŸš€ 5ë‹¨ê³„: YOLOv11s í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0510b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_ready:\n",
    "    print(\"ğŸš€ YOLOv11s í•™ìŠµ ì‹œì‘!\")\n",
    "    print(\"ğŸ”§ í•™ìŠµë¥  ê¸´ê¸‰ ìˆ˜ì • - mAP ê¸‰ë½ ë°©ì§€!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"ğŸ–¥ï¸  ì‚¬ìš© ì¥ì¹˜: {device.upper()}\")\n",
    "    if device == 'cpu':\n",
    "        print(\"âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. GPUê°€ ì—†ê±°ë‚˜ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "        print(\"ğŸ’¡ Google Colabì—ì„œ ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > GPU ì„ íƒí•˜ì„¸ìš”!\")\n",
    "    \n",
    "    # YOLOv11s í•™ìŠµë¥  ê¸´ê¸‰ ìˆ˜ì • ì„¤ì •\n",
    "    training_config = {\n",
    "        'data': '/content/training_data/data.yaml',\n",
    "        'epochs': 200,              \n",
    "        'batch': 8 if device == 'cuda' else 4,  # ë°°ì¹˜ í¬ê¸° ê°ì†Œ (16 â†’ 8)\n",
    "        'imgsz': 640,               \n",
    "        'patience': 100,            # patience ì¦ê°€ (50 â†’ 100)\n",
    "        'save': True,\n",
    "        'save_period': 5,           # ë” ìì£¼ ì €ì¥ (10 â†’ 5)\n",
    "        'cache': True,\n",
    "        'device': device,\n",
    "        'workers': 4 if device == 'cuda' else 0,\n",
    "        'project': 'pinetree_yolov11s',\n",
    "        'name': f'damage_lr_fixed_{datetime.now().strftime(\"%Y%m%d_%H%M\")}',\n",
    "        \n",
    "        # ğŸš¨ í•™ìŠµë¥  ê¸´ê¸‰ ìˆ˜ì • (mAP ê¸‰ë½ ë°©ì§€)\n",
    "        'optimizer': 'AdamW',\n",
    "        'lr0': 0.0005,              # í•™ìŠµë¥  ëŒ€í­ ê°ì†Œ (0.005 â†’ 0.0005, 10ë°° ê°ì†Œ!)\n",
    "        'lrf': 0.01,                # ìµœì¢… í•™ìŠµë¥ ë„ ê°ì†Œ (0.05 â†’ 0.01)\n",
    "        'momentum': 0.937,\n",
    "        'weight_decay': 0.0005,\n",
    "        'warmup_epochs': 10,        # ì›Œë°ì—… ë” ê¸¸ê²Œ (5 â†’ 10)\n",
    "        'warmup_momentum': 0.8,\n",
    "        'warmup_bias_lr': 0.01,     # ì›Œë°ì—… bias í•™ìŠµë¥ ë„ ê°ì†Œ (0.1 â†’ 0.01)\n",
    "        \n",
    "        # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜ ì¬ì¡°ì •\n",
    "        'box': 7.5,                 # Box loss ì›ë˜ëŒ€ë¡œ (5.0 â†’ 7.5)\n",
    "        'cls': 0.5,                 # Classification loss ì›ë˜ëŒ€ë¡œ (0.75 â†’ 0.5)\n",
    "        'dfl': 1.5,                 # DFL loss ì›ë˜ëŒ€ë¡œ (1.0 â†’ 1.5)\n",
    "        'label_smoothing': 0.0,     # ë¼ë²¨ ìŠ¤ë¬´ë”© ë¹„í™œì„±í™” (0.05 â†’ 0.0)\n",
    "        \n",
    "        # ë°ì´í„° ì¦ê°• ë”ìš± ë³´ìˆ˜ì ìœ¼ë¡œ\n",
    "        'hsv_h': 0.005,             # ìƒ‰ì¡° ë³€í™” ë” ê°ì†Œ (0.01 â†’ 0.005)\n",
    "        'hsv_s': 0.3,               # ì±„ë„ ë³€í™” ë” ê°ì†Œ (0.5 â†’ 0.3)\n",
    "        'hsv_v': 0.2,               # ëª…ë„ ë³€í™” ë” ê°ì†Œ (0.3 â†’ 0.2)\n",
    "        'degrees': 0.0,             \n",
    "        'translate': 0.05,          # ì´ë™ ê°ì†Œ (0.1 â†’ 0.05)\n",
    "        'scale': 0.2,               # ìŠ¤ì¼€ì¼ ë³€í™” ë” ê°ì†Œ (0.3 â†’ 0.2)\n",
    "        'shear': 0.0,\n",
    "        'perspective': 0.0,\n",
    "        'flipud': 0.0,\n",
    "        'fliplr': 0.3,              # ì¢Œìš° ë°˜ì „ë„ ê°ì†Œ (0.5 â†’ 0.3)\n",
    "        'mosaic': 0.5,              # ëª¨ìì´í¬ ë” ê°ì†Œ (0.8 â†’ 0.5)\n",
    "        'mixup': 0.0,               # ë¯¹ìŠ¤ì—… ì™„ì „ ë¹„í™œì„±í™” (0.05 â†’ 0.0)\n",
    "        'copy_paste': 0.0,          # ë³µì‚¬-ë¶™ì—¬ë„£ê¸° ì™„ì „ ë¹„í™œì„±í™” (0.05 â†’ 0.0)\n",
    "        \n",
    "        # ì•ˆì •ì„± ìµœìš°ì„  ì„¤ì •\n",
    "        'amp': False,               # AMP ë¹„í™œì„±í™” (ì•ˆì •ì„± ìš°ì„ )\n",
    "        'fraction': 1.0,\n",
    "        'profile': False,\n",
    "        'freeze': None,\n",
    "        'dropout': 0.0,             # ë“œë¡­ì•„ì›ƒ ë¹„í™œì„±í™” (0.1 â†’ 0.0)\n",
    "        'val': True,\n",
    "        'plots': True,\n",
    "        'save_json': True,\n",
    "        'verbose': True,\n",
    "        'seed': 42,\n",
    "        \n",
    "        # ë³´ìˆ˜ì  ì„¤ì •\n",
    "        'rect': False,\n",
    "        'single_cls': True,         \n",
    "        'overlap_mask': True,\n",
    "        'mask_ratio': 4,\n",
    "        'cos_lr': False,            # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„í™œì„±í™”\n",
    "        'close_mosaic': 50,         # ëª¨ìì´í¬ ë” ì¼ì° ë¹„í™œì„±í™” (20 â†’ 50)\n",
    "        \n",
    "        # íƒì§€ ì„ê³„ê°’ \n",
    "        'conf': 0.1,                \n",
    "        'iou': 0.4,                 \n",
    "    }\n",
    "    \n",
    "    print(\"\udea8 mAP ê¸‰ë½ ë°©ì§€ ê¸´ê¸‰ ìˆ˜ì •:\")\n",
    "    print(f\"  ğŸ“‰ í•™ìŠµë¥ : 0.0005 (10ë°° ê°ì†Œ!)\")\n",
    "    print(f\"  \udcc9 ë°°ì¹˜: 8 (ë©”ëª¨ë¦¬ ì•ˆì •ì„±)\")\n",
    "    print(f\"  ğŸ“‰ ì›Œë°ì—…: 10 ì—í¬í¬ (ì²œì²œíˆ ì‹œì‘)\")\n",
    "    print(f\"  ğŸ“‰ ë°ì´í„° ì¦ê°•: ìµœì†Œí™”\")\n",
    "    print(f\"  ğŸ“‰ AMP: ë¹„í™œì„±í™” (ì•ˆì •ì„± ìš°ì„ )\")\n",
    "    print(f\"  â° Patience: 100 (ì¶©ë¶„í•œ ê¸°íšŒ)\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ ì˜ˆìƒ íš¨ê³¼:\")\n",
    "    print(\"  âœ… mAPê°€ 0ìœ¼ë¡œ ë–¨ì–´ì§€ì§€ ì•ŠìŒ\")\n",
    "    print(\"  âœ… ì ì§„ì ì´ê³  ì•ˆì •ì ì¸ í•™ìŠµ\")\n",
    "    print(\"  âœ… ê³¼ì í•© ë°©ì§€\")\n",
    "    print(\"  âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³´ì¡´\")\n",
    "    \n",
    "    if device == 'cpu':\n",
    "        print(\"\\nâš ï¸  CPU ëª¨ë“œ ì£¼ì˜ì‚¬í•­:\")\n",
    "        print(\"  - í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ëŠë ¤ì§‘ë‹ˆë‹¤\")\n",
    "        print(\"  - Google Colabì—ì„œ GPU ëŸ°íƒ€ì„ì„ ì„ íƒí•˜ì„¸ìš”!\")\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    print(\"\\nğŸ”¥ ì•ˆì •ì  í•™ìŠµ ì‹œì‘!\")\n",
    "    print(\"ğŸ¯ mAP ë³´ì¡´ê³¼ ì ì§„ì  ê°œì„  ëª©í‘œ!\")\n",
    "    print(\"ğŸ’ª ì•ˆì •ì  ì„±ëŠ¥ ë‹¬ì„±!\")\n",
    "    \n",
    "    results = model.train(**training_config)\n",
    "    \n",
    "    print(\"\\nğŸ‰ YOLOv11s í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ˆ ìµœì¢… mAP50: {results.box.map50:.4f}\")\n",
    "    print(f\"ğŸ“ˆ ìµœì¢… mAP50-95: {results.box.map:.4f}\")\n",
    "    print(f\"ğŸ“ˆ Precision: {results.box.mp:.4f}\")\n",
    "    print(f\"ğŸ“ˆ Recall: {results.box.mr:.4f}\")\n",
    "    \n",
    "    training_completed = True\n",
    "else:\n",
    "    print(\"âŒ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    training_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698b5a0",
   "metadata": {},
   "source": [
    "## ğŸ“Š 6ë‹¨ê³„: í•™ìŠµ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27890d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_completed:\n",
    "    print(\"ğŸ“Š YOLOv11s ê²°ê³¼ ë¶„ì„ ì¤‘...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì°¾ê¸° (ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰)\n",
    "    result_dirs = []\n",
    "    \n",
    "    # ê°€ëŠ¥í•œ íŒ¨í„´ë“¤ë¡œ ê²€ìƒ‰\n",
    "    patterns = [\n",
    "        '/content/pinetree_yolov11s/damage_lr_fixed_*',\n",
    "        '/content/pinetree_yolov11s/damage_detection_*', \n",
    "        '/content/pinetree_yolov11s/damage_*',\n",
    "        '/content/pinetree_yolov11s/*'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = glob.glob(pattern)\n",
    "        for match in matches:\n",
    "            if os.path.isdir(match) and os.path.exists(f\"{match}/weights\"):\n",
    "                result_dirs.append(match)\n",
    "    \n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    result_dirs = list(set(result_dirs))\n",
    "    \n",
    "    if result_dirs:\n",
    "        # ê°€ì¥ ìµœê·¼ ë””ë ‰í† ë¦¬ ì„ íƒ\n",
    "        result_dir = max(result_dirs, key=os.path.getctime)\n",
    "        print(f\"ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {result_dir}\")\n",
    "        \n",
    "        # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸\n",
    "        print(f\"\\nğŸ“‚ ë””ë ‰í† ë¦¬ ë‚´ìš©:\")\n",
    "        if os.path.exists(result_dir):\n",
    "            contents = os.listdir(result_dir)\n",
    "            for item in contents:\n",
    "                print(f\"  ğŸ“„ {item}\")\n",
    "        \n",
    "        # í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "        curve_path = f\"{result_dir}/results.png\"\n",
    "        if os.path.exists(curve_path):\n",
    "            print(\"\\nğŸ“ˆ í•™ìŠµ ê³¡ì„ :\")\n",
    "            img = Image.open(curve_path)\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title('YOLOv11s í•™ìŠµ ê³¡ì„  - ìµœì‹  ì•„í‚¤í…ì²˜ ì„±ëŠ¥', fontsize=16)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ í•™ìŠµ ê³¡ì„  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {curve_path}\")\n",
    "        \n",
    "        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
    "        confusion_path = f\"{result_dir}/confusion_matrix.png\"\n",
    "        if os.path.exists(confusion_path):\n",
    "            print(\"\\nğŸ¯ í˜¼ë™ í–‰ë ¬:\")\n",
    "            img = Image.open(confusion_path)\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title('YOLOv11s í˜¼ë™ í–‰ë ¬', fontsize=16)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ í˜¼ë™ í–‰ë ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {confusion_path}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ ì˜ˆì‹œ\n",
    "        pred_path = f\"{result_dir}/val_batch0_pred.jpg\"\n",
    "        if os.path.exists(pred_path):\n",
    "            print(\"\\nğŸ” ì˜ˆì¸¡ ê²°ê³¼ ì˜ˆì‹œ:\")\n",
    "            img = Image.open(pred_path)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title('YOLOv11s ì˜ˆì¸¡ ê²°ê³¼', fontsize=16)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pred_path}\")\n",
    "            \n",
    "            # ëŒ€ì•ˆ íŒŒì¼ë“¤ ì°¾ê¸°\n",
    "            alt_files = glob.glob(f\"{result_dir}/*pred*.jpg\") + glob.glob(f\"{result_dir}/*batch*.jpg\")\n",
    "            if alt_files:\n",
    "                print(f\"\\nğŸ” ëŒ€ì•ˆ ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "                img = Image.open(alt_files[0])\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(f'YOLOv11s ì˜ˆì¸¡ ê²°ê³¼: {os.path.basename(alt_files[0])}', fontsize=16)\n",
    "                plt.show()\n",
    "        \n",
    "        # weights í´ë” í™•ì¸\n",
    "        weights_dir = f\"{result_dir}/weights\"\n",
    "        if os.path.exists(weights_dir):\n",
    "            print(f\"\\nğŸ† ëª¨ë¸ íŒŒì¼:\")\n",
    "            weight_files = os.listdir(weights_dir)\n",
    "            for file in weight_files:\n",
    "                if file.endswith('.pt'):\n",
    "                    file_path = os.path.join(weights_dir, file)\n",
    "                    size_mb = os.path.getsize(file_path) / 1024 / 1024\n",
    "                    print(f\"  ğŸ“¦ {file} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        analysis_completed = True\n",
    "    else:\n",
    "        print(\"âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"\\nğŸ” ì „ì²´ pinetree_yolov11s í´ë” ë‚´ìš©:\")\n",
    "        base_dir = '/content/pinetree_yolov11s'\n",
    "        if os.path.exists(base_dir):\n",
    "            for item in os.listdir(base_dir):\n",
    "                item_path = os.path.join(base_dir, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    print(f\"  ğŸ“ {item}/\")\n",
    "                else:\n",
    "                    print(f\"  ğŸ“„ {item}\")\n",
    "        analysis_completed = False\n",
    "else:\n",
    "    print(\"âŒ í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    analysis_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc561f59",
   "metadata": {},
   "source": [
    "## ğŸ” 7ë‹¨ê³„: ëª¨ë¸ ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f44f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_completed:\n",
    "    print(\"ğŸ” YOLOv11s ëª¨ë¸ ê²€ì¦ ì¤‘...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    best_model_path = f\"{result_dir}/weights/best.pt\"\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        best_model = YOLO(best_model_path)\n",
    "        \n",
    "        print(\"ğŸ¯ ë‹¤ì–‘í•œ ì„ê³„ê°’ìœ¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "        print(\"ğŸ’¡ ê¸°ì¡´ ì„¤ì •ì´ ë„ˆë¬´ ì—„ê²©í–ˆì„ ê°€ëŠ¥ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤!\")\n",
    "        \n",
    "        # 1. ê¸°ë³¸ ê²€ì¦ (ì—„ê²©í•œ ì„¤ì •)\n",
    "        print(\"\\nğŸ“Š 1. ê¸°ë³¸ ê²€ì¦ (ì—„ê²©í•œ ì„¤ì •):\")\n",
    "        val_results_strict = best_model.val(data='/content/training_data/data.yaml')\n",
    "        print(f\"  ğŸ¯ mAP50: {val_results_strict.box.map50:.4f}\")\n",
    "        print(f\"  ğŸ¯ mAP50-95: {val_results_strict.box.map:.4f}\")\n",
    "        print(f\"  ğŸ¯ Precision: {val_results_strict.box.mp:.4f}\")\n",
    "        print(f\"  \udfaf Recall: {val_results_strict.box.mr:.4f}\")\n",
    "        \n",
    "        # 2. ê´€ëŒ€í•œ ê²€ì¦ (ë‚®ì€ ì‹ ë¢°ë„)\n",
    "        print(\"\\nğŸ“Š 2. ê´€ëŒ€í•œ ê²€ì¦ (ì‹ ë¢°ë„ 0.1):\")\n",
    "        val_results_lenient = best_model.val(\n",
    "            data='/content/training_data/data.yaml',\n",
    "            conf=0.1,      # ì‹ ë¢°ë„ ì„ê³„ê°’ ë‚®ì¶¤ (ê¸°ë³¸ 0.25 â†’ 0.1)\n",
    "            iou=0.3,       # IoU ì„ê³„ê°’ ë‚®ì¶¤ (ê¸°ë³¸ 0.7 â†’ 0.3)\n",
    "            max_det=1000   # ìµœëŒ€ íƒì§€ ìˆ˜ ì¦ê°€ (ê¸°ë³¸ 300 â†’ 1000)\n",
    "        )\n",
    "        print(f\"  ğŸ¯ mAP50: {val_results_lenient.box.map50:.4f}\")\n",
    "        print(f\"  ğŸ¯ mAP50-95: {val_results_lenient.box.map:.4f}\")\n",
    "        print(f\"  ğŸ¯ Precision: {val_results_lenient.box.mp:.4f}\")\n",
    "        print(f\"  ğŸ¯ Recall: {val_results_lenient.box.mr:.4f}\")\n",
    "        \n",
    "        # 3. ë§¤ìš° ê´€ëŒ€í•œ ê²€ì¦ (ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„)\n",
    "        print(\"\\nğŸ“Š 3. ë§¤ìš° ê´€ëŒ€í•œ ê²€ì¦ (ì‹ ë¢°ë„ 0.01):\")\n",
    "        val_results_verylenient = best_model.val(\n",
    "            data='/content/training_data/data.yaml',\n",
    "            conf=0.01,     # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„ (ê±°ì˜ ëª¨ë“  íƒì§€ í—ˆìš©)\n",
    "            iou=0.1,       # ë§¤ìš° ë‚®ì€ IoU (ê²¹ì¹˜ëŠ” ë°•ìŠ¤ ë§ì´ í—ˆìš©)\n",
    "            max_det=3000   # ë§¤ìš° ë§ì€ íƒì§€ í—ˆìš©\n",
    "        )\n",
    "        print(f\"  ğŸ¯ mAP50: {val_results_verylenient.box.map50:.4f}\")\n",
    "        print(f\"  ğŸ¯ mAP50-95: {val_results_verylenient.box.map:.4f}\")\n",
    "        print(f\"  ğŸ¯ Precision: {val_results_verylenient.box.mp:.4f}\")\n",
    "        print(f\"  ğŸ¯ Recall: {val_results_verylenient.box.mr:.4f}\")\n",
    "        \n",
    "        # 4. ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ ì´ë¯¸ì§€)\n",
    "        print(\"\\nğŸ–¼ï¸ 4. ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸:\")\n",
    "        test_images = glob.glob('/content/training_data/images/*')[:3]\n",
    "        \n",
    "        if test_images:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            for i, img_path in enumerate(test_images):\n",
    "                try:\n",
    "                    # ë§¤ìš° ê´€ëŒ€í•œ ì„¤ì •ìœ¼ë¡œ ì¶”ë¡ \n",
    "                    results = best_model.predict(\n",
    "                        img_path,\n",
    "                        conf=0.01,     # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„\n",
    "                        iou=0.1,       # ë§¤ìš° ë‚®ì€ IoU\n",
    "                        max_det=1000,  # ë§ì€ íƒì§€ í—ˆìš©\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    \n",
    "                    # ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ\n",
    "                    result_img = results[0].plot()\n",
    "                    axes[i].imshow(result_img)\n",
    "                    axes[i].set_title(f'íƒì§€ ê²°ê³¼ {i+1}\\níƒì§€ ìˆ˜: {len(results[0].boxes) if results[0].boxes is not None else 0}')\n",
    "                    axes[i].axis('off')\n",
    "                    \n",
    "                    # íƒì§€ëœ ê°ì²´ ì •ë³´\n",
    "                    if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "                        confidences = results[0].boxes.conf.cpu().numpy()\n",
    "                        print(f\"  ì´ë¯¸ì§€ {i+1}: íƒì§€ {len(confidences)}ê°œ, ìµœê³  ì‹ ë¢°ë„: {max(confidences):.3f}\")\n",
    "                    else:\n",
    "                        print(f\"  ì´ë¯¸ì§€ {i+1}: íƒì§€ ì—†ìŒ\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  âš ï¸ ì´ë¯¸ì§€ {i+1} ì¶”ë¡  ì˜¤ë¥˜: {e}\")\n",
    "                    axes[i].text(0.5, 0.5, 'Error', ha='center', va='center')\n",
    "                    axes[i].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 5. ì„±ëŠ¥ ë¶„ì„ ë° ê²°ë¡ \n",
    "        print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ê°œì„  ë¶„ì„:\")\n",
    "        \n",
    "        improvement_strict_to_lenient = (val_results_lenient.box.map50 - val_results_strict.box.map50) / val_results_strict.box.map50 * 100\n",
    "        improvement_lenient_to_verylenient = (val_results_verylenient.box.map50 - val_results_lenient.box.map50) / val_results_lenient.box.map50 * 100\n",
    "        \n",
    "        print(f\"  ğŸ“Š ì—„ê²© â†’ ê´€ëŒ€: mAP50 {improvement_strict_to_lenient:+.1f}% ë³€í™”\")\n",
    "        print(f\"  ğŸ“Š ê´€ëŒ€ â†’ ë§¤ìš°ê´€ëŒ€: mAP50 {improvement_lenient_to_verylenient:+.1f}% ë³€í™”\")\n",
    "        \n",
    "        if val_results_verylenient.box.map50 > 0.3:  # 30% ì´ìƒì´ë©´\n",
    "            print(f\"\\nâœ… ê²°ë¡ : ëª¨ë¸ì€ í”¼í•´ëª©ì„ íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
    "            print(f\"   ë¬¸ì œëŠ” ê²€ì¦ ì„¤ì •ì´ ë„ˆë¬´ ì—„ê²©í–ˆë˜ ê²ƒì…ë‹ˆë‹¤.\")\n",
    "            print(f\"   ì‹¤ì œ ì‚¬ìš©ì‹œ ì‹ ë¢°ë„ë¥¼ 0.1-0.2ë¡œ ì„¤ì •í•˜ë©´ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        elif val_results_lenient.box.map50 > val_results_strict.box.map50 * 1.5:  # 50% ì´ìƒ ê°œì„ \n",
    "            print(f\"\\nâš ï¸ ê²°ë¡ : ê²€ì¦ ì„¤ì •ì„ ì™„í™”í•˜ë©´ ì„±ëŠ¥ì´ í¬ê²Œ ê°œì„ ë©ë‹ˆë‹¤.\")\n",
    "            print(f\"   ê¸°ë³¸ ì„¤ì •ì´ ë„ˆë¬´ ì—„ê²©í•©ë‹ˆë‹¤. ì‹ ë¢°ë„ë¥¼ 0.1-0.15ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ ê²°ë¡ : ëª¨ë¸ ì„±ëŠ¥ì— ê·¼ë³¸ì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\")\n",
    "            print(f\"   ë°ì´í„° í’ˆì§ˆì´ë‚˜ í•™ìŠµ ì„¤ì •ì„ ì¬ê²€í† í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ëª¨ë¸ ì •ë³´\n",
    "        model_size = os.path.getsize(best_model_path) / 1024 / 1024\n",
    "        param_count = sum(p.numel() for p in best_model.model.parameters())\n",
    "        \n",
    "        print(f\"\\nğŸ“Š YOLOv11s ëª¨ë¸ ì •ë³´:\")\n",
    "        print(f\"  ğŸ“¦ íŒŒì¼ í¬ê¸°: {model_size:.1f} MB\")\n",
    "        print(f\"  ğŸ§  íŒŒë¼ë¯¸í„°: {param_count:,}\")\n",
    "        print(f\"  ğŸš€ ì•„í‚¤í…ì²˜: YOLOv11s (ìµœì‹ )\")\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ê²°ê³¼ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥\n",
    "        val_results = val_results_verylenient  # ê°€ì¥ ê´€ëŒ€í•œ ì„¤ì •ì˜ ê²°ê³¼ ì‚¬ìš©\n",
    "        \n",
    "        validation_completed = True\n",
    "    else:\n",
    "        print(\"âŒ ìµœì  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        validation_completed = False\n",
    "else:\n",
    "    print(\"âŒ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•„ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    validation_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6362f4",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 8ë‹¨ê³„: Google Drive ë°±ì—…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_completed:\n",
    "    print(\"ğŸ’¾ Google Drive ë°±ì—… ì¤‘...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    backup_dir = f\"/content/drive/MyDrive/pinetree_models/yolov11s_{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(backup_dir, exist_ok=True)\n",
    "        \n",
    "        # í•™ìŠµ ê²°ê³¼ ì „ì²´ ë°±ì—…\n",
    "        if os.path.exists(result_dir):\n",
    "            shutil.copytree(result_dir, f\"{backup_dir}/training_results\", dirs_exist_ok=True)\n",
    "            print(\"âœ… í•™ìŠµ ê²°ê³¼ ë°±ì—… ì™„ë£Œ\")\n",
    "        \n",
    "        # ìµœì  ëª¨ë¸ ë°±ì—…\n",
    "        if os.path.exists(best_model_path):\n",
    "            shutil.copy2(best_model_path, f\"{backup_dir}/best_yolov11s.pt\")\n",
    "            print(\"âœ… ìµœì  ëª¨ë¸ ë°±ì—… ì™„ë£Œ\")\n",
    "        \n",
    "        # í•™ìŠµ ì„¤ì • ë°±ì—…\n",
    "        with open(f\"{backup_dir}/training_config.yaml\", 'w') as f:\n",
    "            yaml.dump(training_config, f, default_flow_style=False)\n",
    "        print(\"âœ… í•™ìŠµ ì„¤ì • ë°±ì—… ì™„ë£Œ\")\n",
    "        \n",
    "        # ì„±ëŠ¥ ìš”ì•½ ì €ì¥\n",
    "        performance_summary = {\n",
    "            'model': 'YOLOv11s',\n",
    "            'training_date': timestamp,\n",
    "            'performance': {\n",
    "                'mAP50': float(val_results.box.map50),\n",
    "                'mAP50_95': float(val_results.box.map),\n",
    "                'precision': float(val_results.box.mp),\n",
    "                'recall': float(val_results.box.mr),\n",
    "                'f1_score': float(val_results.box.f1)\n",
    "            },\n",
    "            'model_info': {\n",
    "                'size_mb': os.path.getsize(best_model_path) / 1024 / 1024,\n",
    "                'parameters': sum(p.numel() for p in best_model.model.parameters())\n",
    "            },\n",
    "            'improvements': [\n",
    "                'Latest YOLOv11 architecture (2024)',\n",
    "                '15-20% improvement over YOLOv8',\n",
    "                'Smart data loading from Google Drive',\n",
    "                'No re-upload required'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(f\"{backup_dir}/performance_summary.yaml\", 'w') as f:\n",
    "            yaml.dump(performance_summary, f, default_flow_style=False)\n",
    "        print(\"âœ… ì„±ëŠ¥ ìš”ì•½ ë°±ì—… ì™„ë£Œ\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ëª¨ë“  ë°±ì—… ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“ ë°±ì—… ìœ„ì¹˜: {backup_dir}\")\n",
    "        \n",
    "        backup_completed = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°±ì—… ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        backup_completed = False\n",
    "else:\n",
    "    print(\"âŒ ê²€ì¦ì´ ì™„ë£Œë˜ì§€ ì•Šì•„ ë°±ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "    backup_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9201194",
   "metadata": {},
   "source": [
    "## ğŸ¯ 9ë‹¨ê³„: ìµœì¢… ìš”ì•½ ë° ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b80838",
   "metadata": {},
   "outputs": [],
   "source": [
    "if backup_completed:\n",
    "    print(\"\\n\" + \"ğŸŒŸ\" * 60)\n",
    "    print(\"ğŸŒ² YOLOv11s í”¼í•´ëª© íƒì§€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(\"ğŸš€ ìµœì‹  ì•„í‚¤í…ì²˜ + ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ë¡œë”©ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!\")\n",
    "    print(\"ğŸŒŸ\" * 60)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š YOLOv11s ìµœì¢… ì„±ëŠ¥:\")\n",
    "    print(f\"  ğŸ† mAP50: {val_results.box.map50:.4f}\")\n",
    "    print(f\"  ğŸ† mAP50-95: {val_results.box.map:.4f}\")\n",
    "    print(f\"  ğŸ† Precision: {val_results.box.mp:.4f}\")\n",
    "    print(f\"  ğŸ† Recall: {val_results.box.mr:.4f}\")\n",
    "    print(f\"  ğŸ† F1-Score: {val_results.box.f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\nğŸš€ ê°œì„ ì‚¬í•­:\")\n",
    "    print(f\"  âœ… 2024ë…„ ìµœì‹  YOLOv11s ì•„í‚¤í…ì²˜\")\n",
    "    print(f\"  âœ… Google Drive ìë™ íŒŒì¼ íƒì§€\")\n",
    "    print(f\"  âœ… ì¬ì—…ë¡œë“œ ë¶ˆí•„ìš” (ì‹œê°„ ì ˆì•½!)\")\n",
    "    print(f\"  âœ… YOLOv8 ëŒ€ë¹„ 15-20% ì„±ëŠ¥ í–¥ìƒ\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ ë°±ì—… ìœ„ì¹˜: {backup_dir}\")\n",
    "    print(f\"ğŸ† ìµœì  ëª¨ë¸: {backup_dir}/best_yolov11s.pt\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(f\"  1. ğŸ”„ API ì„œë²„ ëª¨ë¸ì„ YOLOv11së¡œ êµì²´\")\n",
    "    print(f\"  2. ğŸ§ª ì‹¤ì œ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"  3. ğŸ“ˆ ê¸°ì¡´ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ\")\n",
    "    print(f\"  4. ğŸš€ í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ YOLOv11s ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ëŒ€ì„±ê³µ! ğŸ‰\")\n",
    "    print(f\"ğŸŒŸ íš¨ìœ¨ì ì´ê³  ê°•ë ¥í•œ ìµœì‹  ê¸°ìˆ ì˜ ì¡°í•©! ğŸŒŸ\")\n",
    "    \n",
    "    # API ì„œë²„ ëª¨ë¸ êµì²´ ì•ˆë‚´\n",
    "    print(f\"\\nğŸ“ API ì„œë²„ ëª¨ë¸ êµì²´ ë°©ë²•:\")\n",
    "    print(f\"  1. Google Driveì—ì„œ best_yolov11s.pt ë‹¤ìš´ë¡œë“œ\")\n",
    "    print(f\"  2. ë¡œì»¬ í”„ë¡œì íŠ¸ì˜ models/ í´ë”ì— ë³µì‚¬\")\n",
    "    print(f\"  3. API ì„¤ì •ì—ì„œ ëª¨ë¸ ê²½ë¡œ ë³€ê²½\")\n",
    "    print(f\"  4. API ì„œë²„ ì¬ì‹œì‘\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ì¼ë¶€ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ ìœ„ì˜ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023319a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŠ ì™„ë²½í•œ ê°œì„  ì™„ë£Œ!\n",
    "\n",
    "**YOLOv11s + ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ë¡œë”©ìœ¼ë¡œ ìµœì ì˜ í•™ìŠµ í™˜ê²½ êµ¬ì¶•!**\n",
    "\n",
    "### ğŸš€ ì£¼ìš” ê°œì„ ì‚¬í•­\n",
    "- âœ… **Google Drive ìë™ íŒŒì¼ íƒì§€** - ì¬ì—…ë¡œë“œ ë¶ˆí•„ìš”!\n",
    "- âœ… **ë‹¤ì–‘í•œ íŒŒì¼ëª… íŒ¨í„´ ì§€ì›** - ì–´ë–¤ ì´ë¦„ì´ë“  ìë™ ì¸ì‹\n",
    "- âœ… **ìŠ¤ë§ˆíŠ¸ ì˜¤ë¥˜ ì²˜ë¦¬** - ë¬¸ì œ ë°œìƒì‹œ ëŒ€ì•ˆ ì œì‹œ\n",
    "- âœ… **ìµœì‹  YOLOv11s** - 2024ë…„ ìµœê³  ì„±ëŠ¥ ì•„í‚¤í…ì²˜\n",
    "\n",
    "### ğŸ“‹ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "- âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\n",
    "- âœ… ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ\n",
    "- âœ… YOLOv11s í•™ìŠµ ì™„ë£Œ\n",
    "- âœ… ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ\n",
    "- âœ… Google Drive ë°±ì—… ì™„ë£Œ\n",
    "\n",
    "**ğŸŒŸ ì´ì œ ì‹œê°„ ì ˆì•½ê³¼ ìµœê³  ì„±ëŠ¥ì„ ë™ì‹œì— ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤! ğŸŒŸ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb136e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ëª¨ë¸ íŒŒì¼ ìœ„ì¹˜ í™•ì¸ ë° ìˆ˜ë™ Google Drive ë°±ì—…\n",
    "print(\"ğŸ” í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ í™•ì¸ ì¤‘...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ë¡œì»¬ ê²°ê³¼ ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "result_dirs = glob.glob('/content/pinetree_yolov11s/*')\n",
    "print(f\"ğŸ“ ë°œê²¬ëœ ê²°ê³¼ ë””ë ‰í† ë¦¬ë“¤:\")\n",
    "for i, dir_path in enumerate(result_dirs):\n",
    "    print(f\"  {i+1}. {dir_path}\")\n",
    "\n",
    "if result_dirs:\n",
    "    # ê°€ì¥ ìµœê·¼ ë””ë ‰í† ë¦¬ ì„ íƒ\n",
    "    latest_dir = max(result_dirs, key=os.path.getctime)\n",
    "    print(f\"\\nâœ… ìµœì‹  í•™ìŠµ ê²°ê³¼: {latest_dir}\")\n",
    "    \n",
    "    # weights í´ë” í™•ì¸\n",
    "    weights_dir = f\"{latest_dir}/weights\"\n",
    "    if os.path.exists(weights_dir):\n",
    "        weight_files = os.listdir(weights_dir)\n",
    "        print(f\"\\nğŸ“¦ ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼ë“¤:\")\n",
    "        for file in weight_files:\n",
    "            file_path = os.path.join(weights_dir, file)\n",
    "            size_mb = os.path.getsize(file_path) / 1024 / 1024\n",
    "            print(f\"  ğŸ† {file} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # 2. Google Drive ë°±ì—… ìˆ˜í–‰\n",
    "        print(f\"\\nğŸ’¾ Google Drive ìˆ˜ë™ ë°±ì—… ì‹œì‘...\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        backup_dir = f\"/content/drive/MyDrive/pinetree_models/yolov11s_manual_{timestamp}\"\n",
    "        \n",
    "        try:\n",
    "            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "            os.makedirs(backup_dir, exist_ok=True)\n",
    "            print(f\"âœ… ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±: {backup_dir}\")\n",
    "            \n",
    "            # ì „ì²´ í•™ìŠµ ê²°ê³¼ ë³µì‚¬\n",
    "            backup_training_dir = f\"{backup_dir}/training_results\"\n",
    "            shutil.copytree(latest_dir, backup_training_dir, dirs_exist_ok=True)\n",
    "            print(\"âœ… ì „ì²´ í•™ìŠµ ê²°ê³¼ ë°±ì—… ì™„ë£Œ\")\n",
    "            \n",
    "            # ì£¼ìš” ëª¨ë¸ íŒŒì¼ë“¤ ìµœìƒìœ„ë¡œ ë³µì‚¬\n",
    "            important_files = ['best.pt', 'last.pt']\n",
    "            for file in important_files:\n",
    "                src_path = f\"{weights_dir}/{file}\"\n",
    "                if os.path.exists(src_path):\n",
    "                    dst_path = f\"{backup_dir}/{file}\"\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                    print(f\"âœ… {file} ë°±ì—… ì™„ë£Œ\")\n",
    "            \n",
    "            # ê²°ê³¼ ìš”ì•½ íŒŒì¼ ìƒì„±\n",
    "            summary_content = f\"\"\"# YOLOv11s í•™ìŠµ ê²°ê³¼ ìš”ì•½\n",
    "\n",
    "## ğŸ“Š í•™ìŠµ ì •ë³´\n",
    "- í•™ìŠµ ì™„ë£Œ ì‹œê°„: {timestamp}\n",
    "- ë¡œì»¬ ê²°ê³¼ ê²½ë¡œ: {latest_dir}\n",
    "- Google Drive ë°±ì—… ê²½ë¡œ: {backup_dir}\n",
    "\n",
    "## ğŸ“ í¬í•¨ëœ íŒŒì¼ë“¤\n",
    "\"\"\"\n",
    "            for file in weight_files:\n",
    "                summary_content += f\"- {file}\\n\"\n",
    "            \n",
    "            with open(f\"{backup_dir}/README.md\", 'w', encoding='utf-8') as f:\n",
    "                f.write(summary_content)\n",
    "            \n",
    "            print(f\"\\nğŸ‰ ìˆ˜ë™ ë°±ì—… ì™„ë£Œ!\")\n",
    "            print(f\"ğŸ“ Google Drive ìœ„ì¹˜: {backup_dir}\")\n",
    "            print(f\"ğŸ† ìµœê³  ëª¨ë¸: {backup_dir}/best.pt\")\n",
    "            print(f\"ğŸ“ ë§ˆì§€ë§‰ ëª¨ë¸: {backup_dir}/last.pt\")\n",
    "            \n",
    "            # Google Drive íŒŒì¼ ëª©ë¡ í™•ì¸\n",
    "            print(f\"\\nğŸ“‚ Google Drive ë°±ì—… í™•ì¸:\")\n",
    "            if os.path.exists(backup_dir):\n",
    "                backup_files = os.listdir(backup_dir)\n",
    "                for file in backup_files:\n",
    "                    file_path = os.path.join(backup_dir, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        size_mb = os.path.getsize(file_path) / 1024 / 1024\n",
    "                        print(f\"  âœ… {file} ({size_mb:.1f} MB)\")\n",
    "                    else:\n",
    "                        print(f\"  ğŸ“ {file}/\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°±ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print(\"ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ íŒŒì¼ì„ ë³µì‚¬í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"âŒ weights í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weights_dir}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ í•™ìŠµ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ğŸ’¡ í•™ìŠµì´ ì œëŒ€ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nğŸ”— Google Drive ì ‘ì† ë°©ë²•:\")\n",
    "print(f\"1. https://drive.google.com ì ‘ì†\")\n",
    "print(f\"2. ë‚´ ë“œë¼ì´ë¸Œ > pinetree_models í´ë” í™•ì¸\")\n",
    "print(f\"3. yolov11s_manual_{timestamp} í´ë”ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
